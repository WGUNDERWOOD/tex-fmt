\documentclass{cam-thesis}

\usepackage[english]{babel}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{csquotes}

%\usepackage{microtype}

\usepackage[ttscale=.75]{libertine}
\usepackage{dsfont}

\usepackage[parfill]{parskip}

% Set nicer (= less bold, less vertical spacing) mathcal font
\usepackage[cal=cm]{mathalpha}

% % Set up the headers and footers
% \usepackage{fancyhdr}
% \usepackage{ifthen}
% \pagestyle{fancy}
% \fancyhf{}
% % Use ifthenelse to work around the fact that we wish to have
% alternate headers
% % but a onesided document
% \fancyhead[R]{\ifthenelse{\isodd{\value{page}}}{%
% \thepage\hfill\textsc{\nouppercase\leftmark}}{}}
% \fancyhead[L]{\ifthenelse{\isodd{\value{page}}}{}{%
% \textsc{\nouppercase\rightmark}\hfill\thepage}}
% \fancyfoot{}

% % Remove page numbers on the first page of a chapter
% \fancypagestyle{plain}{%
%   \renewcommand{\headrulewidth}{0pt}%
%   \fancyhf{}%
% }

% See the excellent biblatex documentation for more information
\usepackage[
  backend=biber,%
  style=alphabetic,%
  block=ragged,%
  backref=false,%
  useprefix=true,%
  maxnames=8,%
  minnames=7,%
  minalphanames=3,%
  maxalphanames=4,%
  url=false,
  eprint=true,
backrefstyle=two]%
{biblatex}
\renewcommand{\subtitlepunct}{\addcolon\addspace}

% \DefineBibliographyStrings{english}{%
%   bibliography = {References}, }

% Enumerations and tables
\usepackage{calc}
\usepackage[shortlabels]{enumitem}
% \setlist{nosep}
\setlist[description]{font={\textnormal},labelindent=\parindent}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[width=.8\textwidth]{caption}
\captionsetup[table]{skip=1em}

% Math packages
\usepackage{mathtools}

\usepackage{savesym}
\usepackage{amsmath}
\savesymbol{openbox}
\usepackage{amsthm}

\usepackage{thmtools}
\savesymbol{Bbbk}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{bm}
% \usepackage{mathabx}

% % tocbibind allows us to have the toc in the toc
% \usepackage[notbib,notindex]{tocbibind}
% % Supposedly it should also allow us to have the index and the bibliography in
% % the toc, but it has some bugs (e.g. displaying the right page number in the
% % toc, but getting the wrong link with hyperref), so we disable those options
% % here and use corresponding separate options for the index, index of symbols
% % (nomenclature) and bibliography instead.
% %
% % The whole is rather finicky and it is somehow crucial that
% tocbibind is loaded
% % *before* imakeidx.

% \usepackage{imakeidx}
% \makeindex[intoc,columns=2]
% \usepackage[refpage,intoc,noprefix]{nomencl}
% % Set fixed width so that descriptions in the index of symbols are aligned.
% \setlength{\nomlabelwidth}{5cm}
% \renewcommand{\nomname}{Index of symbols}
% % Make page numbers links
% \renewcommand*{\pagedeclaration}[1]{\unskip, \hyperpage{#1}}
% \makenomenclature%

% Used in hyperref's setup, and must be loaded before tikz-cd.
\usepackage[dvipsnames]{xcolor}

\definecolor{Diag1}{RGB}{0,0,255}
\definecolor{Diag2}{RGB}{255,0,0}

\usepackage[most]{tcolorbox}
\usepackage{tikz-cd}

\usepackage[
  colorlinks=true  % Remove the boxes
  , linktocpage=true % Make page numbers (not section titles) links in ToC
  , linkcolor=NavyBlue    % Colour for internal links
  , citecolor=Green  % Colour for bibliographical citations
  , urlcolor=BrickRed % Colour for (external) urls
]{hyperref}

\usepackage[noabbrev,capitalise]{cleveref}
\newcommand{\creflastconjunction}{, and\nobreakspace}
\creflabelformat{equation}{#2\textup{#1}#3} % Write Equation x.y.z
% instead of Equation (x.y.z)
\Crefname{judgement}{Judgement}{Judgements}
\Crefname{diagram}{Diagram}{Diagrams}
\Crefname{rule}{Rule}{Rules}

% Label tables just like equations, theorems, definitions, etc.
%
% NB: This can be confusing if LaTeX does not place the table at the point of
% writing (e.g. for lack of space)!
\numberwithin{equation}{section}

% Colours are as in Andrej Bauer's notes on realizability:
% https://github.com/andrejbauer/notes-on-realizability
\colorlet{ShadeOfPurple}{blue!5!white}
\colorlet{ShadeOfYellow}{yellow!5!white}
\colorlet{ShadeOfGreen} {green!5!white}
\colorlet{ShadeOfBrown} {brown!10!white}

% Add a blue for principles
\colorlet{ShadeOfBlue}{cyan!5!white}
% But we also shade proofs
\colorlet{ShadeOfGray}  {gray!10!white}

\declaretheorem[sibling=equation]{theorem}
\declaretheorem[sibling=theorem]{lemma}
\declaretheorem[sibling=theorem]{proposition}
\declaretheorem[sibling=theorem]{corollary}
\declaretheorem[sibling=theorem,style=definition]{definition}
\declaretheorem[sibling=theorem,style=remark]{example}
\declaretheorem[sibling=theorem,style=remark]{remark}
\declaretheorem[style=definition,name=Guiding principle for
groupoids,numbered=no]{principle-groupoid}
\declaretheorem[style=definition,name=Guiding principle for
categories,numbered=no]{principle-category}
% Now we set the shading using the tcolorbox package.
%
% The related thmtools' option "shaded" and the package mdframed seem to have
% issues: the former does not allow for page breaks in shaded environments and
% the latter puts double spacing between two shaded environments.
%
% Since tcolorbox puts stuff inside a minipage or \parbox (according to this
% stackexchange answer: https://tex.stackexchange.com/a/250170), new
% paragraphs aren't indented. We can fix this by grabbing the parindent
% value and passing it to tcbset.
\newlength{\normalparindent}
\AtBeginDocument{\setlength{\normalparindent}{\parindent}}
\newlength{\normalparskip}
\AtBeginDocument{\setlength{\normalparskip}{\parskip}}
\tcbset{shadedenv/.style={
    colback={#1},
    frame hidden,
    enhanced,
    breakable,
    boxsep=0pt,
    left=2mm,
    right=2mm,
    % LaTeX thinks this is too wide (as becomes clear from the many "Overfull
    % \hbox" warnings, but optically it looks spot on.
    add to width=1.1mm,
    enlarge left by=-0.6mm,
    before upper={\setlength{\parindent}{\normalparindent}%
    \setlength{\parskip}{\normalparskip}}
}}
\newcommand{\setenvcolor}[2]{%
  \tcolorboxenvironment{#1}{shadedenv={#2}}
  \addtotheorempreheadhook[#1]{\tikzcdset{background color=#2}}
}
%
\setenvcolor{theorem}{ShadeOfPurple}
\setenvcolor{lemma}{ShadeOfPurple}
\setenvcolor{proposition}{ShadeOfPurple}
\setenvcolor{corollary}{ShadeOfPurple}
\setenvcolor{definition}{ShadeOfYellow}
\setenvcolor{example}{ShadeOfGreen}
\setenvcolor{remark}{ShadeOfBrown}
\setenvcolor{principle-groupoid}{ShadeOfBlue}
\setenvcolor{principle-category}{ShadeOfBlue}
\setenvcolor{proof}{ShadeOfGray}
\declaretheorem[sibling=theorem,style=remark,numbered=no]{claim}

\usepackage{xspace}

\usepackage{quiver}
\usetikzlibrary{nfold, backgrounds, decorations.pathmorphing, positioning}
\tikzcdset{column sep/smaller/.initial=0em}
\tikzcdset{arrow style = tikz, diagrams={>=stealth}}
\tikzcdset{Rightarrow/.append style ={nfold}}

\usepackage{adjustbox}

\usepackage{cellspace}
\usepackage{makecell}
\setlength\cellspacetoplimit{5pt} \setlength\cellspacebottomlimit{5pt}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\usepackage{ebproof}
\usepackage{mathpartir}
\usepackage{subcaption}
\usepackage{float}
\usepackage{afterpage}
\usepackage{listings}
\lstdefinestyle{cattstyle}{
  keywordstyle=\color{Diag1},
  keywordstyle=[2]\color{Diag2},
  basicstyle=\ttfamily,
  breaklines=true,
  keepspaces=true,
  belowskip=0pt,
}
\lstset{style=cattstyle}
\lstdefinelanguage{Catt}{
  keywords=[1]{def,normalise,assert,size,in},
  keywords=[2]{coh,comp,id}
}

\usepackage{fontspec}
\usepackage{fancyvrb}
%\setmonofont[Scale=0.8]{Hack Nerd Font Mono}
\hfuzz=1.5pt
\def\su{\textsf{su}\xspace}
\def\sua{\textsf{sua}\xspace}
\def\sa{\textsf{sa}\xspace}
\def\Catt{\textsc{Catt}\xspace}
\def\Cattsua{\textsc{Catt}\textsubscript{\sua}\xspace}
\def\Cattsu{\textsc{Catt}\textsubscript{\su}\xspace}
\def\Cattsa{\textsc{Catt}\textsubscript{\sa}\xspace}
\def\Cattr{\textsc{Catt}\textsubscript{\(\mathcal{R}\)}\xspace}
\def\Group{\textsf{Group}\xspace}
\def\Reg{\textsf{Reg}\xspace}
\def\Std{\textsf{Std}\xspace}
\def\dr{\textsf{dr}\xspace}
\def\ecr{\textsf{ecr}\xspace}
\def\prune{\textsf{prune}\xspace}
\def\insert{\textsf{insert}\xspace}
\newcommand\id{\ensuremath{\mathsf{id}}}
\newcommand\proj{\ensuremath{\mathsf{proj}}}

\newcommand*{\Coh}[3]{\ensuremath\mathsf{Coh}_{(#1\,;\,#2)}[#3]}
\newcommand*{\SCoh}[3]{\ensuremath\mathsf{SCoh}_{(#1\,;\,#2)}[#3]}
\newcommand*{\Ctx}{\ensuremath{\mathsf{Ctx}}}
\newcommand*{\Tree}{\ensuremath{\mathsf{Tree}}}
\newcommand*{\Sub}{\ensuremath{\mathsf{Sub}}}
\newcommand*{\Type}{\ensuremath{\mathsf{Type}}}
\newcommand*{\SType}{\ensuremath{\mathsf{SType}}}
\newcommand*{\Term}{\ensuremath{\mathsf{Term}}}
\newcommand*{\STerm}{\ensuremath{\mathsf{STerm}}}
\newcommand*{\arr}[3]{{#1 \to_{#2} #3}}
\newcommand*{\sub}[1]{\ensuremath{\llbracket #1 \rrbracket}}
\newcommand*{\bound}[2]{\ensuremath{\partial_{#1}({#2})}}
\newcommand*{\bdry}[3]{\ensuremath{\partial_{#1}^{#2}({#3})}}
\newcommand*{\incbd}[3]{\ensuremath{\delta_{#1}^{#2}({#3})}}
\newcommand*{\incbdpath}[3]{\ensuremath{\mathrm{I}_{#1}^{#2}({#3})}}
\newcommand*{\stdcoh}[2]{\mathcal{C}_{#1}^{#2}}
\newcommand*{\stdty}[2]{\mathcal{U}_{#1}^{#2}}
\newcommand*{\stdtm}[2]{\mathcal{T}_{#1}^{#2}}
\newcommand*{\stdlbl}[2]{\mathcal{L}_{#1}^{#2}}
\newcommand*{\unrestrict}{\mathop\downarrow}
\newcommand*{\unrestrictfull}{\mathop{\downarrow\downarrow}}
\newcommand*{\restrict}{\mathop\uparrow}
\newcommand*{\Dyck}{\mathsf{Dyck}}
\newcommand*{\Peak}{\mathsf{Peak}}
\newcommand*{\Path}{\mathsf{Path}}
\newcommand*{\MaxPath}{\mathsf{MaxPath}}
\newcommand*{\SPath}{\mathsf{SPath}}
\newcommand*{\SOther}{\mathsf{SOther}}
\newcommand*{\Inc}{\mathsf{Inc}}
\newcommand*{\UDPeak}{\Updownarrow_{\mathsf{pk}}}
\newcommand*{\UpPeak}{\Uparrow_{\mathsf{pk}}}
\newcommand*{\DownPeak}{\Downarrow_{\mathsf{pk}}}
\newcommand*{\eval}{\mathsf{eval}}
\renewcommand*{\quote}{\mathsf{quote}}
\newcommand*{\red}{\rightsquigarrow}
\newcommand*{\redr}{\rightsquigarrow_{\mathcal{R}}}
\newcommand*{\redrts}{\leftrightsquigarrow_{\mathcal{R}}}
\DeclareMathOperator{\doubleplus}{+\kern-1ex+}
\newcommand\emp{{[\kern3pt]}}
\newcommand*{\insertion}[3]{\ensuremath{#1\mathop{\mathord{\ll}_{#2}}#3}}
\newcommand*{\insertionprime}[3]{\ensuremath{#1\mathop{\mathord{\ll'}_{#2}}#3}}
\renewcommand*{\th}{\ensuremath{\mathsf{th}}}
\newcommand*{\bh}{\ensuremath{\mathsf{bh}}}
\newcommand*{\lh}{\ensuremath{\mathsf{lh}}}
\newcommand*{\+}{\mathbin{\#}}
\DeclareMathOperator*{\bighash}{\text{\LARGE \(\+\)}}
\renewcommand*{\sc}{\ensuremath{\mathsf{sc}}}
\newcommand*{\U}{\mathbf{U}}
\DeclareMathOperator{\FV}{FV}
\DeclareMathOperator{\DC}{DC}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Supp}{Supp}
\DeclareMathOperator{\replace}{replace}
\DeclareMathOperator{\drop}{drop}
\DeclareMathOperator{\ty}{Ty}
\DeclareMathOperator{\tm}{Tm}
\DeclareMathOperator{\wk}{wk}
\DeclareMathOperator{\src}{src}
\DeclareMathOperator{\tgt}{tgt}
\DeclareMathOperator{\base}{base}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\inc}{inc}
\DeclareMathOperator{\fst}{fst}
\DeclareMathOperator{\snd}{snd}
\DeclareMathOperator{\dep}{dep}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\ext}{ext}

\makeatletter
\providecommand{\leftsquigarrow}{%
  \mathrel{\mathpalette\reflect@squig\relax}%
}
\newcommand{\reflect@squig}[2]{%
  \reflectbox{$\m@th#1\rightsquigarrow$}%
}
\makeatother

\newcommand{\olsi}[1]{\,\overline{\!{#1}}} % overline short italic

\newcommand*{\module}[1]{%
\href{https://alexarice.github.io/catt-agda/#1.html}{#1}}
\newcommand*{\funcn}[3]{%
\href{https://alexarice.github.io/catt-agda/#1.html\##2}{#3}}
\newcommand*{\func}[2]{\funcn{#1}{#2}{#2}}

\newlist{lemmaenum}{enumerate}{1} % should only occur inside lemma env.
\setlist[lemmaenum]{label=(\roman*),ref=\thelemma(\roman*)}
\crefalias{lemmaenumi}{lemma}
\addbibresource{higher_categories_thesis.bib}

\title{A type-theoretic approach to semistrict higher categories}

%% The full name of the author (e.g.: James Smith):
\author{Alex Rice}

%% College affiliation:
\college{Darwin College}

%% College shield:
%\collegeshield{CollegeShields/Darwin}

%% Submission date [optional]:
\submissiondate{18\textsuperscript{th} April 2024}

%% Declaration date:
\date{18\textsuperscript{th} April 2024}

%% PDF meta-info:
\subjectline{Computer Science}
\keywords{category theory, higher category theory, type theory}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstract:
%%
\abstract{%
  Weak \(\infty\)-categories are known to be more expressive than
  their strict counterparts, but are more difficult to work with, as
  constructions in such a category involve the manipulation of
  explicit coherence data. This motivates the search for definitions
  of semistrict \(\infty\)-categories, where some, but not all, of
  the operations have been strictified.

  We introduce a general framework for adding definitional equality
  to the type theory \Catt, a type theory whose models correspond to
  globular weak \(\infty\)-categories, which was introduced by
  \citeauthor{finster2017type}. Adding equality to this theory causes
  the models to exhibit \emph{semistrict} behaviour, trivialising
  some operations while leaving others weak. The framework consists
  of a generalisation of \Catt extended with an equality relation
  generated by an arbitrary set of equality rules \(\mathcal{R}\),
  which we name \Cattr. We study this framework in detail,
  formalising much of its metatheory in the proof assistant Agda, and
  studying how certain operations of \Catt behave in the presence of
  definitional equality.

  The main contribution of this thesis is to introduce two type
  theories, \Cattsu and \Cattsua, which are instances of this general
  framework. \Cattsu, short for \Catt with strict units, is a variant
  of \Catt where the unitor isomorphisms trivialise to identities. It
  is primarily generated by a reduction we call \emph{pruning}, which
  removes identities from composites, simplifying their structure.
  \Cattsua, which stands for \Catt with strict units and associators,
  trivialises both the associativity and unitality operations of
  \Catt, and is generated by a generalisation of pruning called
  \emph{insertion}. Insertion merges multiple composites into a
  single operation, flattening the structure of terms in the theory.

  Further, we provide reduction systems that generate the equality of
  both \Cattsu and \Cattsua respectively, and prove that these
  reductions systems are strongly terminating and confluent. We
  therefore prove that the equality, and hence typechecking, of both
  theories is decidable. This is used to give an implementation of
  these type theories, which uses an approach inspired by
  normalisation by evaluation to efficiently find normal forms for
  terms. We further introduce a bidirectional typechecking algorithm
  used by the implementation which allows for terms to be defined in
  a convenient syntax where many arguments can be left implicit.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements:
%%
\acknowledgements{%
  I would firstly like to thank everyone that I have collaborated
  with over the course of my PhD, both for their contributions to the
  work that appears in this thesis, but also for their contributions
  to my development as a researcher. I would especially like to thank
  my supervisor, Jamie Vicary, whose guidance throughout was
  invaluable, for keeping my research on track despite the
  disruptions caused by the pandemic during the first years of my PhD.

  I would also like to thank all the friends who have been with me at
  any point in this journey. I particularly want to show my
  appreciation (and apologise) to everyone who was bombarded with
  technical questions throughout the writing up of this text; I
  thoroughly enjoyed our discussions on correct typesetting and use
  of the English language.

  Lastly, I would like to thank my family for supporting me
  throughout my entire education. I would not have made it to this
  point without them.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Contents:
%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page, abstract, declaration etc.:
%% -    the title page (is automatically omitted in the technical report mode).

\frontmatter{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis body:
%%
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

The study of higher-dimensional structures is becoming more prevalent
in both mathematics and computer science. \emph{Higher
categories}~\cite{leinster2004higher,riehl2022elements}, a broad term
for many different generalisations categories which capture these
higher-dimensional ideas, are a central tool for studying these
structures. The ``higher'' nature of these categories typically
corresponds to the existence of morphisms whose source and target may
be other morphisms, instead of just objects. A common method of
organising this data is by giving a set of \(n\)-cells for each \(n
\in \mathbb{N}\). A \(0\)-cell then corresponds to the objects of an
ordinary category, and the source and target of an \((n+1)\)-cell are
given by \(n\)-cells.

These higher categories present in many forms, and have been
characterised into a periodic table of
categories~\cite{cheng2007periodic,cheng2007periodic2}. Of particular
interest are the \((n,k)\)-categories for \(n,k \in \mathbb{N} \cup
\{\infty\}\), higher categories which contain \(m\)-cells for \(m
\leq n\), and whose \(m\)-cells are invertible for \(m < k\). In
mathematics, the study of \((\infty,0)\)-categories, known as
\(\infty\)-groupoids, is motivated by the study of the homotopy
structure of topological spaces~\cite{Bourbaki2016}, where
\(n\)-cells are given by paths in the topological space, with higher
cells taking the form of homotopies between lower cells. In computer
science, many applications have been found for \((n,n)\)-categories
for smaller \(n\), more commonly referred to as \(n\)-categories,
including quantum computing~\cite{Heunen2019-jt},
logic~\cite{Barr1991,mellies2009categorical},
physics~\cite{Baez1995}, and game
theory~\cite{ghani2018compositional}, among others~\cite{Street2012}.

The composition of \(1\)-cells in an \(n\)-category functions
identically to the composition of morphisms in a \(1\) category; two
morphisms \(f : x \to y\) and \(g : y \to z\) can be composed to form
a \(1\)-cell \(f * g : x \to z\). However, there are two distinct
ways of composing \(2\)-cells, depicted by the diagrams below:
% https://q.uiver.app/#q=WzAsNSxbMCwwLCJcXGJ1bGxldCJdLFsyLDAsIlxcYnVsbGV0Il0sWzQsMCwiXFxidWxsZXQiXSxbNSwwLCJcXGJ1bGxldCJdLFs2LDAsIlxcYnVsbGV0Il0sWzAsMSwiIiwwLHsiY3VydmUiOi01fV0sWzAsMSwiIiwyLHsiY3VydmUiOjV9XSxbMCwxXSxbMiwzLCIiLDEseyJjdXJ2ZSI6LTN9XSxbMiwzLCIiLDEseyJjdXJ2ZSI6M31dLFszLDQsIiIsMSx7ImN1cnZlIjotM31dLFszLDQsIiIsMSx7ImN1cnZlIjozfV0sWzcsNSwiXFxhbHBoYSIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbNiw3LCJcXGJldGEiLDIseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzksOCwiXFxnYW1tYSIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbMTEsMTAsIlxcZGVsdGEiLDIseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV1d % tex-fmt: skip
\[
  \begin{tikzcd}
    \bullet && \bullet && \bullet & \bullet & \bullet
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-30pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=30pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0}, from=1-1, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0},
    curve={height=-18pt}, from=1-5, to=1-6]
    \arrow[""{name=4, anchor=center, inner sep=0},
    curve={height=18pt}, from=1-5, to=1-6]
    \arrow[""{name=5, anchor=center, inner sep=0},
    curve={height=-18pt}, from=1-6, to=1-7]
    \arrow[""{name=6, anchor=center, inner sep=0},
    curve={height=18pt}, from=1-6, to=1-7]
    \arrow["\beta"', shorten <=4pt, shorten >=4pt, Rightarrow, from=2, to=0]
    \arrow["\alpha"', shorten <=4pt, shorten >=4pt, Rightarrow, from=1, to=2]
    \arrow["\gamma"', shorten <=5pt, shorten >=5pt, Rightarrow, from=4, to=3]
    \arrow["\delta"', shorten <=5pt, shorten >=5pt, Rightarrow, from=6, to=5]
  \end{tikzcd}
\]
These diagrams mirror the concept of commutative diagrams for
\(1\)-categories, where spaces in the commutative diagram
representing an equality have been replaced by \(2\)-cell arrows. The
first of these composites composes two \(2\)-cells \(\alpha\) and
\(\beta\) along a shared \(1\)-cell boundary creating the vertical
composite \(\alpha \star_1 \beta\). The second composes the
\(2\)-cells \(\gamma\) and \(\delta\) along a \(0\)-cell boundary and
creates the horizontal composite \(\gamma \star_0 \delta\). In higher
dimensions, the pattern continues of having \(n\) distinct ways of
composing two \(n\)-cells. For each \(n\)-cell, there is also an
identity \((n+1)\)-cell.

Similarly to \(1\)-categories, \(n\)-categories must satisfy various
laws concerning their operations. These can be roughly organised into 3 groups:
\begin{itemize}
  \item Associativity laws: Each of the composition operations in an
    \(n\)-category is associative.
  \item Unitality laws: The identity morphisms are a left and right
    unit for the appropriate composition operations.
  \item Interchange laws: These laws govern the relation between
    different compositions on the same cells. For any four
    \(2\)-cells that form the following diagram:
% https://q.uiver.app/#q=WzAsNCxbMCwwLCJcXGJ1bGxldCJdLFsyLDAsIlxcYnVsbGV0Il0sWzYsMF0sWzQsMCwiXFxidWxsZXQiXSxbMCwxLCIiLDAseyJjdXJ2ZSI6LTV9XSxbMCwxLCIiLDIseyJjdXJ2ZSI6NX1dLFswLDFdLFsxLDMsIiIsMix7ImN1cnZlIjotNX1dLFsxLDMsIiIsMix7ImN1cnZlIjo1fV0sWzEsM10sWzYsNCwiXFxiZXRhIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs1LDYsIlxcYWxwaGEiLDIseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzgsOSwiXFxnYW1tYSIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbOSw3LCJcXGRlbHRhIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dXQ== % tex-fmt: skip
    \[
      \begin{tikzcd}
        \bullet && \bullet && \bullet
        \arrow[""{name=0, anchor=center, inner sep=0},
        curve={height=-30pt}, from=1-1, to=1-3]
        \arrow[""{name=1, anchor=center, inner sep=0},
        curve={height=30pt}, from=1-1, to=1-3]
        \arrow[""{name=2, anchor=center, inner sep=0}, from=1-1, to=1-3]
        \arrow[""{name=3, anchor=center, inner sep=0},
        curve={height=-30pt}, from=1-3, to=1-5]
        \arrow[""{name=4, anchor=center, inner sep=0},
        curve={height=30pt}, from=1-3, to=1-5]
        \arrow[""{name=5, anchor=center, inner sep=0}, from=1-3, to=1-5]
        \arrow["\beta"', shorten <=4pt, shorten >=4pt, Rightarrow, from=2, to=0]
        \arrow["\alpha"', shorten <=4pt, shorten >=4pt, Rightarrow,
        from=1, to=2]
        \arrow["\gamma"', shorten <=4pt, shorten >=4pt, Rightarrow,
        from=4, to=5]
        \arrow["\delta"', shorten <=4pt, shorten >=4pt, Rightarrow,
        from=5, to=3]
    \end{tikzcd}\]
    the first of the interchange laws states that two composites
    below are related:
    \[ (\alpha \star_1 \beta) \star_0 (\gamma \star_1 \delta) \simeq
    (\alpha \star_0 \gamma) \star_1 (\beta \star_0 \delta)\]
\end{itemize}
These laws can be combined to create non-trivial emergent behaviour
in a form not seen in the theory of \(1\)-categories. One critical
example of this is known as the \emph{Eckmann-Hilton}
argument~\cite{eckmann1962group}, which states that the composition
of two scalars, morphisms from the identity to the identity, commute.
The argument proceeds by moving the two scalars around each other, as
depicted in \cref{fig:eh}. This crucially uses both the interchange
and unitality laws.
\newsavebox{\ehalpha}
\savebox{\ehalpha}{\adjustbox{scale=0.8}{
    \begin{tikzcd}[ampersand replacement=\&,column sep=small,cramped]
      \bullet \& \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      curve={height=-10pt}, from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=10pt}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      curve={height=-10pt}, from=1-2, to=1-3]
      \arrow[""{name=3, anchor=center, inner sep=0},
      curve={height=10pt}, from=1-2, to=1-3]
      \arrow["\alpha"', color=Diag1, shorten <=3pt, shorten >=3pt,
      Rightarrow, from=1, to=0]
      \arrow["\id"', shorten <=3pt, shorten >=3pt, Rightarrow, from=3, to=2]
\end{tikzcd}}}
\newsavebox{\ehbeta}
\savebox{\ehbeta}{\adjustbox{scale=0.8}{
    \begin{tikzcd}[ampersand replacement=\&,column sep=small,cramped]
      \bullet \& \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      curve={height=-10pt}, from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=10pt}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      curve={height=-10pt}, from=1-2, to=1-3]
      \arrow[""{name=3, anchor=center, inner sep=0},
      curve={height=10pt}, from=1-2, to=1-3]
      \arrow["\id"',  shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
      \arrow["\beta"', color=Diag2, shorten <=3pt, shorten >=3pt,
      Rightarrow, from=3, to=2]
\end{tikzcd}}}
\newsavebox{\ehlefttop}
\savebox{\ehlefttop}{
  \adjustbox{scale=1}{%
    \begin{tikzcd}[ampersand replacement=\&,column sep=small,cramped]
      \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      controls=+(80:0.7) and +(100:0.7),, from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=0}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      controls=+(100:-0.7) and +(80:-0.7),, from=1-1, to=1-2]
      \arrow["\alpha", color=Diag1, shorten <=3pt, shorten >=3pt,
      Rightarrow, from=2, to=1]
      \arrow["\id", shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
\end{tikzcd}}}
\newsavebox{\ehrighttop}
\savebox{\ehrighttop}{
  \adjustbox{scale=1}{%
    \begin{tikzcd}[ampersand replacement=\&,column sep=small,cramped]
      \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      controls=+(80:0.7) and +(100:0.7), from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=0}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      controls=+(100:-0.7) and +(80:-0.7),, from=1-1, to=1-2]
      \arrow["\id", shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=1]
      \arrow["\beta", color=Diag2, shorten <=3pt, shorten >=3pt,
      Rightarrow, from=1, to=0]
\end{tikzcd}}}
\newsavebox{\ehleftbot}
\savebox{\ehleftbot}{
  \adjustbox{scale=1}{%
    \begin{tikzcd}[ampersand replacement=\&,column sep=small,cramped]
      \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      controls=+(80:0.7) and +(100:0.7),, from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=0}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      controls=+(100:-0.7) and +(80:-0.7),, from=1-1, to=1-2]
      \arrow["\id", shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=1]
      \arrow["\alpha", color=Diag1, shorten <=3pt, shorten >=3pt,
      Rightarrow, from=1, to=0]
\end{tikzcd}}}
\newsavebox{\ehrightbot}
\savebox{\ehrightbot}{
  \adjustbox{scale=1}{%
    \begin{tikzcd}[ampersand replacement=\&,column sep=small,cramped]
      \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      controls=+(80:0.7) and +(100:0.7), from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=0}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      controls=+(100:-0.7) and +(80:-0.7),, from=1-1, to=1-2]
      \arrow["\beta", color=Diag2, shorten <=3pt, shorten >=3pt,
      Rightarrow, from=2, to=1]
      \arrow["\id", shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
\end{tikzcd}}}

\begin{figure}[ht]
  \centering

  \[
    \begin{tikzcd}[ampersand replacement=\&,column sep=small]
      \bullet \&\& \bullet \& \simeq \& \bullet \&\&\&\&\& \bullet \&
      \simeq \& \bullet \&\&\& \bullet \&\&\& \bullet \\
      \\
      \&\&\&\&\&\&\&\&\&\&\&\&\&\& \simeq \\
      \\
      \bullet \&\& \bullet \& \simeq \& \bullet \&\&\&\&\& \bullet \&
      \simeq \& \bullet \&\&\& \bullet \&\&\& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0}, "\id",
      curve={height=-24pt}, from=1-1, to=1-3]
      \arrow[""{name=1, anchor=center, inner sep=0}, "\id"',
      curve={height=24pt}, from=1-1, to=1-3]
      \arrow[""{name=2, anchor=center, inner sep=0},
      "\id"{description}, from=1-1, to=1-3]
      \arrow[""{name=3, anchor=center, inner sep=0}, draw=none,
      controls=+(90:1.8) and +(90:1.8), from=1-5, to=1-10]
      \arrow[""{name=4, anchor=center, inner sep=0}, draw=none,
      controls=+(90:-1.8) and +(90:-1.8), from=1-5, to=1-10]
      \arrow[""{name=5, anchor=center, inner sep=0}, from=1-5, to=1-10]
      \arrow[""{name=6, anchor=center, inner sep=0}, draw=none,
      controls=+(90:1.8) and +(90:1.8), from=5-5, to=5-10]
      \arrow[""{name=7, anchor=center, inner sep=0}, draw=none,
      controls=+(90:-1.8) and +(90:-1.8), from=5-5, to=5-10]
      \arrow[""{name=8, anchor=center, inner sep=0}, from=5-5, to=5-10]
      \arrow[""{name=9, anchor=center, inner sep=0}, "\id",
      curve={height=-24pt}, from=5-1, to=5-3]
      \arrow[""{name=10, anchor=center, inner sep=0}, "\id"',
      curve={height=24pt}, from=5-1, to=5-3]
      \arrow[""{name=11, anchor=center, inner sep=0},
      "\id"{description}, from=5-1, to=5-3]
      \arrow[""{name=12, anchor=center, inner sep=0}, draw=none,
      controls=+(80:1.5) and +(100:1.5), from=1-12, to=1-15]
      \arrow[""{name=13, anchor=center, inner sep=0}, draw=none,
      controls=+(100:-1.5) and +(80:-1.5), from=1-12, to=1-15]
      \arrow[""{name=14, anchor=center, inner sep=0}, draw=none,
      controls=+(80:1.5) and +(100:1.5), from=1-15, to=1-18]
      \arrow[""{name=15, anchor=center, inner sep=0}, draw=none,
      controls=+(100:-1.5) and +(80:-1.5), from=1-15, to=1-18]
      \arrow[""{name=16, anchor=center, inner sep=0}, draw=none,
      controls=+(80:1.5) and +(100:1.5), from=5-12, to=5-15]
      \arrow[""{name=17, anchor=center, inner sep=0}, draw=none,
      controls=+(100:-1.5) and +(80:-1.5), from=5-12, to=5-15]
      \arrow[""{name=18, anchor=center, inner sep=0}, draw=none,
      controls=+(80:1.5) and +(100:1.5), from=5-15, to=5-18]
      \arrow[""{name=19, anchor=center, inner sep=0}, draw=none,
      controls=+(100:-1.5) and +(80:-1.5), from=5-15, to=5-18]
      \arrow["\alpha"', color=Diag1, shorten <=3pt, shorten >=5pt,
      Rightarrow, from=1, to=2]
      \arrow["\beta"', color=Diag2, shorten <=5pt, shorten >=3pt,
      Rightarrow, from=2, to=0]
      \arrow["\beta"', color=Diag2, shorten <=3pt, shorten >=5pt,
      Rightarrow, from=10, to=11]
      \arrow["\alpha"', color=Diag1, shorten <=5pt, shorten >=3pt,
      Rightarrow, from=11, to=9]
      \arrow["\usebox{\ehalpha}"{description,inner sep = 0,xshift =
      -1.2pt}, shorten <=3pt, shorten >=3pt, Rightarrow, from=4, to=5]
      \arrow["\usebox{\ehbeta}"{description,inner sep = 0,xshift =
      -1.2pt}, shorten <=3pt, shorten >=3pt, Rightarrow, from=5, to=3]
      \arrow["\usebox{\ehbeta}"{description,inner sep = 0,xshift =
      -1.2pt}, shorten <=3pt, shorten >=3pt, Rightarrow, from=7, to=8]
      \arrow["\usebox{\ehalpha}"{description,inner sep = 0,xshift =
      -1.2pt}, shorten <=3pt, shorten >=3pt, Rightarrow, from=8, to=6]
      \arrow["\usebox{\ehlefttop}"{description,inner sep = 0,xshift =
        -1.3pt, yshift = 0.2pt}, shorten <=3pt, shorten >=3pt,
      Rightarrow, from=13, to=12]
      \arrow["\usebox{\ehrighttop}"{description,inner sep = 0,xshift
        = -1.3pt,yshift = 0.2pt}, shorten <=3pt, shorten >=3pt,
      Rightarrow, from=15, to=14]
      \arrow["\usebox{\ehleftbot}"{description,inner sep = 0,xshift =
      -1.3pt}, shorten <=3pt, shorten >=3pt, Rightarrow, from=17, to=16]
      \arrow["\usebox{\ehrightbot}"{description,inner sep = 0,xshift
      = -1.3pt}, shorten <=3pt, shorten >=3pt, Rightarrow, from=19, to=18]
      \arrow[controls=+(90:1.8) and +(90:1.8), from=1-5, to=1-10]
      \arrow[controls=+(90:-1.8) and +(90:-1.8), from=1-5, to=1-10]
      \arrow[controls=+(90:1.8) and +(90:1.8), from=5-5, to=5-10]
      \arrow[controls=+(90:-1.8) and +(90:-1.8), from=5-5, to=5-10]
      \arrow[controls=+(80:1.5) and +(100:1.5), from=1-12, to=1-15]
      \arrow[controls=+(100:-1.5) and +(80:-1.5), from=1-12, to=1-15]
      \arrow[controls=+(80:1.5) and +(100:1.5), from=1-15, to=1-18]
      \arrow[controls=+(100:-1.5) and +(80:-1.5), from=1-15, to=1-18]
      \arrow[controls=+(80:1.5) and +(100:1.5), from=5-12, to=5-15]
      \arrow[controls=+(100:-1.5) and +(80:-1.5), from=5-12, to=5-15]
      \arrow[controls=+(80:1.5) and +(100:1.5), from=5-15, to=5-18]
      \arrow[controls=+(100:-1.5) and +(80:-1.5), from=5-15, to=5-18]
    \end{tikzcd}
  \]
  \caption{The Eckmann-Hilton argument.}
  \label{fig:eh}
\end{figure}

\paragraph{Semistrict higher categories}

While we have given the types of laws that must hold in
\(n\)-categories, we have not yet stated the full nature of these
laws. By taking each of these laws to hold up to equality, one
obtains the notion of a \emph{strict} \(n\)-category. It is often the
case in category theory that equality is the incorrect notion by
which to compare objects, with the coarser relation of isomorphism
being preferable. In the presence of higher-dimensional cells, arrows
themselves can be compared up to isomorphism. This allows the laws
for an \(n\)-category to be stated with isomorphism replacing
equality, giving rise to the notion of \emph{weak} \(n\)-category.

In such a weak \(n\)-category, each law is given by a set of
isomorphisms, which are given as part of the data of the category.
For the associativity law of three \(1\)-cells \(f\), \(g\), and
\(h\), an invertible \(2\)-cell known as the \emph{associator} must
be given, which takes the following form:
\[ \alpha_{f,g,h} : (f * g) * h \to f * (g * h)\]
Similarly, the unit laws for a \(1\)-cell \(f\) are given by the
\emph{left unitor} \(\lambda_f\) and the \emph{right unitor}
\(\rho_f\) which take the following form:
\[ \lambda_f : \id * f \to f \qquad \rho_f : f * \id \to f\]
Whereas two morphisms being equal is a property of those morphisms,
an isomorphism between the same morphisms is a form of data, and the
choice of isomorphism may not be unique. Weak higher categories
therefore contain higher \emph{coherence laws} which govern the
interaction of these isomorphisms. These coherence laws can also be
given as isomorphisms instead of equalities, and must satisfy their
own coherence laws, leading to a tower of coherence laws. The amount
of data needed to define an \(n\)-category therefore increases
exponentially as \(n\) increases.

In addition to the difficulty in defining a weak \(n\)-category, it
is also more difficult to give proofs in a weak environment, due to
the bureaucracy of working around the various coherence isomorphisms.
Consider the proof of Eckmann-Hilton given in \cref{fig:eh}. In a
weak environment, we would hope to be able to simply replace each
equality by the appropriate isomorphism, however doing so for the
first equality in the proof would require us to give an isomorphism:
\[ \alpha \cong \alpha * \id\]
Each side of this isomorphism has a different source and target, and
hence no such isomorphism can be given in the globular setting used
in this thesis. A full proof of Eckmann-Hilton is still possible but
far more involved.

Weak categories are a more general notion than their strict
counterparts, with every strict \(n\)-category generating a
corresponding weak category by letting every coherence isomorphism be
given by the identity morphism. For \(2\)-categories, the converse is
in fact possible; every weak \(2\)-category is equivalent to a strict
\(2\)-category, allowing proofs for weak \(2\)-categories to be given
by instead proving the same property for strict \(2\)-categories.

This is no longer possible in \(n\)-categories where \(n \geq 3\). It
was shown by
\citeauthor{simpson1998homotopy}~\cite{simpson1998homotopy} that
strict \(n\)-categories do not model the homotopy structure of all
topological spaces, with the topological space \(S^2\) having no
interpretation. More concretely, we consider the morphism
\(\mathsf{EH}_{\alpha,\beta} : \alpha \star_1 \beta \to \beta \star_1
\alpha\) generated by the Eckmann-Hilton argument for scalars
\(\alpha\) and \(\beta\). In a strict \(3\)-category, this morphism
is given by the identity and so:
\[ \mathsf{EH}_{\alpha,\beta} \star_2 \mathsf{EH}_{\beta,\alpha} = \id\]
This equality does not hold in a general weak \(3\)-category (even up
to isomorphism), contradicting that each weak \(3\)-category is
equivalent to a strict \(3\)-category.

This motivates the search for semistrict definitions of
\(n\)-category: definitions where some operations are strict, yet do
not lose the expressivity of weak \(n\)-categories. For
\(3\)-categories, two such definitions have been proposed:
\begin{itemize}
  \item
    \citeauthor{joyal2006weak}~\cite{joyal2006weak,joyalcoherence}
    define a monoidal \(2\)-category (which can be viewed as a
    \(3\)-category with a single \(0\)-cell) which only has weak
    units and unitors, and is otherwise strict. They prove that all
    braided monoidal categories (weak \(3\)-categories with a unique
    \(0\)-cell and unique \(1\)-cell) can be interpreted in this
    setting as the category of endomorphisms on the weak unit morphism.
  \item Gray-categories are a form of semistrict \(3\)-categories for
    which all structure is strict except the interchanger, the
    isomorphism witnessing the interchange law.
    \citeauthor{gordon1995coherence}~\cite{gordon1995coherence} prove
    that every weak \(3\)-category is equivalent to a Gray-category.
\end{itemize}
It is non-trivial to even define such a notion of semistrict
\(n\)-category for \(n > 3\), let alone prove that it loses no
expressivity over its weak counterpart. Simpson
conjectures~\cite{simpson1998homotopy} that having only the unit laws
weak is sufficient to model all homotopy groupoids,
\(\infty\)-groupoids arising from the homotopy of topological spaces,
though it is unclear if such a definition has been given.
\citeauthor{hadzihasanovic2019representable}~%
\cite{hadzihasanovic2019representable} defines weak higher categories
based on \emph{diagrammatic sets}. It could be argued that such a
definition can model strict interchange, though the classes of
diagrams that can be composed in this theory are restricted to those
that are \emph{spherical}, which disallows horizontal composites in
the form stated above and makes comparison difficult.
\citeauthor{Batanin2013}~\cite{Batanin2013} define a notion of
\(\infty\)-category with strict units based on the language of
operads. % A key axiom in this theory is \emph{disc reduction} which
% states that composites trivialise over certain configurations of
% cells known as discs.

Definitions of semistrict \(n\)-categories which are strictly unital
and associative have also been defined, primarily inspired by the
graphical language of \emph{string diagrams}.
\citeauthor{bar2017data}~\cite{bar2017data} define \emph{quasi-strict
\(4\)-categories}, where the associativity and unitality laws hold
strictly up to equality.
\citeauthor{dorn2018associative}~\cite{dorn2018associative} defines
\emph{associative \(n\)-categories}: a definition of strictly
associative and unital \(n\)-category similarly based on geometric
principles. Associative \(n\)-categories are further studied by
Heidemann, Reutter, Tataru, and
Vicary~\cite{reutter2019high,heidemann2022zigzag,tataru2024theory},
which has recently led to the construction of the graphical proof
assistant \textsf{homotopy.io}~\cite{corbyn2024homotopy} for
manipulating higher-dimensional string diagrams. Similarly to the
case for diagrammatic sets, the composition operations in these
theories have a different form to those of strict \(n\)-categories,
making comparison difficult. The connection between these definitions
and geometry is studied by
\citeauthor{dorn2021framed}~\cite{dorn2021framed} and
\citeauthor{heidemann2023framed}~\cite{heidemann2023framed}.

\paragraph{Type theory and higher categories}

Deep links exist between higher category theory and type theory. The
identity type in Martin-Löf type theory
(\textsc{Mltt})~\cite{MARTINLOF197573} naturally leads to
higher-dimensional structure; the identity type \(s =_A t\) can be
formed for any two terms \(s\) and \(t\) of type \(A\), but this
construction can be iterated since the identity type is a type
itself, leading to higher identity types \(p =_{s =_A t} q\) for \(p,
q: s =_A t\). Operations on this type are generated by the J-rule, an
induction principle for the identity type. Independent proofs by
\citeauthor{lumsdaine2010weak}~\cite{lumsdaine2010weak} and
\citeauthor{garner2011types}~\cite{garner2011types} show that the
J-rule is sufficient to equip identity types with the appropriate
operations to form a weak \(\infty\)-groupoid.

Terms of the identity type \(s =_A t\) correspond to witnesses of the
fact that \(s\) and \(t\) are equal, or can even be viewed as proofs
of the equality. The study of these proofs as objects of study in
their own right is known as \emph{proof relevance}. Although the
axiom of uniqueness of identity proofs (UIP), which states that any
two terms of the identity type are themselves equal, is consistent
with \textsc{Mltt}, it was shown that it is not provable by
\citeauthor{hofmannstreicher}, who constructed a model of
\textsc{Mltt} where types are interpreted as \(1\)-groupoids, and
identity types are non-trivial.

The \(\infty\)-groupoidal nature of \textsc{Mltt} is embraced in
Homotopy type theory (\textsc{Hott})~\cite{hottbook}, where types are
interpreted as topological spaces. The key component of
\textsc{Hott}, the \emph{univalence axiom}, which is incompatible
with UIP, states that the identities between types are given by
equivalences between these types, which need not be unique.

The models of \textsc{Hott} are equipped with more structure than is
present in an \(\infty\)-groupoid, and are given by
\(\infty\)-toposes~\cite{shulman2019all}. In the appendices of his
thesis~\cite{brunerie2016homotopy}, \citeauthor{brunerie2016homotopy}
defines a type theory for \(\infty\)-groupoids by removing all
structure from \textsc{Mltt} which does not concern the identity
type. This theory constructs the identity type similarly to
\textsc{Mltt}, but replaces the J-rule with a rule stating that all
terms over \emph{contractible contexts} are equal.
\citeauthor{finster2017type} further refine this idea to produce the
type theory \Catt~\cite{finster2017type}, a type theory for weak
\(\infty\)-categories, using techniques from a definition of weak
\(\infty\)-categories due to
\citeauthor{maltsiniotis2010grothendieck}~\cite{maltsiniotis2010grothendieck}
which itself is based on an earlier definition of
\(\infty\)-groupoids which was given by
\citeauthor{PursuingStacks}~\cite{PursuingStacks}. It was later
shown~\cite{benjamin2021globular} that type-theoretic models of \Catt
coincide with \(\infty\)-categories defined by
\citeauthor{maltsiniotis2010grothendieck}.

The type theory \Catt is unusual, due to having no computation or
equality rules. In the current work we leverage this to define new
notions of semistrict \(\infty\)-category, by adding definitional
equality to \Catt. This equality unifies certain terms, which
correspond to operations in a weak \(\infty\)-category, causing the
semistrict behaviour of the resulting theories. This thesis develops
a framework for working with equality relations in \Catt, and uses
this to define two new type theories, \Cattsu and \Cattsua:
\begin{itemize}
  \item \Cattsu is a version of \Catt which is strictly unital. It is
    primarily generated by the \emph{pruning} reduction, a
    computation rule which removes unnecessary identities from more
    complex terms.
  \item \Cattsua is \Catt with strict unitors and associators. In
    this theory, pruning is replaced by a more general reduction
    which we call \emph{insertion}, which merges multiple composites
    into a single composite, flattening the structure of terms in the
    theory. We claim to give the first algebraic definition of an
    \(\infty\)-category where the unitality and associativity laws
    hold strictly as models of \Cattsua.
\end{itemize}
The majority of the technical content of this thesis is concerned
with proving standard metatheoretic properties of these type
theories. This includes defining a notion of computation for each
theory, given by demonstrating the existence of a confluent and
terminating reduction system, which allows these theories to be
implemented. This is used to produce interpreters for both theories,
allowing complex constructions to be checked mechanically. We
demonstrate the utility of this by formalising a proof of the
\emph{syllepsis}, a \(5\)-dimensional term witnessing a commutativity
property of the Eckmann-Hilton argument.

\clearpage
\paragraph{Overview} We now give an overview of the content contained
in each of the following chapters of the thesis.
\begin{itemize}
  \item \cref{sec:background} gives an introduction to
    \(\infty\)-category theory. It defines strict
    \(\infty\)-categories and continues to define the definition of
    weak \(\infty\)-categories due to Maltsiniotis. The chapter ends
    by giving a definition of the type theory \Catt, as defined by
    \citeauthor{finster2017type}, and describing some preliminary
    well-known constructions in \Catt.
  \item \cref{cha:gener-pres-catt} introduces a general framework for
    studying variants of \Catt with definitional equality relations
    generated from a set of rules \(\mathcal{R}\), which we name
    \Cattr. The chapter also states various properties concerning the
    metatheory of \Cattr, including specifying conditions on the set
    of equality rules \(\mathcal{R}\), under which the theory is
    well-behaved. The description of \Catt in this chapter is
    comprehensive and self-contained, although lacks some exposition
    of the previous chapter. The type theory \Cattr is accompanied by
    an Agda formalisation, which is introduced in this chapter.
  \item \cref{sec:operations-catt} takes an arbitrary well-behaved
    variant of \Cattr, and explores various constructions that can be
    formed in this setting. The primary purpose of this chapter is to
    introduce the \emph{pruning operation}, which is done in
    \cref{sec:pruning}, and the \emph{insertion operation}, which is
    introduced in \cref{sec:insertion}.
    \cref{sec:trees,sec:structured-terms} build up theory about a
    certain class of contexts represented by trees, and terms that
    appear in these contexts. This theory is vital for a complete
    understanding of insertion.
  \item In \cref{cha:cattstrict}, the type theories \Cattsu and
    \Cattsua are finally defined in \cref{sec:cattsu,sec:cattsua}
    respectively, as variants of the framework \Cattr. Preliminary
    results about both theories are proved, primarily by compiling
    results that have been stated in the previous two chapters. The
    main technical contribution of this section involves giving
    reduction systems for both theories, and giving proofs that these
    reductions systems are strongly terminating and globally
    confluent, hence making equality in these theories decidable.

    In \cref{sec:towards-nbe}, the decidability of equality is used
    to implement a typechecker for both theories \Cattsu and
    \Cattsua. The typechecker uses \emph{normalisation by evaluation}
    (NbE) to reduce terms to a canonical form where they can be
    checked for equality. The section discusses the interaction of
    NbE with \Catt, as well as discussing limitations of this
    approach in this setting.

    \cref{sec:models} discusses some properties of the models of
    these type theories, introducing a technique which we call
    \emph{rehydration}, which ``pads out'' terms of the semistrict
    theory with the necessary coherences to produce a term of \Catt
    which is equivalent to the original term. Rehydration can be seen
    as a conservativity result for the semistrict theories introduced
    at the start of the chapter. A proof of rehydration is given for
    the restricted case of terms over a certain class of context
    known as ps-contexts. This partial rehydration result is
    sufficient to determine that the semistrictness defined by
    \Cattsu and \Cattsua is a property, a model of \Catt can be a
    model of \Cattsu or \Cattsua in at most one way. We further
    explore some obstructions to rehydration in a generic context.

    The thesis ends with a discussion of further variants of \Catt
    and other options for future work.
\end{itemize}
Although results of later chapters depend on definitions and results
of the preceding chapters, a linear reading of this thesis is not
essential. A reader who is already familiar with the type theory
\Catt can safely skip \cref{sec:background}, and a reader who is only
interested in the type theory \Cattsu could read
\cref{cha:gener-pres-catt} followed by \cref{sec:pruning,sec:cattsu}.
Similarly, a reader only interested in \Cattsua can ignore any
content on the pruning construction. \cref{sec:towards-nbe} may be of
interest to a reader who is purely interested in the type-theoretic
techniques used, and not the type theory \Catt itself.

\paragraph{Statement of authorship}
The type theory \Cattsu was originally developed in collaboration with
Eric Finster, David Reutter, and Jamie Vicary, and was presented by
the author at the Logic in Computer Science conference in
2022~\cite{finster2022type}. \Cattsua will be presented at Logic in
Computer Science 2024~\cite{finster2023strictly} and was developed in
collaboration
with Eric Finster and Jamie Vicary.

The author claims the development of the framework \Cattr and its
accompanying Agda formalisation as individual contribution, as well as
the implementation of \Cattsu and \Cattsua which appears in
\cref{sec:towards-nbe}.

\chapter{Background}
\label{sec:background}

We begin with an overview of the important concepts required for the
rest of the thesis. Throughout, we will assume knowledge of various
basic concepts from computer science, as well as a basic knowledge of
category theory (including functor categories, presheaves, and
(co)limits) and type theory. The primary purpose of the following
sections is to introduce weak \(\infty\)-categories. While there are
many differing definitions of \(\infty\)-categories (see
\cite{leinster2001survey}), we focus here on models of the type
theory \Catt~\cite{finster2017type}, which are known to be equivalent
to a definition of
\citeauthor{maltsiniotis2010grothendieck}~\cite{maltsiniotis2010grothendieck}
based off an earlier definition by
\citeauthor{PursuingStacks}~\cite{PursuingStacks}, which we introduce
in \cref{sec:weak}. In \cref{sec:type-theory-catt}, we define the
type theory \Catt, similarly to how it was originally defined.

This section additionally serves as a place to introduce various
syntax and notations which will be used throughout the rest of the thesis.

\section{Higher categories}
\label{sec:higher-categories}

A higher category is a generalisation of the ordinary notion of a
category to allow higher-dimensional structure. This manifests in the
form of allowing arrows or morphisms to have their source or target
be another morphism instead of an object. In this thesis, we are
primarily concerned with \(\infty\)-categories, which are equipped
with the notion of an \(n\)-cell for each \(n \in \mathbb{N}\), where
each \((n+1)\)-cell has a source and target \(n\)-cell, and
\(0\)-cells play the role of objects in an ordinary category.

The role of objects is played by \(0\)-cells, with \(1\)-cells as the
morphisms between these objects. For \(0\)-cells \(x\) and \(y\), a
\(1\)-cell \(f\) with source \(x\) and target \(y\) will be drawn as:
\[
  \begin{tikzcd}
    x & y
    \arrow["f", from=1-1, to=1-2]
  \end{tikzcd}
\]
or may be written as \(f: x \to y\). Two cells are \emph{parallel} if
they have the same source and target. Between any two parallel
\(n\)-cells \(f\) and \(g\), we have a set of \((n+1)\)-cells between
them. A \(2\)-cell \(\alpha : f \to g\) may be drawn as:
\[
  \begin{tikzcd}
    x & y
    \arrow[""{name=0, anchor=center, inner sep=0}, "g",
    curve={height=-12pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
    curve={height=12pt}, from=1-1, to=1-2]
    \arrow["\alpha", shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
  \end{tikzcd}
\]
A \(3\)-cell \(\gamma\) between parallel \(2\)-cells \(\alpha\) and
\(\beta\) could be drawn as:
\[
  \begin{tikzcd}
    x && y
    \arrow[""{name=0, anchor=center, inner sep=0}, "f",
    curve={height=-15pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0}, "g"',
    curve={height=15pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0}, "\alpha", shift
    left=4, shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
    \arrow[""{name=3, anchor=center, inner sep=0}, "\beta"', shift
    right=4, shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
    \arrow["\gamma", shorten <=4pt, shorten >=4pt, Rightarrow,
    nfold=3, from=2, to=3]
  \end{tikzcd}
\]

Just as in ordinary \(1\)-category theory, we expect to be able to
compose morphisms whose boundaries are compatible. For \(1\)-cells,
nothing has changed, given \(1\)-cells \(f: x \to y\) and \(g : y \to
z\) we form the composition \(f * g\):
\[
  \begin{tikzcd}
    x & y & z
    \arrow[from=1-1, to=1-2, "f"]
    \arrow[from=1-2, to=1-3, "g"]
  \end{tikzcd}
\]
which has source \(x\) and target \(z\). We pause here to note that
composition will be given in ``diagrammatic order'' throughout the
whole thesis, which is the opposite of the order of function
composition yet the same as the order of the arrows as drawn above.
This is chosen as it will be common for us to draw higher-dimensional
arrows in a diagram, and rare for us to consider categories where the
higher arrows are given by functions. In an attempt to avoid
confusion, we use an asterisk (\(*\)) to represent composition of
arrows or cells in a higher category, and will use a circle
(\(\circ\)) only for function composition.

In two dimensions, there is no longer a unique composition operation.
For \(2\)-cells \(\alpha : f \to g\) and \(\beta : g \to h\), the
composite \(\alpha *_1 \beta\) can be formed as before:
% https://q.uiver.app/#q=WzAsMixbMCwwLCJcXGJ1bGxldCJdLFsyLDAsIlxcYnVsbGV0Il0sWzAsMSwiZiIsMCx7ImN1cnZlIjotNH1dLFswLDEsImgiLDIseyJjdXJ2ZSI6NH1dLFswLDEsImciLDFdLFsyLDQsIlxcYWxwaGEiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzQsMywiXFxiZXRhIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dXQ== % tex-fmt: skip
\[
  \begin{tikzcd}
    x && y
    \arrow[""{name=0, anchor=center, inner sep=0}, "f"',
    curve={height=24pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0}, "h",
    curve={height=-24pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0}, "g"{description},
    from=1-1, to=1-3]
    \arrow["\alpha", shorten <=3pt, shorten >=3pt, Rightarrow, from=0, to=2]
    \arrow["\beta", shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=1]
  \end{tikzcd}
\]

We refer to this composition as \emph{vertical composition}. The
cells \(\gamma : i \to j\) and \(\delta : k \to l\) can also be
composed in the following way:

% https://q.uiver.app/#q=WzAsMyxbMCwwLCJ4Il0sWzEsMCwieSJdLFsyLDAsInoiXSxbMCwxLCIiLDAseyJjdXJ2ZSI6LTN9XSxbMCwxLCIiLDIseyJjdXJ2ZSI6M31dLFsxLDIsIiIsMix7ImN1cnZlIjotM31dLFsxLDIsIiIsMix7ImN1cnZlIjozfV0sWzMsNCwiXFxhbHBoYSIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbNSw2LCJcXGJldGEiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV1d % tex-fmt: skip
\[
  \begin{tikzcd}
    x & y & z
    \arrow[""{name=0, anchor=center, inner sep=0}, "j",
    curve={height=-18pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0}, "i"',
    curve={height=18pt}, from=1-1, to=1-2]
    \arrow[""{name=2, anchor=center, inner sep=0}, "l",
    curve={height=-18pt}, from=1-2, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0}, "k"',
    curve={height=18pt}, from=1-2, to=1-3]
    \arrow["\gamma", shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow["\delta", shorten <=5pt, shorten >=5pt, Rightarrow, from=3, to=2]
  \end{tikzcd}
\]

This composition is called the \emph{horizontal composition}, and is
written \(\gamma *_0 \delta\). The subscript refers to the dimension
of the shared boundary in the composition, with the \(1\)-cell \(g\)
being the shared boundary in the vertical composition example and the
\(0\)-cell \(y\) being the shared boundary in the horizontal
composition example. The dimension of this shared boundary is the
\emph{codimension} of the composition.

This pattern continues with \(3\)-cells, which can be composed at
codimension \(0\), \(1\), or \(2\), as depicted below:

% https://q.uiver.app/#q=WzAsNyxbMiwwLCJcXGJ1bGxldCJdLFswLDAsIlxcYnVsbGV0Il0sWzMsMCwiXFxidWxsZXQiXSxbNSwwLCJcXGJ1bGxldCJdLFs2LDAsIlxcYnVsbGV0Il0sWzcsMCwiXFxidWxsZXQiXSxbOCwwLCJcXGJ1bGxldCJdLFsxLDAsIiIsMCx7ImN1cnZlIjotM31dLFsxLDAsIiIsMix7ImN1cnZlIjozfV0sWzIsMywiIiwwLHsiY3VydmUiOi00fV0sWzIsMywiIiwyLHsiY3VydmUiOjR9XSxbMiwzXSxbNCw1LCIiLDAseyJjdXJ2ZSI6LTN9XSxbNCw1LCIiLDIseyJjdXJ2ZSI6M31dLFs1LDYsIiIsMix7ImN1cnZlIjotM31dLFs1LDYsIiIsMix7ImN1cnZlIjozfV0sWzgsNywiIiwyLHsib2Zmc2V0IjotNSwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs4LDcsIiIsMCx7Im9mZnNldCI6NSwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs4LDcsIiIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbMTAsMTEsIiIsMix7Im9mZnNldCI6LTQsInNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbMTAsMTEsIiIsMCx7Im9mZnNldCI6NCwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFsxMSw5LCIiLDEseyJvZmZzZXQiOi00LCJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzExLDksIiIsMSx7Im9mZnNldCI6NCwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFsxMywxMiwiIiwyLHsib2Zmc2V0IjotMywic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFsxMywxMiwiIiwwLHsib2Zmc2V0IjozLCJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzE1LDE0LCIiLDIseyJvZmZzZXQiOi0zLCJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzE1LDE0LCIiLDAseyJvZmZzZXQiOjMsInNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbMTYsMTgsIlxcZ2FtbWEiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzE4LDE3LCJcXGRlbHRhIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFsyMSwyMiwiXFxnYW1tYSIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbMTksMjAsIlxcZGVsdGEiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzIzLDI0LCJcXGdhbW1hIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFsyNSwyNiwiXFxkZWx0YSIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XV0= % tex-fmt: skip
\[
  \begin{tikzcd}
    \bullet && \bullet & \bullet && \bullet & \bullet & \bullet & \bullet
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-18pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=18pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0},
    curve={height=-24pt}, from=1-4, to=1-6]
    \arrow[""{name=3, anchor=center, inner sep=0},
    curve={height=24pt}, from=1-4, to=1-6]
    \arrow[""{name=4, anchor=center, inner sep=0}, from=1-4, to=1-6]
    \arrow[""{name=5, anchor=center, inner sep=0},
    curve={height=-18pt}, from=1-7, to=1-8]
    \arrow[""{name=6, anchor=center, inner sep=0},
    curve={height=18pt}, from=1-7, to=1-8]
    \arrow[""{name=7, anchor=center, inner sep=0},
    curve={height=-18pt}, from=1-8, to=1-9]
    \arrow[""{name=8, anchor=center, inner sep=0},
    curve={height=18pt}, from=1-8, to=1-9]
    \arrow[""{name=9, anchor=center, inner sep=0}, shift left=5,
    shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow[""{name=10, anchor=center, inner sep=0}, shift right=5,
    shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow[""{name=11, anchor=center, inner sep=0}, shorten <=5pt,
    shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow[""{name=12, anchor=center, inner sep=0}, shift left=4,
    shorten <=3pt, shorten >=3pt, Rightarrow, from=3, to=4]
    \arrow[""{name=13, anchor=center, inner sep=0}, shift right=4,
    shorten <=3pt, shorten >=3pt, Rightarrow, from=3, to=4]
    \arrow[""{name=14, anchor=center, inner sep=0}, shift left=4,
    shorten <=3pt, shorten >=3pt, Rightarrow, from=4, to=2]
    \arrow[""{name=15, anchor=center, inner sep=0}, shift right=4,
    shorten <=3pt, shorten >=3pt, Rightarrow, from=4, to=2]
    \arrow[""{name=16, anchor=center, inner sep=0}, shift left=3,
    shorten <=5pt, shorten >=5pt, Rightarrow, from=6, to=5]
    \arrow[""{name=17, anchor=center, inner sep=0}, shift right=3,
    shorten <=5pt, shorten >=5pt, Rightarrow, from=6, to=5]
    \arrow[""{name=18, anchor=center, inner sep=0}, shift left=3,
    shorten <=5pt, shorten >=5pt, Rightarrow, from=8, to=7]
    \arrow[""{name=19, anchor=center, inner sep=0}, shift right=3,
    shorten <=5pt, shorten >=5pt, Rightarrow, from=8, to=7]
    \arrow["", shorten <=2pt, shorten >=2pt, Rightarrow, nfold=3, from=9, to=11]
    \arrow["", shorten <=2pt, shorten >=2pt, Rightarrow, nfold=3,
    from=11, to=10]
    \arrow["", shorten <=3pt, shorten >=3pt, Rightarrow, nfold=3,
    from=14, to=15]
    \arrow["", shorten <=3pt, shorten >=3pt, Rightarrow, nfold=3,
    from=12, to=13]
    \arrow["", shorten <=2pt, shorten >=2pt, Rightarrow, nfold=3,
    from=16, to=17]
    \arrow["", shorten <=2pt, shorten >=2pt, Rightarrow, nfold=3,
    from=18, to=19]
  \end{tikzcd}
\]
where the unlabelled arrows and objects (which are written
\(\bullet\)) are assumed to represent arbitrary potentially-distinct cells.

For every \(n\)-cell \(x\), there is an \((n+1)\)-cell \(\id(x) : x
\to x\), called the \emph{identity morphism}.

Similarly to 1-categories, \(\infty\)-categories need to satisfy
certain laws, which fall into 3 groups: associativity, unitality, and
interchange. These laws can hold strictly, meaning that they hold up
to equality, or weakly, meaning that they hold up to a
higher-dimensional isomorphism. We delay the discussion of weak
\(\infty\)-categories to \cref{sec:weak}, and begin with the
discussion of strict \(\infty\)-categories.

In these strict categories, associativity laws are the same as for
1-categories, only now a law is needed for each composition (in every
dimension and codimension). Unitality is again similar to the case
for 1-categories, except we again need unitality laws for each
composition. We note that for lower-codimensional compositions, an
iterated identity is needed. For example, given a \(2\)-cell \(\alpha
: f \to g\), the appropriate equation for left unitality of
horizontal composition is:
\[ \id(\id(x)) *_0 \alpha = \alpha \]
In general for a unit to be cancelled, it must be iterated a number
of times equal to the difference between the dimension and
codimension of the composition.

Interchange laws do not appear in 1-categories, and specify how
compositions of different dimensions interact. The first interchange
law states that for suitable \(2\)-cells \(\alpha\), \(\beta\),
\(\gamma\), and \(\delta\), that:
\[ (\alpha *_0 \gamma) *_1 (\beta *_0 \delta) = (\alpha *_1 \beta)
*_0 (\gamma *_1 \delta)\]
This can be diagrammatically depicted as:
\newsavebox{\innertop}
\savebox{\innertop}{
  \adjustbox{scale=0.8}{
    \begin{tikzcd}[ampersand replacement=\&,column sep=small]
      \bullet \& \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      curve={height=-12pt}, from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=12pt}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      curve={height=-12pt}, from=1-2, to=1-3]
      \arrow[""{name=3, anchor=center, inner sep=0},
      curve={height=12pt}, from=1-2, to=1-3]
      \arrow["\alpha", shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
      \arrow["\gamma", shorten <=3pt, shorten >=3pt, Rightarrow, from=3, to=2]
\end{tikzcd}}}
\newsavebox{\innerbot}
\savebox{\innerbot}{
  \adjustbox{scale=0.8}{
    \begin{tikzcd}[ampersand replacement=\&,column sep=small]
      \bullet \& \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      curve={height=-12pt}, from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=12pt}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      curve={height=-12pt}, from=1-2, to=1-3]
      \arrow[""{name=3, anchor=center, inner sep=0},
      curve={height=12pt}, from=1-2, to=1-3]
      \arrow["\beta", shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
      \arrow["\delta", shorten <=3pt, shorten >=3pt, Rightarrow, from=3, to=2]
\end{tikzcd}}}
\newsavebox{\innerleft}
\savebox{\innerleft}{
  \adjustbox{scale=1}{
    \begin{tikzcd}[ampersand replacement=\&,column sep=small,cramped]
      \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      controls=+(80:0.7) and +(100:0.7),, from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=0}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      controls=+(100:-0.7) and +(80:-0.7),, from=1-1, to=1-2]
      \arrow["\alpha", shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=1]
      \arrow["\beta", shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
\end{tikzcd}}}
\newsavebox{\innerright}
\savebox{\innerright}{
  \adjustbox{scale=1}{
    \begin{tikzcd}[ampersand replacement=\&,column sep=small,cramped]
      \bullet \& \bullet
      \arrow[""{name=0, anchor=center, inner sep=0},
      controls=+(80:0.7) and +(100:0.7), from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0},
      curve={height=0}, from=1-1, to=1-2]
      \arrow[""{name=2, anchor=center, inner sep=0},
      controls=+(100:-0.7) and +(80:-0.7),, from=1-1, to=1-2]
      \arrow["\gamma", shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=1]
      \arrow["\delta", shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
\end{tikzcd}}}
\[
  \begin{tikzcd}[column sep=small]
    \bullet &&&&& \bullet & {=} & \bullet &&& \bullet &&& \bullet
    \arrow[""{name=0, anchor=center, inner sep=0}, from=1-1, to=1-6]
    \arrow[""{name=1, anchor=center, inner sep=0}, draw=none,
    controls=+(90:2) and +(90:2), from=1-1, to=1-6]
    \arrow[""{name=2, anchor=center, inner sep=0}, draw=none,
    controls=+(90:-2) and +(90:-2), from=1-1, to=1-6]
    \arrow[""{name=4, anchor=center, inner sep=0}, draw=none,
    controls=+(80:1.5) and +(100:1.5), from=1-8, to=1-11]
    \arrow[""{name=5, anchor=center, inner sep=0}, draw=none,
    controls=+(100:-1.5) and +(80:-1.5), from=1-8, to=1-11]
    \arrow[""{name=6, anchor=center, inner sep=0}, draw=none,
    controls=+(80:1.5) and +(100:1.5), from=1-11, to=1-14]
    \arrow[""{name=8, anchor=center, inner sep=0}, draw=none,
    controls=+(100:-1.5) and +(80:-1.5), from=1-11, to=1-14]
    \arrow["\usebox{\innertop}"{description, inner sep = 0,xshift =
    -1.2pt}, shorten <=4pt, shorten >=4pt, Rightarrow, from=2, to=0]
    \arrow["\usebox{\innerbot}"{description, inner sep = 0,xshift =
    -1.2pt}, shorten <=4pt, shorten >=4pt, Rightarrow, from=0, to=1]
    \arrow[""{name=1, anchor=center, inner sep=0}, controls=+(90:2)
    and +(90:2), from=1-1, to=1-6]
    \arrow[""{name=2, anchor=center, inner sep=0}, controls=+(90:-2)
    and +(90:-2), from=1-1, to=1-6]
    \arrow["\usebox{\innerleft}"{description, inner sep = 0,xshift =
    -1.3pt}, shorten <=2pt, shorten >=2pt, Rightarrow, from=5, to=4]
    \arrow["\usebox{\innerright}"{description, inner sep = 0,xshift =
    -1.3pt}, shorten <=2pt, shorten >=2pt, Rightarrow, from=8, to=6]
    \arrow[controls=+(80:1.5) and +(100:1.5), from=1-8, to=1-11]
    \arrow[controls=+(100:-1.5) and +(80:-1.5), from=1-8, to=1-11]
    \arrow[controls=+(80:1.5) and +(100:1.5), from=1-11, to=1-14]
    \arrow[controls=+(100:-1.5) and +(80:-1.5), from=1-11, to=1-14]
  \end{tikzcd}
\]

There are also interchange laws for the interaction of composition
and identities; A composition of two identities is the same as an
identity on the composition of the underlying cells.

The \(\infty\)-categories that we study in this thesis will be
globular, meaning that their cells form a globular set. A globular
set can be seen as natural extension of the data of a category, whose
data can be arranged into the following diagram:
% https://q.uiver.app/#q=WzAsMixbMCwwLCJZIl0sWzEsMCwiWCJdLFswLDEsInMiLDAseyJvZmZzZXQiOi0xfV0sWzAsMSwidCIsMix7Im9mZnNldCI6MX1dXQ== % tex-fmt: skip
\[
  \begin{tikzcd}
    M & O
    \arrow["s", shift left, from=1-1, to=1-2]
    \arrow["t"', shift right, from=1-1, to=1-2]
  \end{tikzcd}
\]
where \(O\) is a set of objects, \(M\) is a set of all morphisms, and
\(s\) and \(t\) are functions assigning each morphism to its source
and target object respectively. \(2\)-cells can be added to this
diagram in a natural way:

% https://q.uiver.app/#q=WzAsMyxbMSwwLCJDXzEiXSxbMiwwLCJDXzAiXSxbMCwwLCJDXzIiXSxbMCwxLCJzXzAiLDAseyJvZmZzZXQiOi0xfV0sWzAsMSwidF8wIiwyLHsib2Zmc2V0IjoxfV0sWzIsMCwic18xIiwwLHsib2Zmc2V0IjotMX1dLFsyLDAsInRfMSIsMix7Im9mZnNldCI6MX1dXQ== % tex-fmt: skip
\[
  \begin{tikzcd}
    {C_2} & {C_1} & {C_0}
    \arrow["{s_0}", shift left, from=1-2, to=1-3]
    \arrow["{t_0}"', shift right, from=1-2, to=1-3]
    \arrow["{s_1}", shift left, from=1-1, to=1-2]
    \arrow["{t_1}"', shift right, from=1-1, to=1-2]
  \end{tikzcd}
\]
In a globular set, the source and target of any cell must be
parallel, meaning they share the same source and target. This
condition is imposed by \emph{globularity conditions}. Adding these
and iterating the process leads to the following definition.

\begin{definition}
  The category of globes \(\mathbf{G}\) has objects given by the
  natural numbers and morphisms generated from \(\mathbf{s}_n,
  \mathbf{t}_n : n \to n + 1\) quotiented by the \emph{globularity conditions}:
  \begin{align*}
    \mathbf{s}_{n+1} \circ \mathbf{s}_n &= \mathbf{t}_{n+1} \circ \mathbf{s}_n\\
    \mathbf{s}_{n+1} \circ \mathbf{t}_n &= \mathbf{t}_{n+1} \circ \mathbf{t}_n
  \end{align*}

  The category of globular sets \(\mathbf{Glob}\), is the presheaf
  category \([\mathbf{G}^{\mathrm{op}}, \mathbf{Set}]\).
\end{definition}

Unwrapping this definition, a globular set \(G\) consists of sets
\(G(n)\) for each \(n \in \mathbb{N}\), with source and target maps
\(s_n, t_n : G(n+1) \to G(n)\), forming the following diagram:
\[
  \begin{tikzcd}
    \cdots & {G(3)} & {G(2)} & {G(1)} & {G(0)}
    \arrow["{s_0}", shift left, from=1-4, to=1-5]
    \arrow["{t_0}"', shift right, from=1-4, to=1-5]
    \arrow["{s_1}", shift left, from=1-3, to=1-4]
    \arrow["{t_1}"', shift right, from=1-3, to=1-4]
    \arrow["{t_2}"', shift right, from=1-2, to=1-3]
    \arrow["{s_2}", shift left, from=1-2, to=1-3]
    \arrow[shift right, from=1-1, to=1-2]
    \arrow[shift left, from=1-1, to=1-2]
  \end{tikzcd}
\]
and satisfying the globularity conditions. A morphism of globular
sets \(F : G \to H\) is a collection of functions \(G(n) \to H(n)\)
which commute with the source and target maps.

Given a globular set \(G\), we will call the elements of \(G(n)\) the
\(n\)-cells and write \(f : x \to y\) for an \((n+1)\)-cell \(f\)
where \(s_n(f) = x\) and \(t_n(f) = y\). We further define the
\(n\)-boundary operators \(\delta_n^-\) and \(\delta_n^+\) which take
the source or target respectively of a \((n+k)\)-cell \(k\) times,
returning an \(n\)-cell.

\begin{example}
  \label{ex:disc}
  The \(n\)-disc \(D^n\) is a finite globular set given by \(Y(n)\),
  where \(Y\) is the Yoneda embedding \(\mathbf{G} \to
  \mathbf{Glob}\). \(D^n\) has no \(k\)-cells for \(k > n\), a single
  \(n\)-cell \(d_n\), and two \(m\)-cells \(d_m^-\) and \(d_m^+\) for
  \(m < n\). Every \((m+1)\)-cell of \(D^n\) has source \(d_m^-\) and
  target \(d_m^+\). The first few discs are depicted in
  \cref{fig:discs}. The Yoneda lemma tells us that a map of globular
  sets \(D^n \to G\) is the same as an \(n\)-cell of \(G\). For an
  \(n\)-cell \(x\) of \(G\), we let \(\{x\}\) be the unique map \(D^n
  \to G\) which sends \(d_n\) to \(x\).
\end{example}

\begin{figure}[h]
  \centering
  \begin{tabular}{P{3cm} P{3cm} P{3cm} P{4cm}}
    \(D^0\)&\(D^1\)&\(D^2\)&\(D^3\)\\
    {
      \begin{tikzcd}
        d_0
      \end{tikzcd}
    }&{
      \begin{tikzcd}[ampersand replacement=\&]
        d_0^- \& d_0^+
        \arrow[from=1-1, to=1-2, "d_1"]
      \end{tikzcd}
    }&{
      \begin{tikzcd}[ampersand replacement=\&]
        d_0^- \& d_0^+
        \arrow[""{name=0, anchor=center, inner sep=0}, "d_1^+",
        curve={height=-18pt}, from=1-1, to=1-2]
        \arrow[""{name=1, anchor=center, inner sep=0}, "d_1^-"',
        curve={height=18pt}, from=1-1, to=1-2]
        \arrow["d_2"', shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=0]
      \end{tikzcd}
    }&{
      \begin{tikzcd}[ampersand replacement=\&]
        d_0^- \&\& d_0^+
        \arrow[""{name=0, anchor=center, inner sep=0}, "d_1^+",
        curve={height=-25pt}, from=1-1, to=1-3]
        \arrow[""{name=1, anchor=center, inner sep=0}, "d_1^-"',
        curve={height=25pt}, from=1-1, to=1-3]
        \arrow[""{name=2, anchor=center, inner sep=0}, "d_2^-", shift
        left=12pt,Rightarrow, shorten <=5pt, shorten >=5pt, from=1,to=0]
        \arrow[""{name=3, anchor=center, inner sep=0}, "d_2^+"',
        shift right=12pt,Rightarrow, shorten <=5pt, shorten >=5pt, from=1,to=0]
        \arrow["d_3", Rightarrow, nfold = 3, shorten <=3pt, shorten
        >=3pt,from=2,to=3]
    \end{tikzcd}}
  \end{tabular}
  \caption{The first disc globular sets.}
  \label{fig:discs}
\end{figure}

\begin{remark}
  Globular sets are not the only natural extension of the data of a
  1-category. The form of this data in a definition of a higher
  category is referred to as the \emph{shape} of the cells. Notable
  alternatives to globular sets include simplicial sets, opetopic
  sets, and cubical sets.
\end{remark}

We can now give the definition of a strict \(\infty\)-category.

\begin{definition}
  A \emph{strict \(\infty\)-category} is a globular set \(G\) with
  the following operations:
  \begin{itemize}
    \item For \(m < n\), a composition \(*_m\) taking \(n\)-cells
      \(f\) and \(g\) with \(\delta_m^+(f) = \delta_m^-(g)\) and
      producing an \(n\)-cell \(f *_m g\) with:
      \begin{align*}
        s(f *_m g) &=
        \begin{cases*}
          s(f)&\text{if \(m = n - 1\)}\\
          s(f) *_m s(g)&\text{otherwise}
        \end{cases*}\\
        t(f *_m g) &=
        \begin{cases*}
          t(g)&\text{if \(m = n - 1\)}\\
          t(f) *_m t(g)&\text{otherwise}
        \end{cases*}
      \end{align*}
    \item For any \(n\)-cell \(x\), an identity \((n+1)\)-cell
      \(\id(x) : x \to x\).
  \end{itemize}
  and satisfying equalities:
  \begin{itemize}
    \item Associativity: Given \(m < n\) and \(n\)-cells \(f\),
      \(g\), and \(h\) with \(\delta_m^+(f) = \delta_m^-(g)\) and
      \(\delta_m^+(g) = \delta_m^-(h)\):
      \[ (f *_m g) *_m h = f *_m (g *_m h) \]
    \item Unitality: Given \(m < n\) and \(n\)-cell \(f\):
      \begin{align*}
        \id^{n-m}(\delta_m^-(f)) *_m f &= f\\
        f *_m \id^{n-m}(\delta_m^+(f)) &= f
      \end{align*}
    \item Composition interchange: If \(o < m < n\) and \(\alpha\),
      \(\beta\), \(\gamma\), and \(\delta\) be \(n\)-cells with
      \[\delta_m^+(\alpha) = \delta_m^-(\beta)\qquad
        \delta_m^+(\gamma) = \delta_m^-(\delta)\qquad
      \delta_o^+(\alpha) = \delta_o^-(\gamma)\]
      then:
      \[(\alpha *_o \gamma) *_m (\beta *_o \delta) = (\alpha *_m
      \beta) *_o (\gamma *_m \delta)\]
    \item Identity interchange: Let \(m < n\) and \(f\) and \(g\) be
      \(n\)-cells with \(\delta_m^+(f) = \delta_m^-(g)\). Then:
      \[\id(f) *_m \id(g) = \id(f *_m g)\]
  \end{itemize}
  A morphism of \(\infty\) categories is a morphism of the underlying
  globular sets which preserves composition and identities.
\end{definition}

There is a clear forgetful functor from the category of strict
\(\infty\)-categories to the category of globular sets, which has a
left adjoint given by taking the free strict \(\infty\)-category over
a globular set.

We end this section with an example of a non-trivial application of
the axioms of an \(\infty\)-category, known as the Eckmann-Hilton
argument. The argument shows that any two scalars (morphisms from the
identity to the identity) commute.

\begin{proposition}[Eckmann-Hilton]
  \label{prop:eh}
  Let \(x\) be an \(n\)-cell in an \(\infty\)-category and let
  \(\alpha\) and \(\beta\) be \((n+2)\)-cells with source and target
  \(\id(x)\). Then \(\alpha *_{n+1} \beta = \beta *_{n+1} \alpha\).
\end{proposition}
\begin{proof}
  The cells \(\alpha\) and \(\beta\) can be manoeuvred around each
  other as follows:
  \begin{align*}
    &\phantom{{}={}} \alpha *_{n+1} \beta \\
    &= (\alpha *_n i) *_{n+1} (i *_n \beta)&\text{Unitality}\\
    &= (\alpha *_{n+1} i) *_n (i *_{n+1} \beta)&\text{Interchange}\\
    &= \alpha *_n \beta &\text{Unitality}\\
    &= (i *_{n+1} \alpha) *_n (\beta *_{n+1} i)&\text{Unitality}\\
    &= (i *_n \beta) *_{n+1} (\alpha *_n i)&\text{Interchange}\\
    &= \beta *_{n+1} \alpha&\text{Unitality}
  \end{align*}
  where \(i = \id(\id(x))\).
\end{proof}

We give a more graphical representation of the proof in
\cref{fig:eh}, which appeared in the introduction. In this proof the
\(\alpha\) is moved to the left of \(\beta\), though we equally could
have moved it round the right, and the choice made was arbitrary.

\subsection{Pasting diagrams}
\label{sec:pasting-diagrams}

The definition of \(\infty\)-categories given in the previous section
is close in spirit to the ordinary definitions of 1-categories and
clearly demonstrates the different families of axioms present.
However, we will see in \cref{sec:weak} that these sorts of
definitions do not scale well to our eventual setting of weak higher categories.

There is a special class of (finite) globular sets known as
\emph{pasting diagrams}, sometimes known as \emph{pasting schemes}.
The elements of the free strict \(\infty\)-category on a globular set
\(G\) can instead be represented by a pasting diagram equipped with a
map into \(G\). To do this, it must be possible to obtain a canonical
composite from each pasting diagram.

Informally, we can define an \(n\)-dimensional pasting diagram to be
a finite globular set which admits a unique full composite of
dimension \(n\), where a full composite of a globular set \(G\) is an
element of the free \(\infty\)-category over \(G\) which uses all the
maximal elements. This functions as the primary intuition on the role
of pasting diagrams.

Pasting diagrams were used directly by
\citeauthor{batanin1998monoidal}~\cite{batanin1998monoidal} to give a
definition of weak \(\infty\)-categories, and will be pivotal in
\cref{sec:weak} to define the variety of \(\infty\)-categories that
\Catt is based on. A more in-depth discussion of pasting diagrams,
representations of free strict \(\infty\)-categories using them, and
their use in the definition of weak \(\infty\)-categories can be
found in \citetitle{leinster2004higher}~\cite{leinster2004higher}.

Before giving a more formal definition of pasting diagrams, we
explore some examples and non-examples. In contrast to
\citeauthor{leinster2004higher}, we consider pasting diagrams as a
full subcategory of globular sets, rather than a separate category
with a function sending each pasting diagram to a globular set.

The disc contexts introduced in \cref{ex:disc} are all examples of
pasting diagrams. The unique ``composite'' of these globular sets is
just given by their maximal element, noting that we allow a singular
cell in our informal definition of composite. The uniqueness of this
is trivial as the only possible operations we could apply are
compositions with units, which gives the same cell under the laws of
an \(\infty\)-category.

The diagrams used to graphically represent our composition operations
(of which we recall three below) are also pasting diagrams.

\[
  \begin{tikzcd}
    x & y & z
    \arrow["f", from=1-1, to=1-2]
    \arrow["g", from=1-2, to=1-3]
  \end{tikzcd}
  \qquad
  \begin{tikzcd}
    x && y
    \arrow[""{name=0, anchor=center, inner sep=0}, "f"',
    curve={height=24pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0}, "h",
    curve={height=-24pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0}, "g"{description},
    from=1-1, to=1-3]
    \arrow["\alpha", shorten <=3pt, shorten >=3pt, Rightarrow, from=0, to=2]
    \arrow["\beta", shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=1]
  \end{tikzcd}
  \qquad
  \begin{tikzcd}
    x & y & z
    \arrow[""{name=0, anchor=center, inner sep=0}, "g",
    curve={height=-18pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
    curve={height=18pt}, from=1-1, to=1-2]
    \arrow[""{name=2, anchor=center, inner sep=0}, "i",
    curve={height=-18pt}, from=1-2, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0}, "h"',
    curve={height=18pt}, from=1-2, to=1-3]
    \arrow["\alpha", shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow["\beta", shorten <=5pt, shorten >=5pt, Rightarrow, from=3, to=2]
  \end{tikzcd}
\]

The composite of these diagrams is just the composite of the two
maximal cells with the appropriate codimension.

We can also consider composites which are not binary composites of
two cells of equal dimension. For example the following globular set
is a pasting diagram:

\[
  \begin{tikzcd}
    x & y & z
    \arrow[""{name=0, anchor=center, inner sep=0}, "g",
    curve={height=-18pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
    curve={height=18pt}, from=1-1, to=1-2]
    \arrow["h", from=1-2, to=1-3]
    \arrow["\alpha", shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
  \end{tikzcd}
\]
with a composite given by \(\alpha *_0 \id(h)\). This operation is
fairly common (in fact we have already seen it in \cref{prop:eh}) and
is known as \emph{whiskering}. In this case we would say that the
composite is given by the right whiskering of \(\alpha\) with \(h\).

The 1-dimensional pasting diagrams are all given by chains of 1-cells
of the form:
\[x_0 \overset{f_0}\to x_1 \overset{f_1}\to x_2 \overset{f_2}\to
\cdots \overset{f_n}\to x_{n+1}\]
There are multiple ways to form a composite over these diagrams by
repeated binary composition, however these all have the same result
due to associativity.

Lastly we look at the following diagram, where all the \(0\)-cells
and \(1\)-cells are assumed to be distinct:

\[
  \begin{tikzcd}[column sep = large]
    \bullet & \bullet & \bullet
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-30pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=30pt}, from=1-1, to=1-2]
    \arrow[""{name=2, anchor=center, inner sep=0}, from=1-1, to=1-2]
    \arrow[""{name=3, anchor=center, inner sep=0},
    curve={height=-30pt}, from=1-2, to=1-3]
    \arrow[""{name=4, anchor=center, inner sep=0},
    curve={height=30pt}, from=1-2, to=1-3]
    \arrow[""{name=5, anchor=center, inner sep=0}, from=1-2, to=1-3]
    \arrow["\alpha", shorten <=4pt, shorten >=4pt, Rightarrow, from=1, to=2]
    \arrow["\beta", shorten <=4pt, shorten >=4pt, Rightarrow, from=2, to=0]
    \arrow["\gamma", shorten <=4pt, shorten >=4pt, Rightarrow, from=4, to=5]
    \arrow["\delta", shorten <=4pt, shorten >=4pt, Rightarrow, from=5, to=3]
  \end{tikzcd}
\]

We get a composite given by \((\alpha *_1 \beta) *_0 (\gamma *_1
\delta)\). The uniqueness of this composite is due to the interchange law.

Non-examples of pasting diagrams roughly fall into two groups: those
that do not admit a composite, and those that admit many distinct
composites. The following three globular sets fail to admit a
composite (the last is drawn in a box to emphasise that \(z\) is part
of the same globular set as \(x\), \(y\), \(f\), \(g\), and \(\alpha\)):

\[
  \begin{tikzcd}[column sep=large, row sep = small]
    & y \\
    x \\
    & z
    \arrow["f", pos=0.6, from=2-1, to=1-2]
    \arrow["g"', pos=0.6, from=2-1, to=3-2]
  \end{tikzcd}
  \qquad
  \begin{tikzcd}[column sep=large]
    x & y
    \arrow["f", curve={height=-12pt}, from=1-1, to=1-2]
    \arrow["g"', curve={height=12pt}, from=1-1, to=1-2]
  \end{tikzcd}
  \qquad
  \fbox{%
    \begin{tikzcd}[column sep=scriptsize, ampersand replacement = \&]
      x \&\& y \& z
      \arrow[""{name=0, anchor=center, inner sep=0}, "f",
      curve={height=-18pt}, from=1-1, to=1-3]
      \arrow[""{name=1, anchor=center, inner sep=0}, "g"',
      curve={height=18pt}, from=1-1, to=1-3]
      \arrow["\alpha", shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
  \end{tikzcd}}
\]
The globular set with a single \(0\)-cell \(x\), and a single
\(1\)-cell \(f : x \to x\) has too many composites: \(f\) and \(f *_0
f\) need not be equal in an \(\infty\)-category.

To describe the free \(\infty\)-category in terms of pasting diagrams
we need to be able to extract a composite from a pasting diagram, and
construct a pasting diagram from an arbitrary composite. Each pasting
diagram having a unique composite solves the former issue.

To be able to construct a pasting diagram from a composite, we wish
to equip our set of pasting diagrams itself with the structure of an
\(\infty\)-category. We therefore need our pasting diagrams to have a
notion of boundary and a notion of composition. A natural candidate
for composition is given by colimits, as \(\mathbf{Glob}\) has all
colimits due to being a presheaf category, and so it is sufficient
for our class of pasting diagrams to be closed under these specific
colimits. In fact, it is sufficient to contain a class of colimits
known as \emph{globular sums}.

\begin{definition}
  A globular category is a category \(\mathcal{C}\), equipped with a
  disc functor \(D : \mathbf{G} \to \mathcal{C}\), specifying certain
  objects as discs in the category. A \emph{globular sum} is a
  colimit of a diagram of the form:
  \[
    \begin{tikzcd}[column sep = tiny, row sep = tiny]
      {D(i_0)} && {D(i_1)} && {D(i_2)} && {D(i_n)} && {D(i_{n+1})} \\
      &&&&& \cdots \\
      & {D(j_0)} && {D(j_1)} &&&& {D(j_n)}
      \arrow["{f_0}", from=3-2, to=1-1]
      \arrow["{g_0}"', from=3-2, to=1-3]
      \arrow["{f_n}", from=3-8, to=1-7]
      \arrow["{g_n}"', from=3-8, to=1-9]
      \arrow["{f_1}", from=3-4, to=1-3]
      \arrow["{g_1}"', from=3-4, to=1-5]
    \end{tikzcd}
  \]
  Where all morphisms \(f_i\) are a composite of source maps
  (\(D(\mathbf{s}_n)\) for some \(n\)) and the morphisms \(g_i\) are
  a composite of target maps (\(D(\mathbf{t}_n)\) for some \(n\)).
  Given that the maps \(f_i\) and \(g_i\) are uniquely determined, we
  may write such a globular sum as:

  \[ D(i_0) \amalg_{D(j_0)} D(i_1) \amalg_{D(j_1)} D(i_2) \cdots
  D(i_n) \amalg_{D(j_n)} D(i_{n+ 1})\]

  A \emph{globular extension} is a globular category where all
  globular sums exist, and a morphism of globular extensions is a
  functor of the underlying categories commuting with the disc
  functors and preserving globular sums.
\end{definition}

We can now give our first definition of a pasting diagram.

\begin{definition}
  The category \(\mathbf{Glob}\) is a globular category with functor
  \(\mathbf{G} \to \mathbf{Glob}\) given by the Yoneda embedding. The
  category of \emph{pasting diagrams}, \(\mathbf{Pd}\), is the full
  subcategory containing the globular sets which are globular sums.
  The boundary of an \((n+1)\)-dimensional pasting diagram is given
  by replacing each instance of \(D^{n+1}\) by \(D^n\) in its
  globular sum representation. There are two canonical maps including
  the boundary into the original pasting diagram, whose images give
  the source and target of the pasting diagram.
\end{definition}

The category of pasting diagrams clearly forms a globular category,
with the functor \(\mathbf{G} \to \mathbf{Pd}\) sending \(n\) to
\(D^n\). It is a globular extension and is in fact the universal
globular extension; it is initial in the category of globular
extensions~\cite{Ara}.

We finish this section with one larger example.

\begin{example}
  The following depicts a \(2\)-dimensional pasting diagram.
  \[
    \begin{tikzcd}
      x & y & z & w
      \arrow[""{name=0, anchor=center, inner sep=0}, "g",
      curve={height=-18pt}, from=1-1, to=1-2]
      \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
      curve={height=18pt}, from=1-1, to=1-2]
      \arrow["h"', from=1-2, to=1-3]
      \arrow[""{name=2, anchor=center, inner sep=0}, "k",
      curve={height=-24pt}, from=1-3, to=1-4]
      \arrow[""{name=3, anchor=center, inner sep=0}, "i"',
      curve={height=24pt}, from=1-3, to=1-4]
      \arrow[""{name=4, anchor=center, inner sep=0},
      "j"{description}, from=1-3, to=1-4]
      \arrow["\alpha", shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
      \arrow["\beta", shorten <=3pt, shorten >=3pt, Rightarrow, from=3, to=4]
      \arrow["\gamma", shorten <=3pt, shorten >=3pt, Rightarrow, from=4, to=2]
    \end{tikzcd}
  \]
  This has the following globular sum decomposition:
  % https://q.uiver.app/#q=WzAsMTMsWzAsMCwieCJdLFsyLDAsInkiXSxbOCwwLCJ6Il0sWzEwLDAsInciXSxbMywxLCJ5Il0sWzQsMCwieSJdLFs2LDAsInoiXSxbNywxLCJ6Il0sWzksMF0sWzEwLDEsInoiXSxbMTIsMSwidyJdLFsxMiwwLCJ6Il0sWzE0LDAsInciXSxbMCwxLCJnIiwwLHsiY3VydmUiOi0zfV0sWzAsMSwiZiIsMix7ImN1cnZlIjozfV0sWzIsMywiaSIsMix7ImN1cnZlIjo0fV0sWzIsMywiaiIsMV0sWzUsNiwiaCIsMl0sWzQsMSwiIiwyLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV0sWzQsNSwiIiwxLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV0sWzksMTAsImoiLDFdLFsxMSwxMiwiaiIsMV0sWzExLDEyLCJrIiwxLHsiY3VydmUiOi00fV0sWzcsNiwiIiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV0sWzcsMiwiIiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoiZGFzaGVkIn19fV0sWzE0LDEzLCJcXGFscGhhIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFsxNSwxNiwiXFxiZXRhIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFsyMSwyMiwiXFxnYW1tYSIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbMjAsMywiIiwxLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwfSwibGV2ZWwiOjEsInN0eWxlIjp7ImJvZHkiOnsibmFtZSI6ImRhc2hlZCJ9fX1dLFsyMCwxMSwiIiwxLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwfSwibGV2ZWwiOjEsInN0eWxlIjp7ImJvZHkiOnsibmFtZSI6ImRhc2hlZCJ9fX1dXQ== % tex-fmt: skip
  \[
    \begin{tikzcd}[column sep=small, row sep = small]
      x && y && y && z && z & {} & w && z && w \\
      &&& y &&&& z &&& z && w
      \arrow[""{name=0, anchor=center, inner sep=0}, "g",
      curve={height=-18pt}, from=1-1, to=1-3]
      \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
      curve={height=18pt}, from=1-1, to=1-3]
      \arrow[""{name=2, anchor=center, inner sep=0}, "i"',
      curve={height=24pt}, from=1-9, to=1-11]
      \arrow[""{name=3, anchor=center, inner sep=0},
      "j"{description}, from=1-9, to=1-11]
      \arrow["h"', from=1-5, to=1-7]
      \arrow[dashed, from=2-4, to=1-3]
      \arrow[dashed, from=2-4, to=1-5]
      \arrow[""{name=4, anchor=center, inner sep=0},
      "j"{description}, from=2-11, to=2-13]
      \arrow[""{name=5, anchor=center, inner sep=0},
      "j"{description}, from=1-13, to=1-15]
      \arrow[""{name=6, anchor=center, inner sep=0},
      "k"{description}, curve={height=-24pt}, from=1-13, to=1-15]
      \arrow[dashed, from=2-8, to=1-7]
      \arrow[dashed, from=2-8, to=1-9]
      \arrow["\alpha", shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
      \arrow["\beta", shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=3]
      \arrow["\gamma", shorten <=3pt, shorten >=3pt, Rightarrow, from=5, to=6]
      \arrow[shorten <=6pt, dashed, from=4, to=1-11]
      \arrow[shorten <=6pt, dashed, from=4, to=1-13]
    \end{tikzcd}
  \]
  The source and target of the diagram are given by the isomorphic
  pasting diagrams:
  \[
    \begin{tikzcd}
      x & y & z & w
      \arrow["f"', curve={height=18pt}, from=1-1, to=1-2]
      \arrow["h", from=1-2, to=1-3]
      \arrow["i"', curve={height=24pt}, from=1-3, to=1-4]
    \end{tikzcd}
    \qquad\text{and}\qquad
    \begin{tikzcd}
      x & y & z & w
      \arrow["g", curve={height=-18pt}, from=1-1, to=1-2]
      \arrow["h", from=1-2, to=1-3]
      \arrow["k", curve={height=-24pt}, from=1-3, to=1-4]
    \end{tikzcd}
  \]
\end{example}

\subsection{Weak higher categories}
\label{sec:weak}

The \(\infty\)-categories we have defined so far have all been strict
\(\infty\)-categories, meaning that the laws are required to hold up
to equality. In ordinary \(1\)-category theory, isomorphism is
usually preferred over equality for comparing objects. Similarly,
when we have access to higher-dimensional arrows, it follows that we
can also consider isomorphisms between morphisms, and therefore
consider laws such as associativity up to isomorphism instead of equality.

Topological spaces provide one of the primary examples for where it
is useful to consider weak laws. Given a topological space \(X\), we
can define a globular set of paths and homotopies. Let the
\(0\)-cells be given by points \(x\) of the topological space, let
morphisms from \(x\) to \(y\) be given as paths \(I \to X\) (where
\(I\) is the topological interval \([0,1]\)) which send \(0\) to
\(x\) and \(1\) to \(y\), and let higher cells be given by
homotopies. The natural composition of two paths \(p\) and \(q\) is
the following path:
\[
  (p * q)(i) =
  \begin{cases*}
    p(2i)&when \(i < 0.5\)\\
    q(2i-1)&when \(i \geq 0.5\)
  \end{cases*}
\]
which effectively lines up the paths end to end. Given \(3\) paths
\(p\), \(q\), and \(r\), the compositions \((p * q) * r\) and \(p *
(q * r)\) are not identical but are equal up to homotopy, meaning the
two compositions are isomorphic. Therefore, in this case the
composition \(p * q\) does not form a strict \(\infty\)-category
structure, but rather a weak structure.

\paragraph{Weak 2-categories} We start our exploration of weak higher
categories by considering the lower dimension case of bicategories
(weak \(2\)-categories). Here, interchange must still be given by a
strict equality, as there are no non-trivial \(3\)-cells in a
\(2\)-category. However, associativity and unitality can be given by
isomorphisms known as associators and unitors:
\begin{align*}
  \alpha_{f,g,h} &: (f *_0 g) *_0 h \to f *_0 (g *_0 h)\\
  \lambda_f &: \id(x) *_0 f \to f\\
  \rho_f &: f *_0 \id(y) \to f
\end{align*}
for \(f : x \to y\), \(g : y \to z\), and \(h : z \to w\).

\begin{example}
  \label{ex:spans}
  All strict 2-categories are also bicategories. The bicategory of
  spans is an example of a bicategory which is not strict. Starting
  with a category \(\mathcal{C}\) equipped with chosen pullbacks, we
  define the bicategory of spans over \(\mathcal{C}\) to be:
  \begin{itemize}
    \item Objects are the same as \(\mathcal{C}\)
    \item Morphisms \(A\) to \(B\) are spans \(A \leftarrow C \to B\).
    \item A 2-morphism from \(A \leftarrow C \to B\) to \(A
      \leftarrow C' \to B\) is a morphism \(C \to C'\) such that the
      following diagram commutes:
      \[
        \begin{tikzcd}[row sep = small]
          & C \\
          A && B \\
          & {C'}
          \arrow[from=1-2, to=3-2]
          \arrow[from=3-2, to=2-1]
          \arrow[from=1-2, to=2-1]
          \arrow[from=1-2, to=2-3]
          \arrow[from=3-2, to=2-3]
        \end{tikzcd}
      \]
    \item Compositions and identities of 2-morphisms is given by
      composition and identities of the underlying morphisms in \(\mathcal{C}\).
    \item The identity on an object \(A\) is the span \(A \leftarrow A \to A\).
    \item Given spans \(A \leftarrow D \to B\) and \(B \leftarrow E
      \to C\), their composite is given by the pullback:
      \[
        \begin{tikzcd}[row sep=small]
          && {D \times_B E} \\
          & D && E \\
          A && B && C
          \arrow[from=2-2, to=3-1]
          \arrow[from=2-2, to=3-3]
          \arrow[from=2-4, to=3-3]
          \arrow[from=2-4, to=3-5]
          \arrow[from=1-3, to=2-2]
          \arrow[from=1-3, to=2-4]
          \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=-45},
          draw=none, from=1-3, to=3-3]
        \end{tikzcd}
      \]
    \item Associators and unitors are given by the universal property
      of the pullback.
  \end{itemize}
\end{example}

In general, there could be many possible isomorphisms between \((f *
g) * h\) and \(f * (g * h)\), and we require that the chosen
morphisms satisfy certain compatibility properties. The first is that
each of the associator, left unitor, and right unitor should be a
natural isomorphism. The second is a property known as
\emph{coherence}, saying that any two parallel morphisms built purely
from naturality moves, associators, and unitors must be equal.

For bicategories it is sufficient to give two coherence laws: the
triangle equality and pentagon equality. The triangle equality
identifies two ways of cancelling the identity in the composite \(f *
\id * g\), giving a compatibility between the left and right unitors.
It is given by the following commutative diagram:

% https://q.uiver.app/#q=WzAsMyxbMCwwLCIoZiBcXHN0YXIgXFxpZCkgXFxzdGFyIGciXSxbMiwwLCJmIFxcc3RhciAoXFxpZCBcXHN0YXIgZykiXSxbMSwxLCJmIFxcc3RhciBnIl0sWzAsMSwiXFxhbHBoYV97ZixcXGlkLGd9Il0sWzAsMiwiXFxyaG9fZiBcXHN0YXJfMCBcXGlkKGcpIiwyXSxbMSwyLCJcXGlkKGYpXFxzdGFyXzBcXGxhbWJkYV9nIl1d % tex-fmt: skip
\[
  \begin{tikzcd}
    {(f * \id) * g} && {f * (\id * g)} \\
    & {f * g}
    \arrow["{\alpha_{f,\id,g}}", from=1-1, to=1-3]
    \arrow["{\rho_f *_0 \id(g)}"', from=1-1, to=2-2]
    \arrow["{\id(f)*_0\lambda_g}", from=1-3, to=2-2]
  \end{tikzcd}
\]

The pentagon equation identifies two ways of associating \(((f * g) *
h) * k\) to \(f * (g * (h * k))\). It is given by the diagram below:
% https://q.uiver.app/#q=WzAsNSxbMSwzLCIoZiBcXHN0YXIgKGcgXFxzdGFyIGgpKSBcXHN0YXIgayJdLFswLDEsIigoZiBcXHN0YXIgZykgXFxzdGFyIGgpIFxcc3RhciBrIl0sWzIsMCwiKGYgXFxzdGFyIGcpIFxcc3RhciAoaCBcXHN0YXIgaykiXSxbNCwxLCJmIFxcc3RhciAoZyBcXHN0YXIgKGggXFxzdGFyIGspKSJdLFszLDMsImYgXFxzdGFyICgoZyBcXHN0YXIgaCkgXFxzdGFyIGspIl0sWzEsMiwiXFxhbHBoYV97ZiBcXHN0YXIgZyxoLGt9Il0sWzIsMywiXFxhbHBoYV97ZixnLGhcXHN0YXIga30iXSxbMSwwLCJcXGFscGhhX3tmLGcsaH0gXFxzdGFyXzAgXFxpZChrKSIsMl0sWzAsNCwiXFxhbHBoYV97ZixnXFxzdGFyIGgsa30iLDJdLFs0LDMsIlxcaWQoZilcXHN0YXJfMCBcXGFscGhhX3tnLGgsa30iLDJdXQ== % tex-fmt: skip
\[
  \begin{tikzcd}[column sep = -1.5em]
    && {(f * g) * (h * k)} \\
    {((f * g) * h) * k} &&&& {f * (g * (h * k))} \\
    \\
    & {(f * (g * h)) * k} && {f * ((g * h) * k)}
    \arrow["{\alpha_{f * g,h,k}}", from=2-1, to=1-3]
    \arrow["{\alpha_{f,g,h* k}}", from=1-3, to=2-5]
    \arrow["{\alpha_{f,g,h} *_0 \id(k)}"', from=2-1, to=4-2]
    \arrow["{\alpha_{f,g* h,k}}"', from=4-2, to=4-4]
    \arrow["{\id(f)*_0 \alpha_{g,h,k}}"', from=4-4, to=2-5]
  \end{tikzcd}
\]

Surprisingly, these two equations are enough to give full coherence.
For the example of spans from \cref{ex:spans}, these two equations
follow from the uniqueness of the universal morphism.

\paragraph{Weak \(\infty\)-categories} To move from weak
\(2\)-categories to weak \(3\)-categories, new coherence cells for
interchangers are added to replace the interchanger equalities, and
new equalities must be added to specify the interaction between the
interchangers and other coherence morphisms. Furthermore, the
triangle and pentagon equations from \(2\)-categories will become
isomorphisms in a weak \(3\)-category, causing more coherence
equations to be added.

As we move up in dimension, the number of coherence morphisms and
equalities required increases exponentially. A bicategory has 11
operations (1-identity, 2-identity, 1-composition, vertical
  composition, horizontal composition, left unitor (and inverse), right
unitor (and inverse), and associator (and inverse)), whereas a fully
weak tricategory already has around 51
operations~\cite{gurski2006algebraic}. These numbers are obtained by
unwrapping various subdefinitions and should be treated as
approximate. Comparisons between the size of partially weak
definitions can be found in~\cite{bar2017data}.

Because of this complexity, we look for more uniform ways to
represent the operations and axioms of an \(\infty\)-category. In
this thesis, we will work with the type theory \Catt, which is based
on a definition of \(\infty\)-categories due to
\citeauthor{maltsiniotis2010grothendieck}~\cite{maltsiniotis2010grothendieck},
which is itself based on a definition of \(\infty\)-groupoid by
\citeauthor{PursuingStacks}~\cite{PursuingStacks}. We will sketch the
ideas behind these definitions here, and give a definition of \Catt
in \cref{sec:type-theory-catt}.

The key insight behind Grothendieck's definition is that pasting
diagrams should be weakly contractible, instead of containing a
unique composite. Whereas in a strict \(\infty\)-category, each
pasting diagram effectively has 1 composite, in a weak
\(\infty\)-category there can be many operations over a pasting diagram.

These operations are assembled into a globular extension called a
\emph{coherator}. A weak \(\infty\)-groupoid is then a presheaf on
this coherator for which the opposite functor preserves globular sums
(alternatively, the dual notion of globular product could be defined,
and such a presheaf could be asked to preserve globular products).
The objects of a coherator are given by pasting diagrams, with
\(D^n\) being sent to the \(n\)-cells of the category and other
pasting diagrams being sent to composable sets of cells (as
determined by the preservation of globular sums).

Operations over a pasting diagram \(P\) in the coherator are given by
morphisms \(D^n \to P\). When we take a presheaf over this, we obtain
a function that takes an \(P\)-shaped collection of cells to a single
\(n\)-cell. Operations can be precomposed with source and target maps
\(D^{n-1} \to D^n\) to get the source and target of an operation. To
build the coherator, we start by taking the category of pasting
diagrams. The ``operations'' of this category consist solely of the
inclusions of discs into pasting diagrams, which correspond to
picking a single element from the pasting diagram. Other operations
are then built using the following guiding principle.

\begin{principle-groupoid}
  Let \(f\) and \(g\) be two parallel operations over a pasting
  diagram \(P\). Then there is an operation \(h\) over \(P\) with
  source \(f\) and target \(g\).
\end{principle-groupoid}

We define a pair of operations \(f,g : D^n \to X\) to be
\emph{parallel} if \(n = 0\) or both \(n > 0\) and \(f \circ
\mathbf{s}_{n-1} = g \circ \mathbf{s}_{n-1}\) and \(f \circ
\mathbf{t}_{n-1} = g \circ \mathbf{t}_{n-1}\). A \emph{lift} for such
a pair of parallel operations is an operation \(h : D^{n+1} \to X\)
such that \(h \circ \mathbf{s}_{n} = f\) and \(h \circ \mathbf{t}_n =
g\). Closing under this principle then amounts to inductively adding
lifts for all parallel operations, while ensuring that the category
remains a globular extension.

We start with some basic operations: Consider the pasting diagram \(A
= D^1 \amalg D^1\) given by:
\[
  \begin{tikzcd}
    x & y & z
    \arrow["a", from=1-1, to=1-2]
    \arrow["b", from=1-2, to=1-3]
  \end{tikzcd}
\] Our rule now tells us that since \(x\) and \(z\) are elements of
\(A\), that there should be an operation returning a cell with source
\(x\) and target \(z\), namely the composition of \(a\) and \(b\). In
the language of coherators, there are operations \(f, g : D^0 \to
A\), where \(f\) includes into the source of the first disc of \(A\),
and \(g\) includes into the target of the second disc of \(A\). These
are trivially parallel, and so there exists a lift \(h : D^1 \to A\),
giving 1-composition. Similarly, if we take the pasting diagram with
a single \(0\)-cell \(x\) and no other cells, then applying our rule
with \(f,g\) both being the operation returning the element \(x\)
produces an operation with source and target \(x\), the identity on \(x\).

We can generate more complicated operations with this principle,
consider pasting diagram \(B\):
\[
  \begin{tikzcd}
    x & y & z & w
    \arrow["f", from=1-1, to=1-2]
    \arrow["g", from=1-2, to=1-3]
    \arrow["h", from=1-3, to=1-4]
  \end{tikzcd}
\]
We already know the coherator contains 1-composition, and using
composition and the universal property of globular sums, we can
generate operations realising the compound composites \((f * g) * h\)
and \(f * (g * h)\). The principle then gives us an operation
returning the \(2\)-cell \((f * g) * h \to f * (g * h)\), which is of
course the associator. This one principle allows us to generate all
the structure we need, as well as structure that is arguably
unnecessary, such as ternary compositions that did not appear in the
definition of bicategory.

Unfortunately, as we have already mentioned, Grothendieck's
definition is for \(\infty\)-groupoids, where everything is
invertible, instead of \(\infty\)-categories in full generality, as
we want to study in this thesis. This can be seen by taking the
pasting diagram \(C\):
\[
  \begin{tikzcd}
    x & y
    \arrow["f", from=1-1, to=1-2]
  \end{tikzcd}
\]
and applying the rule with \(f\) returning \(y\) and \(g\) returning
\(x\), giving an operation that returns a \(1\)-cell \(f^{-1} : y \to
x\), the inverse of \(f\). The rule as we have stated it is too powerful.

Maltsiniotis' definition provides a solution to this problem by
giving a more refined version of the principle. Whereas
Grothendieck's definition treats all operations as coherences,
Maltsiniotis' definition splits operations into two classes:
compositions and equivalences. Both classes are obtained by
restricting the classes of parallel operations that admit lifts.

We begin by defining what it means for an operation to be algebraic:
\begin{definition}
  Let \(\mathcal{C}\) be a globular extension for which the canonical
  functor \(P : \mathbf{Pd} \to \mathcal{C}\) is faithful and the
  identity on objects. Then an operation \(f : D^n \to X\) in
  \(\mathcal{C}\) is \emph{algebraic} if whenever \(f = P(g) \circ
  f'\), \(g = \id\).
\end{definition}
Intuitively, an operation is algebraic when it does not factor
through any proper inclusion. Algebraicity is equivalent to requiring
that an operation makes use of all the locally maximal elements of
the pasting diagram, elements which do not appear in the source or
target of a higher-dimensional element of the diagram.

Equivalences contain the various invertible laws of our
\(\infty\)-categories such as associators, unitors, identities, and
interchangers. For two operations \(f,g : D^n \to X\) to admit a lift
under the rule for equivalences, they must both be algebraic. This
gives the following rule:

\begin{principle-category}[Equivalences]
  Let \(f\) and \(g\) be two parallel operations over a pasting
  diagram \(P\). If both \(f\) and \(g\) use all locally maximal
  variables of \(f\), then there is an operation over \(P\) with
  source \(f\) and target \(g\).
\end{principle-category}

Clearly any operations generated by this principle are invertible, as
the extra condition imposed is symmetric. For compositions, we
introduce the following asymmetric principle, recalling that pasting
diagrams are equipped with source and target inclusions, and letting
\(\partial^-(P)\) and \(\partial^+(P)\) be the images of these inclusions:

\begin{principle-category}[Composites]
  Let \(f\) and \(g\) are parallel operations over a (non-singleton)
  pasting diagram \(P\) such that \(f\) uses all locally maximal
  cells of \(\partial^-(P)\) and no cells outside of
  \(\partial^-(P)\) and \(g\) uses all locally maximal cells of
  \(\partial^+(P)\) and no cells outside of \(\partial^+(P)\). Then
  there is an operation over \(P\) with source \(f\) and target \(g\).
\end{principle-category}

The condition required to form a composite can be expressed by the
operation \(f : D^n \to P\) factoring into an algebraic map composed
with the source inclusion into \(P\), and similar for \(g\) with the
target inclusion. It can be easily checked that the inverse operation
given above does not satisfy the criteria for being an equivalence or composite.

As with Grothendieck's definition, a coherator can be made by closing
the globular extension of pasting diagrams under these restricted
principles, and then weak \(\infty\)-categories can be defined to be
presheaves on this coherator such that the opposite functor preserves
globular sums.

\begin{remark}
  We have claimed that a coherator can be formed by closing under
  adding lifts to parallel operations, though this is not precise and
  there are actually multiple ways of performing this closure that
  lead to different coherators. For example, one could add the lift
  for 1-composition twice, to get two distinct 1-composition
  operations, as long as one also added a lift between these now
  parallel operations. Grothendieck gives a general schema for
  producing coherators, and conjectures that any two coherators give
  rise to equivalent models of \(\infty\)-categories.
\end{remark}

We now turn our attention back to the proof of Eckmann-Hilton from
\cref{fig:eh}. Given a \(0\)-cell \(x\) and two scalars \(\alpha,
\beta : \id(x) \to \id(x)\), we expect the Eckmann-Hilton argument to
give us an isomorphism in a weak higher category, rather than the
equality obtained in the strict case. In fact, we immediately see
that equalities 2, 3, and 4 in the proof can be immediately replaced
by isomorphisms (interchangers and unitors).

The first and last equalities however are more problematic, although
at first we may believe that there should exist some horizontal
unitor isomorphism, upon closer inspection the two compositions do
not even have the same boundary and so are not parallel. The
composition \(\alpha *_1 \beta\) has source and target \(\id(x)\),
whereas the source of \(\alpha *_0 \id(\id(x))\) is \(\id(x) *_0 \id(x)\).

To recover the proof in a weak setting, the intermediate composites
must be composed with unitors so that they all have source and target
\(\id(x)\). To give equivalences for the first and last step, these
unitors must be moved around with naturality moves, and at a critical
point the isomorphism \(\lambda_{\id(x)} \simeq \rho_{\id(x)}\) is
required. Multiple full proofs of Eckmann-Hilton will be given in
\cref{sec:examples}. The proof of Eckmann-Hilton is vastly simpler in
the strict case, mainly due to the presence of the equation \(\id(x)
*_0 \id(x) = \id(x)\).

\subsection{Computads}
\label{sec:computads}

A free group is generated by a set, and a free category is generated
by a directed graph, and so it is a natural question what the
generating data for a free \(\infty\)-category is. We have already
seen that a free \(\infty\)-category can be generated by a globular
set, but free \(\infty\)-categories can also be generated by data
that does not form a globular set.

Consider the minimum data needed to state the Eckmann-Hilton
principle (see \cref{fig:eh} or \cref{prop:eh}). We require a single
\(0\)-cell \(x\), and two \(2\)-cells \(\alpha, \beta : \id(x) \to
\id(x)\). This data does not form a globular set as, for example, the
source of the \(2\)-cell \(\alpha\) is not in the generating data,
but is rather an operation applied to the data. We could try to
remedy this by adding a new \(1\)-cell \(f\) to the data to represent
\(\id(x)\), but then the connection between \(\id(x)\) and \(f\)
would be lost and \(f\) and \(\id(x)\) would be distinct in any free
\(\infty\)-category generated on this data.

The correct generating data for an \(\infty\)-category is a
\emph{computad}. A version for 2-categories was introduced by
\citeauthor{street1976limits}~\cite{street1976limits}, which allows a
generating \(2\)-cell to have a composite or identity as its source
or target. These were extended to strict \(\infty\)-categories by
\citeauthor{burroni1993higher}~\cite{burroni1993higher} and weak
\(\infty\)-categories by
\citeauthor{batanin1998computads}~\cite{batanin1998computads}, which
allow the source and target of an \(n\)-cell to be any \((n-1)\)-cell
of the free \(\infty\)-category generated by the lower-dimensional data.

A modern approach to computads for weak \(\infty\)-categories is
given by \citeauthor{dean2022computads}~\cite{dean2022computads},
which avoids much of the complexity of globular operads, relying only
on (mutual) structural induction. This definition of a computad is
much closer in style (and is inspired by) the type theory \Catt which
we review in \cref{sec:type-theory-catt}.

\section{The type theory \Catt}
\label{sec:type-theory-catt}

In this section we give an overview of the dependent type theory
\Catt~\cite{finster2017type}. \Catt serves as a definition of weak
\(\infty\)-categories, by defining a weak \(\infty\)-category to be a
model of the type theory (e.g.\ using categories with
families~\cite{cwf}). In \cref{cha:gener-pres-catt}, we give a more
general and comprehensive presentation of \Catt, allowing the
addition of equality relations to the type theory, pre-empting
\cref{cha:cattstrict}. In contrast, this section presents the version
of \Catt closer to the one found in the literature, and compares its
various constructions to the ideas introduced in \cref{sec:weak}.

\subsection{Syntax of \Catt}
\label{sec:syntax-catt}

\Catt has 4 classes of syntax: contexts, terms, types, and substitutions.
\begin{itemize}
  \item Contexts contain a list of variables with an associated type.
    We can consider contexts as finite computads, the generating data
    for a weak \(\infty\)-category (see \cref{sec:computads}). It is
    alternatively valid to consider contexts in \Catt as finitely
    generated \(\infty\)-categories. The set of contexts contains all
    globular sets (and hence all pasting diagrams).
  \item Terms over a context \(\Gamma\) correspond to the operations
    from \cref{sec:weak}. Terms can either be a variable, which
    corresponds to the operations which pick a single cell out of a
    globular set, or those generated by the unique constructor
    \(\mathsf{Coh}\), which correspond to the operations generated by
    lifting. A term over a context \(\Gamma\) can also be seen as an
    element of the free \(\infty\)-category generated from \(\Gamma\).
  \item Types over a context \(\Gamma\) consist of a collection of
    terms over the same context, and contain the boundary information
    for a term. Types either take the form of the constructor \(*\),
    the type of \(0\)-cells (which have no boundary data), or an
    arrow type \(\arr s A t\), where \(s\) and \(t\) are terms giving
    the source and target of the boundary and the type \(A\) gives
    lower-dimensional boundary information. This can be viewed as a
    directed version of the equality type \(s =_A t\) from
    Martin-L\"of type theory.
  \item Substitutions from a context \(\Gamma\) to a context
    \(\Delta\) are a mapping from variables of \(\Gamma\) to terms of
    \(\Delta\). These play the role of functors between the
    \(\infty\)-categories generated by \(\Gamma\) and \(\Delta\) and
    are also syntactically crucial for forming compound composites in
    the theory.
\end{itemize}

\begin{figure}[ht]
  \centering
  \begin{tabular}{Sc Sc}
    {
      \begin{prooftree}
        \hypo{\phantom{\Term}} \infer1{\emptyset : \Ctx}
      \end{prooftree}
    }
    &
    {
      \begin{prooftree}
        \hypo{\Gamma : \Ctx} \hypo{A : \Type_\Gamma}
        \infer2{\Gamma, (x : A) : \Ctx}
    \end{prooftree}}
    \\
    {
      \begin{prooftree}
        \hypo{\phantom{\Term}} \infer1{\langle \rangle : \emptyset \to \Gamma}
      \end{prooftree}
    }
    & {
      \begin{prooftree}
        \hypo{\sigma : \Delta \to \Gamma} \hypo{t : \Term_\Gamma}
        \hypo{A : \Type_\Delta}
        \infer3{\langle \sigma , t \rangle : \Delta, (x : A) \to \Gamma}
      \end{prooftree}
    }
    \\
    {
      \begin{prooftree}
        \hypo{\phantom{\Type}} \infer1{\star : \Type_\Gamma}
      \end{prooftree}
    }
    & {
      \begin{prooftree}
        \hypo{A : \Type_\Gamma} \hypo{s : \Term_\Gamma} \hypo{t : \Term_\Gamma}
        \infer3{\arr s A t : \Type_\Gamma}
      \end{prooftree}
    }
    \\
    {
      \begin{prooftree}
        \hypo{x \in \Var(\Gamma)\vphantom{\Type}} \infer1{x : \Term_\Gamma}
      \end{prooftree}
    }
    & {
      \begin{prooftree}
        \hypo{\Delta : \Ctx} \hypo{A : \Type_\Delta} \hypo{\sigma :
        \Delta \to \Gamma}
        \infer3{\Coh \Delta A \sigma : \Term_\Gamma}
      \end{prooftree}
    }

  \end{tabular}

  \vspace{-5pt}
  \caption{Syntax constructions in \Catt.}
  \label{fig:syntax}
\end{figure}

The rules for constructing each piece of syntax are given in
\cref{fig:syntax}. To simplify the notation, we may avoid writing
substitutions in a fully nested fashion, writing \(\langle \sigma , s
, t \rangle\) instead of \(\langle \langle \sigma, s \rangle, t
\rangle\), or \(\langle s \rangle\) instead of \(\langle \langle
\rangle, s \rangle\). We may also omit the subscript in the arrow
type. As opposed to the original paper on \Catt, we fibre terms,
types, and substitutions over contexts, allowing us to avoid any
problems with substitution only extending to a partial operation on
terms. We write \(\Ctx\) for the set of contexts, \(\Term_\Gamma\)
for the set of terms in a context \(\Gamma\), \(\Type_\Gamma\) for
the set of types in a context \(\Gamma\), and write \(\sigma : \Delta
\to \Gamma\) when \(\sigma\) is a substitution taking variables of
\(\Delta\) to terms of \(\Gamma\). In the literature, substitutions
are often written as going in the opposite direction. We emphasise
here that the direction of our substitution morphisms agrees with the
direction of the function from variables to terms, the direction of
the induced functor between the \(\infty\)-categories freely
generated from the domain and codomain contexts, and the direction of
arrows in a Grothendieck coherator.

We write \(\equiv\) for \emph{syntactic equality}, up to renaming of
variables and \(\alpha\)-equivalence. The various pieces of syntax
will be considered as equal up to this relation, which can be
achieved by using a de Bruijn index representation of the syntax as
we present in \cref{cha:gener-pres-catt} for the formalisation.
However, we continue to use named variables in the prose of the
thesis to aid readability, assuming that all variables in a context
are always distinct. We contrast this with the equality symbol,
\(=\), which will represent the equality derived from extra equality
rules we have placed on \Catt in \cref{sec:catt-with-equality}, and
will be referred to as \emph{definitional equality}.

The action of a substitution \(\sigma : \Delta \to \Gamma\) can be
extended from variables to all terms \(t \in \Term_\Delta\), types
\(A \in \Type_\Delta\), and substitutions \(\tau : \Theta \to
\Delta\) by mutual recursion:
\begin{align*}
  x \sub \sigma &= t&\text{if }(x \mapsto t) \in \sigma\\
  \Coh \Theta A \tau \sub \sigma &= \Coh \Theta A {\tau \bullet \sigma}\\
  \star \sub \sigma &= \star\\
  \arr s A t \sub \sigma &= \arr {s \sub \sigma} {A \sub \sigma} {t
  \sub \sigma}\\
  \langle \rangle \bullet \sigma &= \langle \rangle\\
  \langle \tau , t \rangle \bullet \sigma &= \langle \tau \bullet
  \sigma , t \sub \sigma \rangle
\end{align*}

For every context \(\Gamma\), there is an identity substitution
\(\id_\Gamma\), which sends every variable to itself, which along
with composition of substitutions above gives a category of contexts
and substitutions.

The coherence constructor \(\Coh \Delta A \sigma\) allows us to
construct lifts between parallel operations over pasting diagrams.
The context \(\Delta\) plays the role of the pasting diagram. The
type \(A\) will always be of the form \(\arr s B t\), and the terms
\(s\) and \(t\) play the role of the parallel operation (with the
  type \(\arr s B t\) being well-formed ensuring that \(s\) and \(t\)
are parallel). The substitution \(\sigma : \Delta \to \Gamma\) holds
the data of a set of arguments to the coherence, allowing compound
composites/operations to be formed and taking the role of composition
of morphisms in the coherator.

We next define the free variables of each piece of syntax. These will
be used to encode the condition of an operation being algebraic from
the theory of non-invertible coherators. Let \(\Var(\Gamma)\) denote
the variables of \(\Gamma\). For a term \(t \in \Term_\Gamma\), a
type \(A \in \Type_\Gamma\) and a substitution \(\sigma : \Delta \to
\Gamma\) we define their free variables \(\FV(t), \FV(A), \FV(\sigma)
\subseteq \Var(\Gamma)\) by mutual recursion.
\begin{align*}
  \FV(x) &= \{x\} &\text{if \(x\) is a variable}\\
  \FV(\Coh \Delta A \sigma) &= \FV(\sigma)\\
  \FV(\star) &= \{\}\\
  \FV(\arr s A t) &= \FV(s) \cup \FV(A) \cup \FV(t)\\
  \FV(\langle \rangle) &= \{\}\\
  \FV(\langle \sigma , t \rangle) &= \FV(\sigma) \cup \FV(t)
\end{align*}

The free variables of a term are often the wrong notion to use for
testing algebraicity. For example in the context \(D^1\), the term
\(d_1\) has free variables \(\{d_1\}\), whereas the unary composite
of \(d_1\), \(\Coh {D_1} {\arr {d_0^-} \star {d_0^+}} {\id_{D^1}}\),
has free variables \(\{d_0^-,d_0^+,d_1\}\). To remedy this, the
original paper considers \(\FV(t) \cup \FV(A)\), for a term \(t\) of
type \(A\). In this thesis we instead define the support of each
piece of syntax as a purely syntactic construction.

\begin{definition}
  Fix a context \(\Gamma\). The subset \(V \subseteq \Var(\Gamma)\)
  is \emph{downwards closed} if for all \((x : A) \in \Gamma\) we have:
  \[x \in V \implies \FV(A) \subseteq V\]
  The downwards closure of a set \(V\) in a context \(\Gamma\),
  \(\DC_\Gamma(V)\) can be defined by induction on the context:
  \begin{align*}
    \DC_\emptyset(\emptyset) &= \emptyset\\
    \DC_{\Gamma, x : A}(V) &=
    \begin{cases*}
      \DC_\Gamma(V)&if \(x \not\in V\)\\
      \{x\} \cup \DC_\Gamma(V \cup \FV(A))&if \(x \in V\)\\
    \end{cases*}
  \end{align*}
  The support of a term, type, or substitution is then defined as the
  downwards closure of its free variables:
  \[ \Supp(t) = \DC_\Gamma(\FV(t))\qquad \Supp(A) =
  \DC_\Gamma(\FV(A))\qquad \Supp(\sigma) = \DC_\Gamma(\FV(\sigma)) \]
  for terms \(t \in \Term_\Gamma\), types \(A \in \Type_\Gamma\), and
  substitutions \(\sigma : \Delta \to \Gamma\).
\end{definition}

We will see later (\cref{item:supp-tm-char-2}) that for well-formed
terms \(t\) of typed \(A\) that the support of \(t\) is equal to
\(\FV(t) \cup \FV(A)\) and that \(\Supp(A) = \FV(A)\) for well-formed
types. Modifying \Catt to use the support operation therefore does
not change the theory.

We lastly define the \emph{dimension} of types, contexts, and terms.
For types this is defined recursively:
\[ \dim(\star) = 0 \qquad \dim(\arr s A t) = 1 + \dim(A) \]
For contexts, we define \(\dim(\Gamma)\) to be the maximum of the
dimension of each type in \(\Gamma\). For coherences \(\Coh \Gamma A
\sigma\), the dimension is given by \(\dim(A)\), and for variables
the dimension is given by the dimension of the associated type in the context.

\subsection{Ps-contexts}
\label{sec:ps-contexts}

We need to be able to describe pasting diagrams within the theory
\Catt. As contexts model globular sets it is natural to treat pasting
diagrams as a subset of contexts. We will build pasting diagrams by
iteratively attaching discs to a context, which is done by
introducing the judgements:

\[ \Delta \vdash_{\mathsf{ps}} x : A \qquad \text{and}\qquad \Delta
\vdash_{\mathsf{ps}} \]

If the first judgement holds, then \(\Delta\) is a pasting diagram
for which a disc can be attached to the variable \(x\), called a
\emph{dangling variable}, which has type \(A\). The contexts
\(\Delta\) for which the second judgement holds are fully formed
pasting diagrams, which we call \emph{ps-contexts} (short for pasting
scheme contexts). The rules for these judgements are given in
\cref{fig:ps-context}.

We note that these rules do not just specify which globular sets are
pasting diagrams, but they also specify an ordering on the elements
of the pasting diagram, ensuring that there is a unique ps-context
for each pasting diagram. For example, the following judgement holds:
\begin{equation}
  \label[judgement]{judg:ps}
  (x : \star), (y : \star), (f : \arr x \star y), (z: \star), (g :
  \arr y \star z) \vdash_{\mathsf{ps}}
\end{equation}
However, the context:
\[(y : \star), (z : \star), (g : \arr y \star z), (x : \star), (f :
\arr x \star y)\]
represents the same globular set but is not a ps-context.

\begin{figure}[ht]
  \centering
  \begin{mathpar}
    \inferrule{ }{(x : \star) \vdash_{\mathsf{ps}} x : \star} {(\textsc{pss})}
    \and
    \inferrule{\Gamma \vdash_{\mathsf{ps}} x : A}{\Gamma, (y : A), (f
    : \arr x A y)} {(\textsc{pse})}
    \and
    \inferrule{\Gamma \vdash_{\mathsf{ps}} x : \arr s A t}{\Gamma
    \vdash_{\mathsf{ps}} t : A} {(\textsc{psd})}
    \and
    \inferrule{\Gamma \vdash_{\mathsf{ps}} x : \star}{\Gamma
    \vdash_{\mathsf{ps}}} {(\textsc{ps})}
  \end{mathpar}
  \caption{Rules for ps-contexts.}
  \label{fig:ps-context}
\end{figure}

\begin{example}
  \Cref{judg:ps} is given by the following derivation:
  \[
    \begin{prooftree}
      \hypo{ }
      \infer1[(\textsc{pss})]{(x : \star) \vdash_{\mathsf{ps}} x : \star}
      \infer1[(\textsc{pse})]{(x : \star), (y : \star), (f : \arr x
      \star y) \vdash_{\mathsf{ps}} f : \arr x \star y}
      \infer1[(\textsc{psd})]{(x : \star), (y : \star), (f : \arr x
      \star y) \vdash_{\mathsf{ps}} y : \star}
      \infer1[(\textsc{pse})]{(x : \star), (y : \star), (f : \arr x
        \star y), (z : \star), (g : \arr y \star z)
      \vdash_{\mathsf{ps}} g : \arr x \star y}
      \infer1[(\textsc{psd})]{(x : \star), (y : \star), (f : \arr x
        \star y), (z : \star), (g : \arr y \star z)
      \vdash_{\mathsf{ps}} z : \star}
      \infer1[(\textsc{ps})]{(x : \star), (y : \star), (f : \arr x
      \star y), (z : \star), (g : \arr y \star z) \vdash_{\mathsf{ps}}}
  \end{prooftree}\]

  The applications of (\textsc{pse}) allow new variables to be added
  to the context, by adding a fresh variable, and attaching a
  variable from the dangling variable to the new fresh variable. The
  rule (\textsc{psd}) encodes that if we can attach a variable to \(f
  : x \to y\), then we can also attach a variable to \(y\). The rule
  (\textsc{ps}) forces as many (\textsc{psd}) rules to be applied as
  possible before completing the derivation, ensuring that
  derivations of ps-contexts are unique.
\end{example}

We now state the following theorem, which follows immediately
from~\cite[Theorem~53]{benjamin2021globular}.

\begin{theorem}
  The set of ps-contexts is in bijection with the set of pasting diagrams.
\end{theorem}

In order to use ps-contexts as our notion of pasting diagram, we need
to be able to identify the source and target variables of each
ps-context. This will be done by specifying the dimension \(i\)
source and target of each pasting context.

More precisely, for each ps-context \(\Gamma\) and \(i \in
\mathbb{N}\), we define a ps-context \(\bound i \Gamma\) and
subcontext inclusions:
\[ \incbd i - \Gamma : \bound i \Gamma \to \Gamma \qquad
\text{and}\qquad \incbd i + \Gamma : \bound i \Gamma \to \Gamma\]
Intuitively, the context \(\bound i \Gamma\) can be constructed by
removing any variables of dimension greater than \(i\) from
\(\Gamma\), and quotienting the dimension \(i\) variables by the
(symmetric transitive closure of the) relation \(x \sim y\) if there
exists an \(f : x \to y\). The inclusions then send this quotiented
variable to the variable appearing first in the equivalence class for
the source inclusion, and the variable appearing last in the class
for the target inclusion.

These contexts and substitutions can be defined by recursion on the
context \(\Gamma\):
\begin{align*}
  \bound i {(x : \star)} &= {(x : \star)}\\
  \bound i {\Gamma, (y : A), (f : \arr x A y)} &=
  \begin{cases*}
    \bound i \Gamma&if \(i \leq \dim(A)\)\\
    \bound i \Gamma, (y : A), (f : \arr x A y)&otherwise
  \end{cases*}\\
  \incbd i \epsilon {(x : \star)} &= \langle x \rangle\\
  \incbd i \epsilon {\Gamma, (y : A) , (f : \arr x A y)} &=
  \begin{cases*}
    \mathrlap{\incbd i \epsilon \Gamma}{\phantom{\bound i \Gamma, (y
    : A), (f : \arr x A y)}}&if \(i < \dim(A)\)\\
    \incbd i - \Gamma&if \(i = \dim(A)\) and \(\epsilon = -\)\\
    \replace(\incbd i + \Gamma, y)&if \(i = \dim(A)\) and \(\epsilon = +\)\\
    \langle \incbd i \epsilon \Gamma, y, f \rangle &otherwise
  \end{cases*}
\end{align*}
where \(\epsilon \in \{-,+\}\) and \(\replace(\langle \sigma, s
\rangle, t) = \langle \sigma, t \rangle\). As it will be common to
take the boundary of \(\Gamma\) at the dimension below the dimension
of \(\Gamma\) itself, we write
\[\incbd {} \epsilon \Gamma = \incbd {\dim(\Gamma) - 1} \epsilon \Gamma\]
when \(\dim(\Gamma)\) is not zero.

In the original \Catt paper, these inclusion substitutions are not
given and instead the source and target variables are given directly
as subcontexts. It can be easily checked that the free variables of
the inclusions are equal to the subcontexts, and that the free
variable sets of these inclusions are downwards closed. It is known,
e.g.\ from~\cite[Lemma~55]{benjamin2021globular}, that these
constructions agree with the constructions of the source and target
pasting diagrams in \cref{sec:pasting-diagrams}.

We state the following well-known result (see~\cite{finster2017type})
about isomorphisms between pasting contexts.

\begin{proposition}
  \label{prop:ps-context-iso}
  Let \(\Gamma\) and \(\Delta\) be ps-contexts and suppose \(\sigma :
  \Gamma \to \Delta\) is an isomorphism. Then \(\Gamma \equiv
  \Delta\) and \(\sigma\) is the identity substitution.
\end{proposition}

\subsection{Typing for \Catt}
\label{sec:typing-catt}

We now have all the prerequisites in place to state the typing rules
for \Catt. These take the form of 4 judgements (not including the
judgements for ps-contexts introduced in \cref{sec:ps-contexts}):
\begin{alignat*}{2}
  &\Gamma \vdash&\qquad&\text{\(\Gamma \in \Ctx\) is a well-formed context.}\\
  &\Gamma \vdash A&&\text{\(A \in \Type_\Gamma\) is a well-formed
  type in context \(\Gamma\).}\\
  &\Gamma \vdash t : A &&\text{\(t \in \Term_\Gamma\) is a
  well-formed term of type \(A \in \Type_\Gamma\).}\\
  &\Gamma \vdash \sigma : \Delta &&\text{\(\sigma : \Delta \to
  \Gamma\) is a well-formed substitution.}
\end{alignat*}

The typing rules for these judgements are then given in
\cref{fig:catt-typing}. As most of these are standard we draw
attention to a couple of the key rules. The rule for arrow types
ensures that both the source and target of the arrow themselves have
the same type, namely the one given in the subscript of the arrow.
This effectively ensures the globular nature of the type theory, as
given a term \(f : \arr s {\arr x A y} t\), both the source of the
source and source of the target are \(x\), and both the target of the
source and target of the target are \(y\).

\begin{figure}[ht]
  \centering
  \begin{mathpar}
    \inferrule{ }{\emptyset \vdash}
    \and
    \inferrule{\Gamma \vdash\\ \Gamma \vdash A}{\Gamma, (x : A) \vdash}
    \and
    \inferrule{ }{\Gamma \vdash \star}
    \and
    \inferrule{\Gamma \vdash s : A \\ \Gamma \vdash A \\ \Gamma
    \vdash t : A}{\Gamma \vdash \arr s A t}
    \\
    \inferrule{ }{\Gamma \vdash \langle\rangle : \emptyset}
    \and
    \inferrule{\Gamma \vdash \sigma : \Delta\\ \Gamma \vdash t :
    A\sub\sigma}{\Gamma \vdash \langle \sigma , t \rangle : \Delta, (x : A)}
    \and
    \inferrule{(x : A) \in \Gamma}{\Gamma \vdash x : A}
    \and
    \inferrule{\Delta \vdash_{\mathsf{ps}}\\ \Delta \vdash \arr s A t
      \\ \Gamma \vdash \sigma : \Delta\\\dim(\Delta) \neq 0\\\Supp(s) =
      \Supp(\incbd {} - \Delta)\\\Supp(t) = \Supp(\incbd {} +
    \Delta)}{\Gamma \vdash \Coh \Delta {\arr s A t} \sigma : \arr {s
    \sub \sigma} {A \sub \sigma} {t \sub \sigma}}
    \and
    \inferrule{\Delta \vdash_{\mathsf{ps}}\\ \Delta \vdash \arr s A t
      \\ \Gamma \vdash \sigma : \Delta\\\Supp(s) = \Supp(t) =
    \Var(\Delta)}{\Gamma \vdash \Coh \Delta {\arr s A t} \sigma :
    \arr {s \sub \sigma} {A \sub \sigma} {t \sub \sigma}}
  \end{mathpar}
  \caption{Typing rules for \Catt.}
  \label{fig:catt-typing}
\end{figure}

There are two rules given for typing coherence, corresponding to the
two guiding principles for categories from \cref{sec:weak}. The first
rule allows composites to be typed and the second allows equivalences
to be typed. In both, the ps-context \(\Delta\) corresponds to the
pasting diagram \(P\), the terms \(s\) and \(t\) correspond to the
operations \(f\) and \(g\) over \(P\) (with the judgement \(\Delta
\vdash \arr s A t\) enforcing that they are parallel), and the
conditions involving support give the remaining side conditions.

By a straightforward mutual induction we can prove that application
of substitution to terms, types, and other substitutions preserves
typing. Therefore, the \emph{syntactic category} of \Catt can be
formed, which contains well-formed contexts as objects and
well-formed substitutions between these contexts as morphisms, which
by an abuse of notation we call \textsf{Catt}. There is a full
subcategory \(\mathsf{Catt}^{\mathsf{ps}}\), which only contains the
contexts which are ps-contexts.

\begin{theorem}
  The category \(\mathsf{Catt}^{\mathsf{ps}}\) is a coherator for
  \(\infty\)-categories.
\end{theorem}
\begin{proof}
  Follows from \cite[Theorem~73]{benjamin2021globular}, noting that
  the opposite convention for substitution is used in that paper.
\end{proof}

Thus, we immediately get that a presheaf over
\(\mathsf{Catt}^{\mathsf{ps}}\) which preserves globular products is
an \(\infty\)-category (using the Maltsiniotis definition). Further,
presheaves of this form are equivalent to type-theoretic models of
\Catt by \cite[Theorem~88]{benjamin2021globular}, meaning
type-theoretic models of \Catt are \(\infty\)-categories.

\subsection{Basic constructions}
\label{sec:basic-constructions}

We now introduce some examples of basic categorical operations in
order to give some early examples. Suppose we have terms \(a : \arr s
\star t\) and \(b : \arr t \star u\) in some context \(\Gamma\). Then
the ps-context
\[ \Delta = (x : \star), (y : \star), (f : \arr x \star y), (z :
\star), (g : \arr y \star z) \]
from \cref{judg:ps} can be used to form the 1-composite:
\[ a *_0 b = \Coh \Delta {\arr x \star z} {\langle s, t, a, u, b \rangle}\]
It is often not necessary to give all the terms in a substitution,
especially when the substitution is from a pasting diagram (or more
generally a globular set). In these cases it is sufficient to give
terms for the \emph{locally maximal} variables of the context, those
that do not appear as the source or target of another variable. For
\(\Delta\), the locally maximal variables are \(f\) and \(g\), and so
it suffices to give the substitution above as \(\langle a , b
\rangle\), with the rest of the terms being inferable.

The disc contexts \(D^n\) can be formed in \Catt as the analogue of
the disc globular sets given in \cref{ex:disc} and satisfy the
property that a substitution from a disc context \(D^n\) contains the
same data as a term and \(n\)-dimensional type. Given a term \(t\) of
type \(A\) in context \(\Gamma\), we write this substitution
\(\{A,t\} : D^{\dim(A)} \to \Gamma\). All disc contexts are ps-contexts.

Using these, the identity can be formed on a term \(t\) of type \(A\)
in \(\Gamma\):
\[\id(A,t) = \Coh {D^n} {\arr {d_n} {} {d_n}} {\{A, t\}}\]
where \(\dim(A) = n\), which is typed using the rule for
equivalences. The structure of this term changes for different values
of \(n\), and we will relate these different terms in
\cref{sec:suspension}. As before, the non-locally maximal elements of
a substitution
can be inferred, and so we may write \(\id(t)\) or \(\{t\}\) when the
type \(A\) is inferable. In \Catt, all types are inferable, though
later when we consider semistrict variations of \Catt it may be
necessary to specify the exact type we are using up to syntactic equality.

\paragraph{Standard coherences}
The composite and identity above form part of a more general
collection of coherences, which we call \emph{standard coherences}.

\begin{definition}
  Given a pasting diagram \(\Delta\), we mutually define for all
  \(n\) the \emph{standard coherence} \(\stdcoh\Delta n\), the
  \emph{standard term} \(\stdtm \Delta n\), and the \emph{standard
  type} \(\stdty \Delta n\):
  \begin{alignat*}{2}
    &\stdcoh \Delta n &&= \Coh \Delta {\stdty \Delta n} {\id_\Delta}\\
    &\stdtm \Delta n &&=
    \begin{cases}
      d^n &\text{when \(\Delta\) is the disc \(D^n\)}\\
      \stdcoh \Delta n &\text{otherwise}
    \end{cases}\\
    &\stdty \Delta 0 &&= \star\\
    &\stdty \Delta {n+1} &&= \arr {\stdtm {\bound n \Delta} n \sub
    {\incbd n - \Delta}} {\stdty \Delta n} {\stdtm {\bound n \Delta}
    n \sub {\incbd n + \Delta}}
  \end{alignat*}
  The standard type takes the standard term over each boundary of
  \(\Delta\), includes these all back into \(\Delta\) and assembles
  them into a type. When \(n = \dim(\Delta)\) we will refer to the
  standard coherence as the \emph{standard composite}.
\end{definition}

Intuitively, the standard coherence \(\stdcoh \Delta n\) is the
canonical composite in dimension \(n\) of the pasting diagram
\(\Delta\). To give this a type is needed to form the coherence, for
which the standard type \(\stdty \Delta n\) is used. The standard
term \(\stdtm \Delta n\) is used as a variant of the standard
coherence which special cases disc contexts. This avoids the standard
type containing unary composites and allows standard composites (of
non-disc contexts) to be normal forms of the reduction systems that
will be described in \cref{cha:cattstrict}.

It is immediate that the composite of \(1\)-cells \(a *_0 b\) is
given by \(\stdcoh \Delta 1\sub{\langle a , b \rangle}\) and the
identity on a term \(t\) of dimension \(n\) is given by \(\stdcoh
{D^n} {n+1}\sub{\{t\}}\). This construction can be used to generate
all the composites in the definition of a strict \(\infty\)-category.
For example the vertical composite of \(2\)-cells is the standard
composite over the context given by the diagram:
\[
  \begin{tikzcd}
    x && y
    \arrow[""{name=0, anchor=center, inner sep=0}, "f"',
    curve={height=24pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0}, "h",
    curve={height=-24pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0}, "g"{description},
    from=1-1, to=1-3]
    \arrow["\alpha", shorten <=3pt, shorten >=3pt, Rightarrow, from=0, to=2]
    \arrow["\beta", shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=1]
  \end{tikzcd}
\]
and the horizontal composite of \(2\)-cells is the standard composite over:
\[
  \begin{tikzcd}
    x & y & z
    \arrow[""{name=0, anchor=center, inner sep=0}, "g",
    curve={height=-18pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
    curve={height=18pt}, from=1-1, to=1-2]
    \arrow[""{name=2, anchor=center, inner sep=0}, "i",
    curve={height=-18pt}, from=1-2, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0}, "h"',
    curve={height=18pt}, from=1-2, to=1-3]
    \arrow["\alpha", shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow["\beta", shorten <=5pt, shorten >=5pt, Rightarrow, from=3, to=2]
  \end{tikzcd}
\]
Noting that the standard type over the above diagram has source \(f *
h\) and target \(g * i\), themselves being standard compositions
demonstrating the mutual recursive behaviour of these constructions.

\begin{remark}
  Above we gave two ps-contexts by drawing a diagram of the globular
  set that they represent. Ps-contexts fix the order that variables
  occur in and as such the mapping from ps-contexts to globular sets
  is injective. The use of diagrams to define ps-contexts is
  therefore unambiguous.
\end{remark}

\paragraph{Further examples}
The substitution component of a coherence allows operations to be
combined into compound operations. Consider the (Ps-)context given by
the following diagram:
\[\Gamma =
  \begin{tikzcd}
    s & t & u & v
    \arrow["a", from=1-1, to=1-2]
    \arrow["b", from=1-2, to=1-3]
    \arrow["c", from=1-3, to=1-4]
  \end{tikzcd}
\]
There are (at least) 3 ways to compose together the elements of this
context. We could take the unbiased ternary composite \(a * b * c =
\stdcoh \Gamma 1\sub{\langle a, b, c\rangle}\), but could also
construct either biased composite:
\begin{align*}
  (a * b) * c &= \stdcoh \Delta 1\sub{\langle \stdcoh \Delta
  1\sub{\langle a,b\rangle}, c\rangle}\\
  a * (b * c) &= \stdcoh \Delta 1\sub{\langle a, \stdcoh \Delta
  1\sub{\langle b, c\rangle}\rangle}\\
\end{align*}
Using the equivalence typing rule, we can relate these biased
composite with the following term:
\[ \alpha_{a,b,c} = \Coh \Gamma {\arr {(a * b) * c} {} {a * (b * c)}}
{\id_\Gamma}\]
which is the associator. Similarly, for a term \(f : \arr x \star
y\), unitors can be formed over the disc context \(D^1\) using the
equivalence rule:
\begin{align*}
  \lambda_f &= \Coh {D^1} {\arr {\id(d_0^-) * d_1} {} {d_1}} {\{f\}}\\
  \rho_f &= \Coh {D^1}  {\arr {d_1 * \id(d_0^-)} {} {d_1}} {\{f\}}
\end{align*}
The remainder of the operations for a 2-category can be defined
similarly, as each displays the equivalence of two terms built over a
pasting diagram. We observe that both the unitors and associator (as
well as any coherence typed with the equivalence rule) are trivially invertible.

\subsection{Suspension}
\label{sec:suspension}

To end this section, we introduce the meta-operation of
\emph{suspension}, as described for \Catt by
\citeauthor{benjamin2020type}~\cite{benjamin2020type}. Suspension
takes any piece of syntax as input and produces one with a dimension
one higher. It can be used as an aid to defining operations in \Catt,
but will also form a key part of the formal development of the
constructions described in \cref{sec:operations-catt}.

Suspension is inspired by the identically named operation on
topological spaces. Given a topological space \(X\), its suspension
\(\Sigma X\) is formed by quotienting the space \(X \times [0,1]\) by
the relation that identifies all points of the form \((x,0)\) for \(x
\in X\) and identifies points \((x,1)\) for \(x \in X\).

The suspension on a space \(X\) can be alternatively viewed as the
space containing two distinguished points \(N\) and \(S\), and a path
from \(N\) to \(S\) for each point \(x \in X\). The names \(N\) and
\(S\) stand for north and south, as the suspension of a circle can be
visualised as a globe, with \(N\) and \(S\) being the north and south
pole and each of the paths between them being a meridian.

A similar operation can be applied to globular sets. Given a globular
set \(G\), its suspension \(\Sigma G\) is obtained by shifting the
dimension of every \(n\)-cell up by one (making it into an
\((n+1)\)-cell), adding two new \(0\)-cells \(N\) and \(S\), and
letting the source of every \(1\)-cell be \(N\) and the target be
\(S\). The globularity conditions for this construction can be quickly verified.

This construction extends to all
computads~\cite{benjamin2024duamity}, and can be defined in \Catt by
mutually defining the operation on contexts, types, terms, and substitutions.

\begin{definition}
  For contexts \(\Gamma \in \Ctx\), types \(A \in \Type_\Gamma\),
  terms \(t \in \Term_\Gamma\), and substitutions \(\sigma : \Delta
  \to \Gamma\), we define their \emph{suspensions} \(\Sigma(\Gamma)
  \in \Ctx\), \(\Sigma(A) \in \Type_{\Sigma(\Gamma)}\),
  \(\Sigma(t)\in \Term_{\Sigma(\Gamma)}\), and \(\Sigma(\sigma) :
  \Sigma(\Delta) \to \Sigma(\Gamma)\) by mutual recursion.
  \begin{align*}
    \Sigma (\emptyset) &= (N : \star), (S : \star)
    &\Sigma (\Gamma, (x : A)) &= \Sigma \Gamma, (x : \Sigma A)\\
    \Sigma (\star) &= \arr N \star S
    &\Sigma (\arr s A t) &= \arr {\Sigma s} {\Sigma A} {\Sigma t}\\
    \Sigma(\langle \rangle) &= \langle N, S \rangle
    &\Sigma(\langle \sigma, x \rangle) &= \langle \Sigma(\sigma),
    \Sigma(t) \rangle\\
    \Sigma (x) &= x
    &\Sigma (\Coh \Delta A \sigma) &= \Coh {\Sigma(\Delta)}
    {\Sigma(A)} {\Sigma(\sigma)}
  \end{align*}
  where \(x\) is a variable of \(\Gamma\).
\end{definition}

The dimension shift of suspension is driven by the cases for types,
especially the case for the base type \(\star\), which returns a type
of dimension \(1\), namely \(\arr N \star S\), using the two new
variables \(N\) and \(S\). We note that the suspension of any
ps-context is also a ps-context, and in general the suspension of any
piece of well-formed \Catt syntax can be well-formed. These results
are given in \cite[Section~3.2]{benjamin2020type}, but will be proved
in \cref{sec:ruleset} in more generality.

We can now investigate the action of suspension on the operations we
have already defined. Take the context:
\[ (x : \star), (y : \star), (f : \arr x \star y), (z : \star), (g :
\arr y \star z) \]
used in \cref{sec:basic-constructions} to generate 1-composition.
Applying suspension to this context gives:
\[
  \begin{tikzcd}
    N && S
    \arrow[""{name=0, anchor=center, inner sep=0}, "x"',
    curve={height=24pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0}, "z",
    curve={height=-24pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0}, "y"{description},
    from=1-1, to=1-3]
    \arrow["f"', shorten <=3pt, shorten >=3pt, Rightarrow, from=0, to=2]
    \arrow["g"', shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=1]
  \end{tikzcd}
\]
which is the context used to generate vertical 2-composition.
Furthermore, applying suspension directly to 1-composition operation
forms the vertical 2-composition operation.

The suspension of each disc context \(D^n\) is (up to
\(\alpha\)-renaming) \(D^{n+1}\). It can be checked that applying
suspension to the identity operation for \(n\)-dimensional terms
returns the identity operation for \((n+1)\)-dimensional terms.
Repeating this logic, all identity operations can be obtained as
iterated suspensions of the identity for \(0\)-cells. The following
more general result about standard coherences holds:

\begin{proposition}
  The following syntactic equalities hold:
  \[\Sigma(\stdcoh \Delta n) = \stdcoh {\Sigma(\Delta)} {n+1}\qquad
    \Sigma(\stdtm \Delta n) = \stdtm {\Sigma(\Delta)} {n+1}\qquad
  \Sigma(\stdty \Delta n) = \stdty {\Sigma(\Delta)} {n+1}\]
  for all ps-contexts \(\Delta\) and \(n \in \mathbb{N}\).
\end{proposition}

The proof of these results is delayed to \cref{sec:operations-catt},
where we will have more tools for dealing with these constructions.

\chapter{A formalised presentation of \Catt with equality}
\label{cha:gener-pres-catt}

The main purpose of this chapter will be to define the family of type
theories \Cattr, which extend the base type theory \Catt with a
specified set \(\mathcal{R}\) of equality rules. These equality rules
equate various terms of the theory, which unifies the corresponding
operations their models, allowing us in \cref{cha:cattstrict} to
generate type theories that model semistrict categories, categories
where some but not all structure is strictified.

This chapter will also introduce the Agda
formalisation~\cite{alex_rice_2024_10964565} which accompanies this
thesis, which compiles with Agda v2.6.4 and standard library v2.0.
The formalisation implements the syntax and typing judgements of
\Cattr, and contains proofs of most results in this chapter and
\cref{sec:operations-catt}. By formalising \Cattr, instead of the
more specific type theories \Cattsu and \Cattsua introduced in
\cref{sec:cattsu,sec:cattsua}, the formalisation of many results can
be applied to both type theories. This also allows these results to
be applied to any future type theories of this form.

A dependency graph of the formalisation is given in
\cref{fig:dep-graph}, and an online version of this graph can be
found at \url{https://alexarice.github.io/catt-agda/dep-graph.svg}
for which each node is a clickable link to an HTML version of the
code. This graph was generated by processing the dependency graph
output of Agda with the tool \textsf{sd-visualiser}~\cite{sd-visualiser}.

\section{Extended substitution}
\label{sec:extend-subst}

\Cattr uses the same syntax as \Catt with one exception. In \Cattr we
make a natural generalisation to substitutions, which will allow more
operations to be defined for working with the suspension operation
introduced in \cref{sec:suspension}. Unfortunately, the full utility
of this generalisation will not be realised until
\cref{sec:structured-terms}, but we choose to introduce it here as it
forms a core part of the syntax, and requires little modification to
the rules of the type theory.

We recall that the suspension operation \(\Sigma\) acts on contexts,
substitutions, types, and terms. Given a substitution \(\sigma :
\Delta \to \Gamma\), its suspension \(\Sigma(\sigma)\) has domain
\(\Sigma(\Delta)\) and codomain \(\Sigma(\Gamma)\). When we define
trees and tree labellings in \cref{sec:operations-catt}, which will
be used to define the insertion operation in \cref{sec:insertion}, we
will need to be able to define substitutions from suspended contexts
to arbitrary contexts. More generally, we would like to be able to
describe substitutions of the form:
\[ \Sigma^n(\Delta) \to \Gamma\]
where \(\Sigma^n(\Delta)\) is the operation that applies suspension
\(n\) times to \(\Delta\).

Consider the data contained in a substitution \(\tau : \Sigma(\Delta)
\to \Gamma\). There are two terms \(N \sub \tau\) and \(S \sub \tau\)
of type \(\star\), and then a term for each variable of \(\Delta\).
Temporarily ignoring the typing conditions for substitutions, we see
that the data is equivalent to a substitution from \(\Delta\) to
\(\Gamma\) and two additional terms.

If we now consider a substitution \(\tau : \Sigma(\Sigma(\Delta)) \to
\Gamma\), we notice that there is a term in \(\Gamma\) for each
variable of \(\Delta\), as well as two terms \(s = N \sub \tau\) and
\(t = S \sub \tau\) for the outer suspension and terms \(u = N' \sub
\tau\) and \(v = S' \sub \tau\) for the inner suspension. As before,
the terms \(s\) and \(t\) should have type \(\star\), but the terms
\(u\) and \(v\) should have type \(\arr s \star t\). We note that
this is the exact condition needed for \(\arr u {\arr s \star t} v\)
to be a well-formed type. This motivates the notion of an
\emph{extended substitution}, which is obtained by equipping a
substitution with a type.

We have not yet determined the typing conditions required on the
substitution part of these extended substitutions. We return to the
example of a substitution \(\tau : \Sigma^2(\Delta) \to \Gamma\), and
suppose that \(\Delta\) has a variable \(x\) of type \(\star\). In
\(\Sigma^2(\Delta)\), \(x\) has the type \(\arr {N'} {\arr N \star S}
{S'}\), and so \(x\) should be sent to a term of type \(\arr u {\arr
s \star t} v\), the type portion of the extended substitution. In a
substitution \(\sigma : \Delta \to \Gamma\), \(x\) would be sent to a
term of type \(\star \sub \sigma\), which suggests that \(\star \sub
\sigma\) should be redefined to send \(\star\) to the type part of
the extended substitution.

This one change to the application of substitution to types is
sufficient to generalise from substitutions to extended
substitutions. An extended substitution \(\sigma : \Delta \to
\Gamma\) then has the following intuition: The substitution part
specifies where each variable in \(\Delta\) should be sent, and the
type part specifies where the base type \(\star\) should be sent. The
other cases for the application of substitution extend this to all
terms, types, and (extended) substitutions as before. The extended
substitution \(\sigma\) then represents a standard substitution
\(\Sigma^n(\Delta)\) to \(\Gamma\), where \(n\) is the dimension of
the type part of \(\sigma\). Hence, a regular substitution can be
recovered as an extended substitution with type part \(\star\).

We modify the syntax of \Catt as follows, and will refer to these
extended substitutions simply as substitutions, as extended
substitutions are a direct generalisation of substitutions, and the
notion of substitution is still recoverable by setting the type part
to \(\star\):
\begin{itemize}
  \item Substitutions will now be fibred over a type of their
    codomain context, which we will write \(\sigma : \arr \Delta A
    \Gamma\) where \(A \in \Type_\Gamma\). We note that this allows
    us to specify that \(\sigma\) is a regular substitution by
    writing \(\sigma : \arr \Delta \star \Gamma\).
  \item The constructor \(\langle\rangle\) is removed, and is
    replaced by the constructor \(\langle A \rangle : \arr \emptyset
    A \Gamma\), where \(A \in \Type_\Gamma\). Adding a term to a
    substitution preserves the type of the substitution. As before we
    may write a substitution \(\langle \langle \langle A \rangle, s
    \rangle, t \rangle\) as \(\langle A , s, t\rangle\). We let
    \(\FV(\langle A \rangle) = \FV(A)\).
  \item An operation \(\ty(\sigma)\) is introduced that returns the
    type portion of a substitution. For \(\sigma : \arr \Delta A
    \Gamma\), we have \(\ty(\sigma) = A\).
  \item Coherences \(\Coh \Delta A \sigma \in \Term_\Gamma\) are
    restricted so that \(\sigma\) is a regular substitution. In other
    words \(\ty(\sigma)\) must be \(\star\) for \(\sigma\) to appear
    in a substitution. While this condition could be dropped, it is
    convenient to keep the same operations as \Catt.
\end{itemize}

To witness the equivalence of extended substitutions \(\Delta \to
\Gamma\) and regular substitutions \(\Sigma^n(\Delta) \to \Gamma\),
we introduce new operations.

\begin{definition}
  For a substitution \(\sigma : \arr {\Delta} {\arr s A t} \Gamma\),
  we define its \emph{unrestriction}:
  \[\unrestrict\sigma : \arr {\Sigma(\Delta)} A \Gamma\]
  by induction on the length of \(\Delta\):
  \begin{align*}
    \unrestrict \langle \arr s A t \rangle &= \langle A, s, t \rangle\\
    \unrestrict \langle \sigma' , u \rangle &= \langle \unrestrict
    \sigma' , u \rangle
  \end{align*}
  The unrestrict operation simply moves two terms from the type part
  of the substitution into the main body of the substitution.
\end{definition}

To define the second operation, we need to first specify the changes
to application of substitution:
\begin{itemize}
  \item The composition of substitutions takes substitutions \(\sigma
    : \arr \Theta A \Delta\) and \(\tau : \arr \Delta B \Gamma\) to a
    substitution \(\sigma \bullet \tau : \arr \Theta {A \sub \tau} \Gamma\).
  \item For a substitution \(\sigma : \arr \Delta A \Gamma\), we
    define \(\star \sub{\sigma} = A\).
  \item As the substitution in a coherence must have type \(\star\),
    we define the application of an extended substitution \(\tau :
    \arr \Delta {\arr s A t} \Gamma\) to a coherence as:
    \[ \Coh \Theta A \sigma \sub \tau = \Coh {\Sigma(\Theta)}
    {\Sigma(A)} {\Sigma(\sigma)} \sub {\unrestrict \tau}\]
    The case for applying a regular substitution to a coherence
    remains unchanged.
\end{itemize}

We can now define an inverse to the unrestriction operation.

\begin{definition}
  For a substitution \(\sigma : \arr {\Sigma(\Delta)} A \Gamma\), its
  \emph{restriction}
  \[ \restrict \sigma : \arr \Delta {\arr {N \sub \sigma} A {S \sub
  \sigma}} \Gamma \]
  is defined by induction on the length of \(\Delta\):
  \begin{align*}
    \restrict \langle A, s, t \rangle &= \langle \arr s A t \rangle\\
    \restrict \langle \sigma', u \rangle &= \langle \restrict \sigma', u \rangle
  \end{align*}
  Inversely to the unrestrict operation, the restrict operation moves
  two terms into the type part of the substitution.
\end{definition}

As restriction and unrestriction cancel each other, the suspension of
the substitution \(\sigma : \arr \Delta \star \Gamma\) can be
factored into \((\unrestrict \circ (\restrict \circ \Sigma))
(\sigma)\). We observe that the second part of this composition,
\(\restrict \circ \Sigma\), is the operation that simply applies the
suspension to each term in the substitution as well as the type of
the substitution. This motivates the final definition of this section.

\begin{definition}
  Let the \emph{restricted suspension} of a substitution \(\sigma :
  \arr \Delta A \Gamma\) be a substitution
  \[\Sigma'(\sigma) : \arr \Delta {\Sigma(A)} {\Sigma(\Gamma)}\]
  defined inductively by the equations:
  \begin{align*}
    \Sigma'(\langle A \rangle) &= \langle \Sigma(A)\rangle \\
    \Sigma'(\langle \sigma' , t \rangle) &= \langle \Sigma'(\sigma'),
    \Sigma(t) \rangle
  \end{align*}
  The suspension of a substitution \(\tau : \arr \Delta \star
  \Gamma\) can be defined by \(\Sigma(\tau) = \unrestrict\Sigma'(\tau)\).
\end{definition}
For the rest of the thesis and the formalisation, the suspension on a
substitution is defined as the composition of unrestriction and
restricted suspension.

\section[\texorpdfstring{\Cattr}{Cattr}: \Catt with
equality]{\boldmath\texorpdfstring{\Cattr}{Cattr}: \Catt with equality}
\label{sec:catt-with-equality}

This section will define the type theory \Cattr, a variation of \Catt
with specified equality rules. This section, in addition to the
following sections in this chapter, will be used to motivate certain
choices in the formalisation. All the preliminary definitions as well
as syntax, typing, and equality rules are assembled in \cref{fig:cattr}.

\subsection{Syntax}
\label{sec:syntax}

The syntax of \Cattr is based on the syntax of \Catt with the changes
specified in \cref{sec:extend-subst}. This creates a dependence chain
of needing to define the base syntax before suspension can be
defined, and needing to define suspension before application of
substitution can be defined. In the formalisation these are defined
in the following files:
\begin{itemize}
  \item The core syntax is defined in \module{Catt.Syntax.Base}.
  \item Suspension is defined in \module{Catt.Suspension}.
  \item Other syntactic operations are defined in
    \module{Catt.Syntax}, which re-exports the core syntax.
\end{itemize}
To avoid any issues with \(\alpha\)-equivalence, especially as we
have terms that contain contexts, we work with de Bruijn indices
throughout the formalisation. This means that a context is simply a
vector of types, a fixed length list, which are given a nicer syntax.
Variables are then simply bounded natural numbers, represented by the
sets \(\mathsf{Fin}_n\), where \(\mathsf{Fin}_n\) is the set
\(\{0,\dots,n-1\}\). Given a context \(A , B , C\), the variables
over this context are simply \(\mathsf{var\ 0}\), which has type
\(C\), \(\mathsf{var\ 1}\), which has type \(B\), and
\(\mathsf{var\ 2}\), with type \(A\). We note that \(3\) is not in
\(\mathsf{Fin}_3\), and so \(\mathsf{var\ 3}\) is not a term of this
context. Hence, we do not need to deal with unknown variables when
applying substitutions. We will still make use of variable names in
this text to aid readability, and will ignore any potential problems
that could arise from this, knowing that the results are formalised
in a setting where they do not appear.

The formalisation also differs from the presentation in the texts by
the way that the various notions of syntax are fibred. We fibre
contexts by a natural number representing their length, and then
fibre terms, types, and substitutions over these lengths instead of
fibring them over the contexts. We then get the following 4 syntactic
classes defined as mutually inductive families, where \(\mathcal{U}\)
is a type universe:
\[ \funcn{Catt.Syntax.Base}{Ctx}{\Ctx} : \mathbb{N} \to \mathcal{U}
  \quad \funcn{Catt.Syntax.Base}{Ty}\Type : \mathbb{N} \to \mathcal{U}
  \quad \funcn{Catt.Syntax.Base}{Tm}\Term : \mathbb{N} \to \mathcal{U}
  \quad \funcn{Catt.Syntax.Base}{Sub}\Sub : (n\ m : \mathbb{N}) \to
\Type_m \to \mathcal{U}\]
This decision was made purely for convenience, by fibring over
natural numbers instead of contexts, we sometimes avoid the need for
providing more explicit arguments to syntactic constructions. It
comes with drawback that the context must be provided for certain
operations, such as the support of a piece of syntax, or the
dimension of a term.

One place an explicit argument can be avoided is when defining the
weakening of a piece of syntax, an operation witnessing that for a
piece of syntax living in a context \(\Gamma\), there is a copy
living in \(\Gamma , A\) for any \(A\). These operations are defined
in \module{Catt.Syntax} and take the following form, where we re-use
the name \(\wk\) here as an abuse of notation:
\[ \funcn{Catt.Syntax}{wk-tm}{\wk} : \Term_{\Gamma} \to
  \Term_{\Gamma, A}\quad\funcn{Catt.Syntax}{wk-ty}{\wk} :
  \Type_{\Gamma} \to \Type_{\Gamma, A}\quad
  \funcn{Catt.Syntax}{wk-sub}{\wk} : (\arr \Gamma B \Delta) \to (\arr
{\Gamma} {\wk(B)} {\Delta, A}) \]
If terms are fibred over contexts then this type \(A\) must often be
specified, though with the fibring over context length this is no
longer necessary. When using de Bruijn indices, this operation is no
longer the identity on terms, as each variable must be incremented
due to the index in a variable counting from the end of the context.
One might ask why de Bruijn levels (which index from the start of the
context) were not used instead, but this would not solve our problem
as \(\mathsf{Fin}_n\) is not a subtype of \(\mathsf{Fin}_{n+1}\) in
Agda. Furthermore, using de Bruijn levels would cause the
substitution application introduced in \cref{sec:syntax-catt} (and
expanded in \cref{sec:extend-subst}) to compute poorly, due to the
way substitutions are defined. The definition of weakening is given
in \cref{fig:wk}.

Weakening can be used to give a short inductive definition of the
identity substitution, a substitution \(\id_\Gamma : \Gamma \to
\Gamma\) which sends every variable to itself. On the inductive case
\(\id_{\Gamma, (x : A)}\), it is clear that the variable \(x\) should
be sent to \(x\), but the constructor for substitutions also requires
a substitution \(\Gamma \to \Gamma, (x : A)\). This can be obtained
by weakening a recursive call to the identity on \(\Gamma\).
Similarly, an inclusion \(\Gamma \to \Gamma, (x : A)\) can be defined
as \(\wk(\id_\Gamma)\), and applying this substitution is the same
operation as weakening.

To begin proving syntactic properties of \Cattr, we need a notion of
syntactic equality. This will be written \(\Gamma \equiv \Delta\) for
contexts \(\Gamma\) and \(\Delta\), and similarly for terms \(s\) and
\(t\), types \(A\) and \(B\), and substitutions \(\sigma\) and
\(\tau\). It is given by \(\alpha\)-equivalence, and so we would hope
that the formalisation could leverage the use of de Bruijn indices to
use the in-built equality type for syntactic equality. This is too
restrictive however, there will be many times when we want to compare
two terms of differing context length (in practice this context
length will be propositionally equal, instead of definitionally equal).

Therefore, four syntactic equality relations are defined mutually
inductively on the constructors of each piece of syntax in
\module{Catt.Syntax.Properties}. These definitions can easily be
heterogeneous, allowing two terms \(s : \Term_n\) and \(t : \Term_m\)
to be compared. Unfortunately, using these comes at the cost of large
amounts of boilerplate, as these inductively defined equalities do
not come equipped with the J-rule, and so it must be manually proved
that each operation respects syntactic equality. An example of such a
function is
\funcn{Catt.Syntax.Properties}{wk-tm-≃}{wk-tm-\(\simeq\)}, which
states that the weakenings of two syntactically equal terms are
syntactically equal.

\module{Catt.Syntax.Properties} contains many of the basic properties
about the syntax of \Cattr, including:
\begin{itemize}
  \item Syntactic equality is decidable.
  \item Syntactic equality is propositional, there is at most one
    proof of \(s \equiv t\).
  \item Functoriality of suspension.
  \item Interaction of weakening with substitution application. We
    have \(\wk(s) \sub {\langle \sigma , t \rangle} \equiv s \sub
    \sigma\) and \(s \sub {\wk(\sigma)} \equiv \wk(s \sub \sigma)\)
    and equivalent lemmas for the application of substitution to
    types and substitutions.
\end{itemize}
It also contains the following proposition.

\begin{proposition}
  \label{prop:categorical}
  Application of substitution is associative and unital with respect
  to the identity substitution. More precisely, given substitutions
  \(\sigma : \arr \Theta A \Delta\) and \(\tau : \arr \Delta B
  \Gamma\), the following equalities hold:
  \begin{mathpar}
    A \sub \sigma \sub \tau \equiv A \sub {\sigma \bullet \tau} \and
    A \sub \id_\Theta \equiv A\\
    t \sub \sigma \sub \tau \equiv t \sub {\sigma \bullet \tau} \and
    t \sub \id_\Theta \equiv t\\
    (\mu \bullet \sigma) \bullet \tau \equiv \mu \bullet (\sigma
    \bullet \tau) \and \mu \bullet \id_\Theta \equiv \mu \and \id_\Xi
    \bullet \mu \equiv \mu
  \end{mathpar}
  for types \(A \in \Type_\Theta\), terms \(t \in \Term_\Theta\), and
  substitutions \(\mu : \arr \Xi C \Theta\).
\end{proposition}

\begin{proof}
  The last equation is a simple induction on \(\mu\) (and the context
  \(\Xi\)). Both the unitality equations and associativity equations,
  as with the vast majority of syntactic proofs, are given by mutual
  induction on types, terms, and substitutions. The only difficult case is:
  \[ \Coh \Theta C \mu \sub \sigma \sub \tau \equiv t \sub {\sigma
  \bullet \tau} \]
  where the type part of \(\sigma: \arr \Theta A \Delta\) or \(\tau :
  \arr \Delta B \Gamma\) is not \(\star\). First suppose \(B = \arr s
  {B'} t\) but \(A = \star\):
  \begin{align*}
    \Coh \Theta C \mu \sub \sigma \sub \tau
    &\equiv \Coh \Theta C {\mu \bullet \sigma} \sub \tau\\
    &\equiv \Coh {\Sigma(\Theta)} {\Sigma(C)} {\Sigma(\mu \bullet
    \sigma)} \sub {\unrestrict \tau}\\
    &\equiv \Coh {\Sigma(\Theta)} {\Sigma(C)} {\Sigma(\mu) \bullet
    \Sigma(\sigma)} \sub {\unrestrict \tau}\\
    &\equiv \Coh {\Sigma(\Theta)} {\Sigma(C)} {\Sigma(\mu)} \sub
    {\Sigma(\sigma) \bullet \unrestrict \tau}\\
    &\equiv \Coh {\Sigma(\Theta)} {\Sigma(C)} {\Sigma(\mu)} \sub
    {\unrestrict (\sigma \bullet \tau)}\\
    &\equiv \Coh {\Theta} {C} {\mu} \sub {\sigma \bullet \tau}
  \end{align*}
  where the second to last line is given by property
  \[\unrestrict (\sigma \bullet \tau) \equiv \Sigma(\sigma) \bullet
  \unrestrict \tau\]
  which holds for all \(\sigma : \arr \Theta \star \Delta\) and is
  proven in
  \funcn{Catt.Syntax.Properties}{↓-comp}{\textsf{\(\downarrow\)-comp}},
  and the line before is given by the inductive hypothesis.

  If instead we had \(A = \arr s {A'} t\), then:
  \begin{align*}
    \Coh \Theta C \mu \sub \sigma \sub \tau
    &\equiv \Coh {\Sigma(\Theta)} {\Sigma(C)} {\Sigma(\mu)} \sub
    {\unrestrict \sigma} \sub \tau\\
    &\equiv \Coh {\Sigma(\Theta)} {\Sigma(C)} {\Sigma(\mu)} \sub
    {\unrestrict \sigma \bullet \tau}\\
    &\equiv \Coh {\Sigma(\Theta)} {\Sigma(C)} {\Sigma(\mu)} \sub
    {\unrestrict (\sigma \bullet \tau)}\\
    &\equiv \Coh \Theta C \mu \sub {\sigma \bullet \tau}
  \end{align*}
  where we use the inductive hypothesis after applying the equality
  \[ \unrestrict (\sigma \bullet \tau) \equiv \unrestrict \sigma \bullet \tau \]
  which holds for all \(\sigma : \arr \Theta {\arr s {A'} t} \Delta\)
  by \funcn{Catt.Syntax.Properties}{↓-comp-higher}%
  {\textsf{\(\downarrow\)-comp-higher}}.
\end{proof}

This proposition proves that the syntax of \Cattr forms a category,
which we will not name as we will work instead with the subcategory
containing well-formed contexts and substitutions, introduced in the
following sections.

\paragraph{Discs}
We finish our discussion of the syntax of \Cattr by giving formal
definitions of disc and sphere contexts, some constructions on these,
and their properties. This will allow these to be used as examples in
following sections, and pre-empts the use of discs in the first two
equality rules that we will introduce, disc removal and endo-coherence removal.

We begin with the definitions of discs, spheres, and sphere types,
which can be found in \module{Catt.Discs} as \func{Catt.Discs}{Disc},
\func{Catt.Discs}{Sphere}, and \func{Catt.Discs}{sphere-type}. We
write the sphere type as \(U^n\), which is intentionally close to the
notation of the standard type \(\mathcal{U}_\Delta^n\), as it will
turn out that these coincide.

\begin{definition}
  We mutually define the disc contexts \(D^n\), sphere contexts
  \(S^n\), and sphere type \(U^n \in \Type_{S^n}\).
  \begin{mathpar}
    D^n = S^n , (d_n^- : U^n) \and
    S^0 = \emptyset \and
    S^{n+1} = D^n , (d_n^+ : \wk(U^n)) \\
    U^0 = \star \and
    U^{n+1} = \arr {d_n^-} {\wk(\wk(U^{n+1}))} {d_n^+}
  \end{mathpar}
  We will sometimes refer to the last variable of \(D^n\) as \(d_n\)
  instead of \(d_n^-\), given that there is no \(d_n^+\) in the context.
\end{definition}

We also characterise the substitutions from a sphere or disc. These
are given by \func{Catt.Discs}{sub-from-sphere} and
\func{Catt.Discs}{sub-from-disc} in the formalisation.

\begin{definition}
  Let \(A : \Type_\Gamma\) be a type and suppose \(n = \dim(A)\).
  Define the substitution \(\{A\} : S^n \to \Gamma\) inductively by:
  \[ \{\star\} = \langle \rangle \qquad \{\arr s A t\} = \langle \{ A
  \}, s, t \rangle\]
  Further, given a term \(t : \Term_\Gamma\), define the substitution
  \(\{A,t\} : D^n \to \Gamma\) by \(\{A, t\} = \langle \{A\}, t \rangle\).
\end{definition}

In \module{Catt.Discs.Properties}, various facts about these
constructions are proved which we list below.

\begin{lemma}
  \label{lem:disc-prop}
  The following hold:
  \begin{lemmaenum}
  \item \label{item:disc-prop-dim}\(\dim(D^n) = \dim(U^n) = n\) and
    \(\dim(S^n) = \max(n - 1, 0)\).
  \item \label{item:disc-prop-susp} \(\Sigma(D^n) \equiv D^{n+1}\),
    \(\Sigma(S^n) \equiv S^{n+1}\), and \(\Sigma(U^n) \equiv U^{n+1}\).
  \item \label{item:disc-prop-wk} \(\{\wk(A)\} \equiv \wk(\{A\})\)
    and \(\{\wk(A), \wk(t)\} \equiv \wk(\{A,t\})\).
  \item \label{item:disc-prop-sub-susp} \(\{\Sigma(A)\} \equiv
    \Sigma(\{A\})\) and \(\{\Sigma(A),\Sigma(t)\} \equiv \Sigma(\{A,t\})\).
  \item \label{item:disc-prop-sub-sub} \(\{A \sub \sigma\} \equiv
    \{A\} \bullet \sigma\) and \(\{A \sub \sigma,t \sub \sigma\}
    \equiv \{A,t\}\bullet \sigma\).
  \item \label{item:disc-prop-sub-from} \(U^n \sub{\{A\}} \equiv A\)
    and hence \(\wk(U^n)\sub{\{A,t\}} \equiv A\).
  \item For \(\tau : S^n \to \Gamma\), \(\tau \equiv \{U^n \sub \tau\}\).
  \item For \(\tau : D^n \to \Gamma\), \(\tau \equiv \{\wk(U^n) \sub
    \tau, d_n \sub \tau\}\).
  \end{lemmaenum}
  for all \(n \in \mathbb{N}\) and appropriate \(A\), \(t\), and \(\sigma\).
\end{lemma}

The last two statements finish the characterisation of substitutions
from spheres and discs as all such substitutions are of the form
\(\{A\}\) or \(\{A,t\}\) respectively.

In \module{Catt.Discs.Pasting}, it is shown that \(D^n\) is a
ps-context for each \(n\). Therefore, as in
\cref{sec:basic-constructions}, the identity on a term \(t\) of type
\(A\) can be defined as:
\[ \id(A,t) = \Coh {D^n} {\arr {d_n} {\wk(U^n)} {d_n}} {\{A,t\}} \]
where \(n = \dim(A)\). Many properties of identity terms can be
easily derived from \cref{lem:disc-prop}.

\subsection{Typing and equality}
\label{sec:typing-equality}

The typing rules for \Cattr differ from those from \Catt in three key ways:
\begin{enumerate}
  \item The fixed conditions on the support of the types in a
    coherence have been replaced by a set of operations
    \(\mathcal{O}\). Instead of having two typing rules for
    coherences, one for equivalences and one for composites, we
    simply have one typing rule and specify that a coherence \(\Coh
    \Delta {\arr s A t} \sigma\) can be well-formed when:
    \[ (\Delta, \Supp(s), \Supp(t)) \in \mathcal{O} \]
    This will be further motivated and explained in \cref{sec:support}.
  \item A definitional equality is added to the system, generated by
    a set of equality rules \(\mathcal{R}\) which specifies pairs of
    terms which should be equated. The equality takes the form of
    three new judgements:
    \begin{alignat*}{2}
      &\Gamma \vdash A = B&\qquad&\text{\(A, B \in \Type_\Gamma\) are
      equal in context \(\Gamma\).}\\
      &\Gamma \vdash s = t &&\text{\(s, t \in \Term_\Gamma\) are
      equal in context \(\Gamma\).}\\
      &\Gamma \vdash \tau = \sigma &&\text{\(\tau : \Theta \to
      \Gamma\) and \(\sigma : \Delta \to \Gamma\) are equal.}
    \end{alignat*}
    These judgements are all mutually defined (and are in fact
    mutually defined with the typing judgements). We may sometimes
    abbreviate these judgements to \(A = B\), \(s = t\), and \(\tau =
    \sigma\) when the contexts of each piece of syntax is clear.
  \item The typing rules are adjusted to account for this
    definitional equality, via the addition of a conversion rule.
\end{enumerate}

The conversion rule is the only additional typing rule that must be
added to \Cattr, and takes the following form:
\begin{mathpar}
  \inferrule {\Gamma \vdash s : A \and \Gamma \vdash A = B}{\Gamma
  \vdash s : B}\textsc{conv}
\end{mathpar}
allowing the type of any term to vary up to the definitional
equality. This rule accounts for all the semistrict behaviour in the
theories we introduce in \cref{cha:cattstrict}.

By adding this rule, and allowing the type of a term to vary up to
definitional equality instead of syntactic equality, we allow more
terms in the theory to become composable. Suppose we have terms \(f :
x \to y\) and \(g : y' \to z\). In \Catt, we would not be able to
form the vertical composition of these terms, as \(y\) and \(y'\) are
not the same. If we now suppose that \(\Gamma \vdash y = y'\), then
it will follow that \(\Gamma \vdash (x \to y) = (x \to y')\), and so
using the conversion rule we get:
\begin{mathpar}
  \inferrule{\inferrule*{\Gamma \vdash f : x \to y \and
      \inferrule*{\Gamma \vdash y = y'}{\Gamma \vdash (x \to y) = (x \to
    y')}}{\Gamma \vdash f : x \to y'} \and \Gamma \vdash g : y' \to
  z}{\Gamma \vdash f * g : x \to z}
\end{mathpar}
We remark that adding definitional equality does not simply quotient
the terms of the theory, but also allows new terms to be well-formed as above.

The definitional equality judgements are given by the rules in
\cref{fig:equality} and appear in the formalisation alongside the
typing rules in \module{Catt.Typing}. These are generated by the set
of \emph{equality rules} \(\mathcal{R}\), which is a set of triples
of the form \((\Gamma, s, t)\) where \(\Gamma\) is a context and
\(s,t \in \Term_\Gamma\). The key inference rule for equality is then:
\begin{mathpar}
  \inferrule{\Gamma \vdash s : A \and (\Gamma,s,t) \in
  \mathcal{R}}{\Gamma \vdash s = t}\textsc{rule}
\end{mathpar}
which says that if a triple \((\Gamma, s, t)\) is in \(\mathcal{R}\),
then \(\Gamma \vdash s = t\) if \(s\) is well-formed in \(\Gamma\).
The typing prerequisite forces the definitions of equality and typing
to be mutually defined, and ensures that we only apply our equality
rules to well-behaved terms.

We note the asymmetry of this rule, in that only the left-hand side
is required to be well-formed. Every rule introduced in this thesis
will take the form of some reduction from the left-hand side to the
right-hand side, and we will be able to prove that typing for the
right-hand side follows from typing for the left-hand side for every
equality we consider. The converse may not hold in general,
necessitating the condition on the left-hand side. This is similar to
\(\beta\)-reduction in the \(\lambda\)-calculus, where an untyped
term can reduce to a simply typed term.

The remainder of the inference rules for equality simply close under
each constructor, reflexivity, symmetry, and transitivity. It is only
necessary to give symmetry and transitivity rules for terms, and a
reflexivity rule for variables, with these properties following for
the other judgements by simple induction.

\begin{lemma}
  The definitional equality relations on terms, types, and
  substitutions are equivalence relations, for any \(\mathcal{R}\).
\end{lemma}
\begin{proof}
  Proofs of these are found in \module{Catt.Typing.Properties.Base}.
\end{proof}

It is also possible to prove that each term has a canonical type.

\begin{definition}
  The \emph{canonical type} of a term \(t : \Term_\Gamma\),
  \(\ty(t)\), is defined by a case split on \(t\). If \(t\) is a
  variable then the canonical type is the corresponding type in the
  context \(\Gamma\). Otherwise, if \(t \equiv \Coh \Delta A \sigma\)
  then the canonical type is \(A \sub \sigma\).
\end{definition}

This can be used to show that the type of a well-formed term is
unique up to definitional equality, and is equal to this canonical type.

\begin{lemma}
  \label{lem:ty-unique}
  If \(\Gamma \vdash s : A\), then \(\Gamma \vdash s : ty(s)\) and
  \(\Gamma \vdash A = \ty(s)\). Further, if \(\Gamma \vdash s : A\)
  and \(\Gamma \vdash s : B\) then \(\Gamma \vdash A = B\).
\end{lemma}
\begin{proof}
  We prove the first part by induction on the derivation \(\Gamma
  \vdash s : A\). If the derivation is derived from the conversion
  rule applied to \(\Gamma \vdash s : B\) and \(\Gamma \vdash A =
  B\), then by inductive hypothesis we have \(\Gamma \vdash s :
  \ty(s)\) and \(\Gamma \vdash B = \ty(s)\). By transitivity, we
  obtain \(\Gamma \vdash A = \ty(s)\) as required. The second part
  follows directly from the applying the first part to both derivations.
\end{proof}

Using the canonical type, we can define the canonical identity on a term.

\begin{definition}
  \label{def:canonical-id}
  Given a term \(t : \Term_\Gamma\), let its \emph{canonical
  identity} be given by:
  \[ \id(t) \equiv \id(\ty(t), t)\]
  This construction can be iterated, and we say that a term is an
  \emph{iterated canonical identity} if it is on the form
  \(\id^k(t)\) for some \(k\).
\end{definition}

There is not much more that can be proved about the definitional
equality at this point without knowing more about the rule set
\(\mathcal{R}\). In \cref{sec:ruleset}, certain conditions will be
imposed on the set of equality rules, that will allow further lemmas
to be proved in large generality.

\paragraph{Disc removal}

We now give our first example of an equality rule, \emph{disc
removal}. Disc removal removes unary composites, replacing them with
the underlying term. We recall that for every \(n\), there exists the
\(n\)-dimensional disc context \(D^n\), and that given a term \(t \in
\Term_\Gamma\) and \(n\)-dimensional type \(A \in \Type_\Gamma\),
there exists a substitution \(\{A,t\} : D^n \to \Gamma\). The unary
composite of a term \(t\) of type \(A\) of dimension \(n\) is then
the coherence:
\[\Coh {D^n} {\wk(U^n)} {\{A,t\}}\]
Disc removal equates this with the term \(t\), making the following
rule admissible:
\begin{mathpar}
  \inferrule{\Gamma \vdash t : A \\ \Gamma \vdash A}{\Gamma \vdash
  \Coh {D^n} {\wk(U^n)} {\{A,t\}} = t}\textsc{dr}
\end{mathpar}
with the removal of the disc coherence giving the name to this equality rule.

Assembling disc removal into a rule set \(\mathcal{R}\) is simple, as
it is possible to simply give a syntactic condition with no need to
refer to typing.

\begin{definition}
  The \emph{disc removal rule set}, \dr, is the set consisting of the triples:
  \[ (\Gamma, \Coh {D^n} {\wk(U^n)} {\{A,t\}}, t) \]
  for each context \(\Gamma\), type \(A : \Type_\Gamma\), and term
  \(t : \Term_\Gamma\) where \(n = \dim(A)\).

  A set of rules \(\mathcal{R}\) \emph{contains disc removal} if
  \(\dr \subseteq \mathcal{R}\). Further we say that \(\mathcal{R}\)
  \emph{has disc removal} if the rule \textsc{dr} holds in the generated theory.
\end{definition}

The inference rule \textsc{dr} follows the \textsc{rule} and typing
properties about discs which will be given in \cref{sec:ruleset}.

We draw attention to the typing premise of \textsc{rule}. If we know
that the unary composite of a term \(t\) is well-formed, then it
follows that \(t\) itself must have been well-formed, but we cannot
infer that the term \(\Coh {D^n} {\wk(U^n)} {\{A,t\}}\) is
well-formed from \(t\) being well-formed. In particular, knowing that
\(t\) is well-formed does not constrain \(A\) at all without knowing
that the given type \(A\) is the type of \(t\). We must therefore
include an additional typing premise if we want to avoid well-formed
and non-well-formed terms being equated.

\afterpage{%
  \clearpage% flush all other floats
  \ifodd\value{page}
  \else%
  \expandafter\afterpage% put it on the next page if this one is odd
  \fi
  {%
    \begin{figure}[hbtp]
      \centering
      \fbox{%
        \begin{subfigure}{0.47\textwidth}
          \begin{mathpar}
            \inferrule{ }{\star : \Type_\Gamma}
            \and
            \inferrule{x \in \Var(\Gamma)} {x : \Term_\Gamma}
            \and
            \inferrule{A : \Type_\Gamma}{\langle A \rangle :
            \emptyset \to \Gamma}
            \and
            \inferrule{ }{\emptyset : \Ctx}
            \and
            \inferrule{\Gamma : \Ctx \\ A : \Type_\Gamma}{\Gamma, (x
            : A) : \Ctx}
            \and
            \inferrule{\sigma : \arr \Delta A \Gamma \\ t :
            \Term_\Gamma \\ B : \Type_\Delta}{\langle \sigma , t
            \rangle : \arr {\Delta, (x : B)} A \Gamma}
            \and
            \inferrule{A : \Type_\Gamma \\ s : \Term_\Gamma \\ t :
            \Term_\Gamma} {\arr s A t : \Type_\Gamma}
            \and
            \inferrule{\\\\\Delta : \Ctx \\ A : \Type_\Delta
            \\ \sigma : \arr \Delta \star \Gamma}{\Coh \Delta A
            \sigma : \Term_\Gamma}
          \end{mathpar}
          \caption{Syntax.}
      \end{subfigure}}
      \hfill
      \fbox{%
        \begin{subfigure}{0.49\textwidth}
          \begin{mathpar}
            \inferrule{ }{\emptyset \vdash}
            \and
            \inferrule{\Gamma \vdash\\ \Gamma \vdash A}{\Gamma, (x : A) \vdash}
            \and
            \inferrule{ }{\Gamma \vdash \star}
            \and
            \inferrule{\Gamma \vdash s : A \\ \Gamma \vdash A
            \\ \Gamma \vdash t : A}{\Gamma \vdash \arr s A t}
            \and
            \inferrule{\Gamma \vdash A}{\Gamma \vdash \langle A
            \rangle : \emptyset}
            \and
            \inferrule{\Gamma \vdash \sigma : \Delta\\ \Gamma \vdash
            t : A\sub\sigma}{\Gamma \vdash \langle \sigma , t \rangle
            : \Delta, (x : A)}
            \and
            \inferrule{(x : A) \in \Gamma}{\Gamma \vdash x : A}
            \and
            \inferrule{\Gamma \vdash t : A\\ \Gamma \vdash A =
            B}{\Gamma \vdash t : B}
            \and
            \inferrule{\Delta \vdash_{\mathsf{ps}}\\ \Delta \vdash
              \arr s A t \\ \Gamma \vdash \sigma : \Delta\\(\Delta,
            \Supp(s), \Supp(t)) \in \mathcal{O}}{\Gamma \vdash \Coh
              \Delta {\arr s A t} \sigma : \arr {s \sub \sigma} {A \sub
            \sigma} {t \sub \sigma}}
          \end{mathpar}
          \caption{Typing.}
      \end{subfigure}}

      \vspace{7pt}
      \fbox{%
        \begin{subfigure}{0.9852\textwidth}
          \begin{mathpar}
            \inferrule{\Gamma \vdash s : A \\ (\Gamma, s, t) \in
            \mathcal{R}}{\Gamma \vdash s = t}\textsc{rule} \and
            \inferrule{x \in \Var(\Gamma)}{\Gamma \vdash x = x} \and
            \inferrule{\Gamma \vdash s = t}{\Gamma \vdash t = s} \and
            \inferrule{\Gamma \vdash s = t \\ \Gamma \vdash t =
            u}{\Gamma \vdash s = u} \and
            \inferrule{\Delta \vdash A = B \\ \Gamma \vdash \sigma =
            \tau}{\Gamma \vdash \Coh \Delta A \sigma = \Coh \Delta B \tau} \and
            \inferrule{ }{\Gamma \vdash \star = \star} \and
            \inferrule{\Gamma \vdash s = s' \\ \Gamma \vdash t = t'
            \\ \Gamma \vdash A = A'}{\Gamma \vdash \arr s A t = \arr
            {s'} {A'} {t'}}\and
            \inferrule{\Gamma \vdash A = B}{\Gamma \vdash \langle A
            \rangle = \langle B \rangle}\and
            \inferrule{\Gamma \vdash \sigma = \tau \\ \Gamma \vdash s
            = t}{\Gamma \vdash \langle \sigma, s \rangle = \langle
            \tau, t \rangle}
          \end{mathpar}
          \caption{Equality.}
          \label{fig:equality}
      \end{subfigure}}

      \vspace{7pt}
      \fbox{%
        \begin{subfigure}{0.47\textwidth}
          \vspace{3.7pt}
          \begin{mathpar}
            \inferrule{ }{(x : \star) \vdash_{\mathsf{ps}} x : \star}
            \and
            \inferrule{\Gamma \vdash_{\mathsf{ps}} x : A}{\Gamma, (y
            : A), (f : \arr x A y)}
            \and
            \inferrule{\Gamma \vdash_{\mathsf{ps}} x : \arr s A
            t}{\Gamma \vdash_{\mathsf{ps}} t : A}
            \and
            \inferrule{\Gamma \vdash_{\mathsf{ps}} x : \star}{\Gamma
            \vdash_{\mathsf{ps}}}
          \end{mathpar}
          \caption{Ps-contexts.}
      \end{subfigure}}
      \hfill
      \fbox{%
        \begin{subfigure}{0.49\textwidth}
          \begin{mathpar}
            \FV(\star) = \{\}
            \and
            \FV(\langle A \rangle) = \FV(A)
            \\
            \FV(x) = \{x\} \text{ for }x \in \Var
            \\
            \FV(\Coh \Delta A \sigma) = \FV(\sigma)
            \\
            \FV(\arr s A t) = \FV(s) \cup \FV(A) \cup \FV(t)
            \\
            \FV(\langle \sigma , t \rangle) = \FV(\sigma) \cup \FV(t)
          \end{mathpar}
          \caption{Free variables.}
      \end{subfigure}}
      \caption{\Cattr: syntax, typing, and operations.}
      \label{fig:cattr}
    \end{figure}

    \begin{figure}
      \ContinuedFloat
      \fbox{%
        \begin{subfigure}{1\textwidth}
          \begin{align*}
            \DC_\emptyset(\emptyset) &= \emptyset\\
            \DC_{\Gamma, x : A}(V) &=
            \begin{cases*}
              \DC_\Gamma(V)&if \(x \not\in V\)\\
              \{x\} \cup \DC_\Gamma(V \setminus \{x\} \cup \FV(A))&if
              \(x \in V\)\\
            \end{cases*}\\
            \Supp(t) &= \DC_\Gamma(\FV(t))\text{ for }t \in \Term_\Gamma\\
            \Supp(A) &= \DC_\Gamma(\FV(A))\text{ for }A \in \Type_\Gamma\\
            \Supp(\sigma) &= \DC_\Gamma(\FV(\sigma))\text{ for
            }\sigma : \arr {\Delta} A \Gamma
          \end{align*}
          \caption{Support.}
      \end{subfigure}}

      \vspace{7pt}
      \fbox{%
        \begin{subfigure}{\textwidth}
          \begin{align*}
            x \sub \sigma &= t\text{ if }(x \mapsto t) \in \sigma\\
            \Coh \Theta A \tau \sub \sigma &=
            \begin{cases*}
              \Coh \Theta A {\tau \bullet \sigma}&if \(\dim(\ty(\sigma)) = 0\)\\
              \Coh {\Sigma(\Theta)} {\Sigma(A)} {\Sigma(\tau)} \sub
              {\unrestrict\sigma}&otherwise
            \end{cases*}
            \\
            \star \sub \sigma &= \ty(\sigma)\\
            (\arr s A t) \sub \sigma &= \arr {s \sub \sigma} {A \sub
            \sigma} {t \sub \sigma}\\
            \langle A \rangle \bullet \sigma &= \langle A \sub \sigma \rangle\\
            \langle \tau , t \rangle \bullet \sigma &= \langle \tau
            \bullet \sigma , t \sub \sigma \rangle
          \end{align*}
          \caption{Substitution application.}
      \end{subfigure}}

      \vspace{7pt}
      \fbox{%
        \begin{subfigure}{0.475\textwidth}
          \begin{align*}
            \Sigma (\emptyset) &= (N : \star), (S : \star)\\
            \Sigma (\Gamma, (x : A)) &= \Sigma \Gamma, (x : \Sigma A)\\
            \Sigma (\star) &= \arr N \star S\\
            \Sigma (\arr s A t) &= \arr {\Sigma s} {\Sigma A} {\Sigma t}\\
            \Sigma (x) &= x\\
            \Sigma (\Coh \Delta A \sigma) &= \Coh {\Sigma(\Delta)}
            {\Sigma(A)} {\Sigma(\sigma)}\\
            \Sigma(\sigma) &= \unrestrict(\Sigma'(\sigma))\\[7.25pt]
            \Sigma'(\langle A \rangle) &= \langle \Sigma(A) \rangle\\
            \Sigma'(\langle \sigma, x \rangle) &= \langle
            \Sigma'(\sigma), \Sigma(t) \rangle\\
            \unrestrict\langle \arr s A t \rangle &= \langle A , s , t \rangle\\
            \unrestrict\langle \sigma, t \rangle &= \langle
            \unrestrict \sigma, t \rangle
          \end{align*}
          \caption{Suspension.}
      \end{subfigure}}
      \hfill
      \begin{subfigure}{0.49\textwidth}
        \fbox{%
          \begin{subfigure}{1\textwidth}
            \begin{align*}
              \wk(\star) &= \star\\
              \wk(\arr s A t) &= \arr {\wk(s)} {\wk(A)} {\wk(t)}\\
              \wk(x) &= x\\
              \wk(\Coh \Delta A \sigma) &= \Coh \Delta A {\wk(\sigma)}\\
              \wk(\langle A \rangle) &= \langle \wk(A) \rangle\\
              \wk(\langle \sigma, t \rangle) &= \langle \wk(\sigma),
              \wk(t) \rangle
            \end{align*}
            \caption{Weakening.}
            \label{fig:wk}
        \end{subfigure}}

        \vspace{7pt}
        \fbox{%
          \begin{subfigure}{1\textwidth}
            \begin{align*}
              \id_\emptyset &= \langle \star \rangle\\
              \id_{\Gamma, (x : A)} &= \langle \wk(\id_\Gamma), x \rangle
            \end{align*}
            \caption{Identity substitution.}
        \end{subfigure}}
      \end{subfigure}
      \caption{\Cattr: syntax, typing, and operations.}
    \end{figure}
  }%
}

\section[The set of operations
\texorpdfstring{\(\mathcal{O}\)}{O}]{The set of operations
\texorpdfstring{\boldmath\(\mathcal{O}\)}{O}}
\label{sec:support}

In \cref{sec:typing-equality}, we introduced a set of operations
\(\mathcal{O}\), which allows us to vary the operations available in
the theory, much like the set \(\mathcal{R}\) allows us to vary the
equality rules of the theory. The set \(\mathcal{O}\) replaces the
conditions on the support of the type contained in a coherence, and
consists of a set of triples of a context \(\Delta\), along with two
sets \(x,y \subseteq \Var(\Delta)\). A certain type \(\arr s A t :
\Type_\Delta\) is permitted to appear in a coherence exactly when
\((\Delta , \Supp(s), \Supp(t))\) is an element of \(\mathcal{O}\).

There are two key advantages to setting up the theory this way.
\begin{itemize}
  \item A clear separation is introduced in the metatheory and
    formalisation between properties that are specific to the support
    conditions in \Catt and those that are independent of the
    specific support conditions present.
  \item The results in the following sections can be proven
    generically for different variants of \Catt.
\end{itemize}
In particular, the main utility we extract in this thesis is the
ability to define groupoidal versions of the various semistrict
theories we define in \cref{cha:cattstrict}. By letting
\(\mathcal{O}\) consists of all possible triples, the support
condition is effectively removed, producing a version of \Catt closer
to Grothendieck's definition of \(\infty\)-groupoid (see \cref{sec:weak}).

\subsection{Operation sets}
\label{sec:operation-sets}

As previously mentioned, an operation set \(\mathcal{O}\) consists of
a collection of triples of a context \(\Delta\) and two subsets of
the variables of \(\Delta\).

We call a subset of the variables of a context a \emph{variable set}.
In the formalisation, these variable sets are given as a list of
booleans, one boolean for each variable of the context. These are
given in \module{Catt.Support}, which also contains many
constructions on them, including unions of these sets, subset
relations, and the free variables of each piece of syntax. The
variable sets of \(\Delta\) form a lattice with top element
\(\Var(\Delta)\) and bottom element \(\emptyset\). The free variable
constructions commute with weakening, as is proved in
\module{Catt.Support.Properties} by mutual induction.

We recall the function \(\DC\) on these variable sets, given by
\func{Catt.Support}{DC} in the formalisation, which produces the
downwards closure of a variable set. This admits the following properties:

\begin{proposition}
  \(\DC\) is an idempotent join-semilattice homomorphism. It
  preserves binary joins (unions), subset inclusions, and preserves
  the top and bottom element of the lattice.
\end{proposition}

We further define the application of a substitution to a variable set below.

\begin{definition}
  Given a variable set \(V\) of \(\Delta\) and (regular) substitution
  \(\sigma : \Delta \to \Gamma\), we define the application of
  \(\sigma\) to \(V\), written \(V \sub \sigma\) to be a variable set
  of \(\Gamma\) given by:
  \begin{align*}
    V \sub {\langle \rangle} &= \emptyset\\
    V \sub {\langle \sigma , t \rangle} &=
    \begin{cases*}
      (V \setminus \{x\}) \sub \sigma \cup \FV(t)&if \(x \in V\)\\
      V \sub \sigma &otherwise
    \end{cases*}
  \end{align*}
  Where \(x\) is assumed to be the last variable of \(\Delta\) in the
  second case.
\end{definition}

We note that when representing variable sets as a list of booleans,
these definitions are given by simple inductions on the length of the
context. These constructions admit the following properties.

\begin{proposition}
  \label{prop:vs-sub}
  Let \(\Delta\) be a context. Then the function taking a variable
  set \(V\) of \(\Delta\) to \(V \sub \sigma\) is a join-semilattice
  homomorphism for any substitution \(\sigma : \Delta \to \Gamma\).
  Further, for a term \(t : \Term_\Delta\), a type \(A :
  \Type_\Delta\), or a substitution \(\tau : \arr \Theta A \Delta\),
  the following equalities hold:
  \begin{align*}
    \FV(t \sub \sigma) &= \FV(t) \sub \sigma \\
    \FV(A \sub \sigma) &= \FV(A) \sub \sigma \\
    \FV(\tau \bullet \sigma) &= \FV(\tau) \sub \sigma
  \end{align*}
  and hence \(\Var(\Delta) \sub \sigma = \FV(\id_\Delta) \sub \sigma
  = \FV(\id_\Delta \bullet \sigma) = \FV(\sigma)\). For any variable
  set \(V \subseteq \Var(\Theta)\) we have:
  \[ V \sub {\id_\Theta} = V \qquad V \sub {\tau \bullet \sigma} = V
  \sub \tau \sub \sigma \]
  for \(\tau : \Theta \to \Delta\) and \(\sigma : \Delta \to \Gamma\).
\end{proposition}
\begin{proof}
  All proofs proceed by induction on the length of the context
  \(\Delta\) and are given in \module{Catt.Support.Properties}.
\end{proof}

An operation set is then an element of:
\[ \Sigma_{\Delta : \Ctx} \mathcal{P}(\Var(\Delta)) \times
\mathcal{P}(\Var(\Delta)) \]
In the formalisation this is defined in \module{Catt.Ops} to be a
function from a context and two variable sets of that context to a universe.

\begin{remark}
  The definition of an operation set in the formalisation deviates
  from the presentation given here, as the version in the
  formalisation is proof relevant. The proof relevant definition
  allows us to give any type as the type of witnesses that a certain
  triple appears in \(\mathcal{O}\), including a type containing many
  distinct witnesses.

  If we wished to recover a definition closer to the classical
  set-based definition, we could enforce that this function has a
  universe of propositions as its codomain, instead of a universe of
  types, and use propositional truncations to define various versions
  of \(\mathcal{O}\). This is however unnecessary for any of the
  proofs appearing in this thesis, hence the choice of the proof
  relevant definition for simplicity. A similar observation will
  apply to the definition of equality rule sets introduced in
  \cref{sec:ruleset}.
\end{remark}

We can now introduce our first operation set, the operation set for
groupoidal operations, which imposes no support conditions and allows
all operations.

\begin{definition}
  We define the \emph{groupoidal operation set} \(\Group\) as:
  \[ \Group = \{ (\Delta, U, V) \mid \Delta : \Ctx, U \subseteq
  \Var(\Delta), V \subseteq \Var(\Delta) \} \]
  We will refer to \Cattr with the operation set \(\Group\) as
  \emph{groupoidal \Cattr} or \emph{groupoidal \Catt} (when
  \(\mathcal{R} = \emptyset\)).
\end{definition}

To recover the standard definition of \Catt, we must define the
boundary sets of a pasting diagram. In \cref{sec:typing-catt}, these
are given as the free variables of the boundary inclusion
substitutions of pasting diagrams. Here we will instead give a direct
definition of the variable sets corresponding to the free variables
of the substitutions, delaying the definition of boundary inclusions
of pasting diagrams until \cref{sec:trees}.

\begin{definition}
  Let \(\Delta\) be a ps-context. Define the \(n\)-boundary variable
  sets \(\bdry n -  \Delta\) and \(\bdry n + \Delta\) by induction on
  \(\Delta\):
  \begin{align*}
    \bdry i \epsilon {(x : \star)} &= \{ x \}\\
    \bdry i \epsilon {\Gamma, (y : A) , (f : \arr x A y)} &=
    \begin{cases*}
      \bdry i \epsilon \Gamma&if \(i < \dim(A)\)\\
      \bdry i - \Gamma&if \(i = \dim(A)\) and \(\epsilon = -\)\\
      (\bdry i + \Gamma \cup \{ y \}) \setminus \{x\}&if \(i =
      \dim(A)\) and \(\epsilon = +\)\\
      \bdry i \epsilon \Gamma \cup \{ y , f \}&otherwise
    \end{cases*}
  \end{align*}
  These boundary sets appear in the formalisation as
  \func{Catt.Support}{pd-bd-vs}.
\end{definition}

The following lemma is immediate:

\begin{lemma}
  \label{lem:bdry-full}
  If \(n \geq \dim(\Delta)\), then \(\bdry n \epsilon \Delta = \Var(\Delta)\).
\end{lemma}
\begin{proof}
  A simple induction on the definition. A formalised proof appears as
  \func{Catt.Support.Properties}{pd-bd-vs-full} in the module
  \module{Catt.Support.Properties}.
\end{proof}

With this definition we can introduce the regular operation set,
which recovers the regular support conditions used in the definition of \Catt.

\begin{definition}
  The \emph{regular operation set} \Reg is defined to be:
  \[ \Reg = \{ (\Delta, \Var(\Delta), \Var(\Delta)) \mid \Delta
    \vdash_{\mathsf{ps}} \} \cup \{ (\Delta, \bdry {\dim(\Delta)-1} -
      \Delta, \bdry {\dim(\Delta)-1} + \Delta) \mid \Delta
  \vdash_{\mathsf{ps}} \} \]
  The first component allows equivalences to be well-formed, and the
  second gives the support condition for composites.
\end{definition}

The regular operation set has more standard presentation.

\begin{proposition}
  \label{prop:std-op}
  Let the set \Std of standard operations be defined as:
  \[ \Std = \{ (\Delta, \bdry n - \Delta, \bdry n + \Delta) \mid
  \Delta \vdash_{\mathsf{ps}} , n \geq \dim(\Delta) - 1 \} \]
  Then \(\Std = \Reg\).
\end{proposition}
\begin{proof}
  Suppose \((\Delta, U, V) \in \Reg\). If \(U = \bdry {\dim(\Delta) -
  1} - \Delta\) and \(V = \bdry {\dim(\Delta) - 1} + \Delta\), then
  \((\Delta , U ,V)\) is trivially in \Std by letting \(n =
  \dim(\Delta) - 1\). If instead \(U = V = \Var(\Delta)\), then
  \((\Delta, U , V) \in \Std\) by letting \(n = \dim(\Delta)\) and
  applying \cref{lem:bdry-full}.

  Conversely, assume \((\Delta, U, V) \in \Std\). Then there is \(n
  \geq \dim(\Delta) - 1\) with \(U = \bdry n - \Delta\) and \(V =
  \bdry n + \Delta\). If \(n = \dim(\Delta) - 1\) then \((\Delta, U
  ,V)\) is trivially in \(\Reg\), and otherwise by
  \cref{lem:bdry-full} we have \(U = V = \Var(\Delta)\), and so
  \((\Delta,U,V)\) is again an element of \Reg. Hence, \(\Reg = \Std\).
\end{proof}

This more uniform presentation is sometimes easier to work with, and
will be used to prove properties of \Reg in \cref{sec:operation-properties}.

\begin{remark}
  By letting \(\mathcal{O} = \emptyset\), we recover the type theory
  \textsf{GSeTT}~\cite{benjamin2021globular}, a type theory for globular sets.
\end{remark}

It would be possible to generalise the notion of operation set
presented here by instead letting the set \(\mathcal{O}\) consist of
triples \((\Delta, s,t)\) where \(s\) and \(t\) are terms over
\(\Delta\) instead of variable sets over \(\Delta\). This would allow
more control over which operations were allowed in the theory. As an
example, we would be able to restrict the class of composites to
contain only the standard composites, or even further restrict it to
binary composites.

This is however unnecessary to present the regular and groupoidal
versions of \Cattr. By only allowing the set of available operations
to be specified up to the support of the contained terms, it is
possible to show that a coherence being an operation is closed under
equality by proving that equality preserves the support of a term.

\subsection{Operation properties}
\label{sec:operation-properties}

Currently, our set of operations is completely unconstrained, and we
will be limited in the constructions that can be made in \Cattr. We
therefore constrain these sets in two ways. The first enforces that
our set of operations is closed under suspension, for which we need
to be able to suspend variable sets. This is defined in the
formalisation as \func{Catt.Suspension.Support}{susp-vs}.

\begin{definition}
  Let \(\Delta\) be a context. The suspension of a variable set \(V\)
  over \(\Delta\) is defined to be:
  \[ \Sigma(V) = \{ N , S \} \cup V \]
  where \(\Sigma(V)\), the suspension of \(V\) is a variable set over
  \(\Sigma(\Delta)\).
\end{definition}
The suspension of a variable set commutes with taking the support of
a piece of syntax, as shown in the next lemma.
\begin{lemma}
  \label{lem:susp-vs-prop}
  The following equalities hold:
  \[ \Supp(\Sigma(s)) = \Sigma(\Supp(s)) \qquad \Supp(\Sigma(A)) =
  \Sigma(\Supp(A)) \qquad \Supp(\Sigma(\sigma)) = \Sigma(\Supp(\sigma)) \]
  for term \(s : \Term_\Gamma\), type \(A : \Type_\Gamma\), and
  substitution \(\sigma : \arr \Delta \star \Gamma\).
\end{lemma}
\begin{proof}
  All equalities hold by a mutual induction on terms, types, and
  substitutions, with a secondary induction on the context \(\Gamma\)
  for the case of the variables and the base type \(\star\). These
  calculations are given in \module{Catt.Suspension.Support}.
\end{proof}

We can then define our first property on operation sets.

\begin{definition}
  An operation set \(\mathcal{O}\) is \emph{suspendable} if:
  \[ (\Delta, U, V) \in \mathcal{O} \implies (\Sigma(\Delta),
  \Sigma(U), \Sigma(V)) \in \mathcal{O} \]
  For \(\Delta : \Ctx\) and \(U, V \subseteq \Var(\Delta)\).
\end{definition}

The groupoidal operation set is trivially suspendable. To show that
the regular operation set is suspendable, we prove the following proposition.

\begin{proposition}
  Let \(\Delta\) be a ps-context. Then:
  \[\Sigma(\bdry n \epsilon \Delta) = \bdry {n + 1} {\epsilon}
  {\Sigma(\Delta)}\]
  for \(n \in \mathbb{N}\) and \(\epsilon \in \{-,+\}\).
\end{proposition}
\begin{proof}
  We proceed by induction on \(\Delta\). First suppose \(\Delta = (x
  : \star)\). We then have:
  \[ \Sigma(\bdry n \epsilon {(x : \star)}) = \Sigma(\{x\}) =
  \{N,S,x\} = \bdry {n + 1} {\epsilon} {\Sigma((x: \star))} \]
  Now suppose that \(\Delta = \Delta', (y : A), (f : \arr x A y)\).
  We split into cases on \(n\), \(\dim(A)\), and \(\epsilon\):
  \begin{itemize}
    \item If \(n < \dim(A)\) then
      \begin{align*}
        \Sigma(\bdry n \epsilon \Delta) &= \Sigma(\bdry n \epsilon {\Delta'})\\
        &=  \bdry {n + 1} {\epsilon} {\Sigma(\Delta')} &\text{by
        inductive hypothesis}\\
        &= \bdry {n + 1} {\epsilon} {\Sigma(\Delta)} &\text{as }n + 1
        < \dim(\Sigma(A))\\
        \intertext{
        \item If \(n = \dim(A)\) and \(\epsilon = -\) then the proof
          is similar to the preceding case.
        \item If \(n = \dim(A)\) and \(\epsilon = +\) then:
        }
        \Sigma(\bdry n + \Delta) &= \Sigma((\bdry n + {\Delta'} \cup
        \{y\}) \setminus \{x\})\\
        &= (\Sigma(\bdry n + {\Delta'}) \cup \{y\}) \setminus \{x\} \\
        &= (\bdry {n+1} + {\Sigma(\Delta')} \cup \{y\}) \setminus
        \{x\} &\text{by inductive hypothesis}\\
        &= \bdry {n+1} + {\Sigma(\Delta)} &\text{as }n + 1 = \dim(\Sigma(A))\\
        \intertext{
      \item If \(n > \dim(A)\) then}
        \Sigma(\bdry n \epsilon \Delta) &= \Sigma((\bdry n \epsilon
        {\Delta'}) \cup \{y,f\})\\
        &= \Sigma(\bdry n + {\Delta'}) \cup \{y,f\} \\
        &= \bdry {n+1} + {\Sigma(\Delta')} \cup \{y, f\} &\text{by
        inductive hypothesis}\\
        &= \bdry {n+1} + {\Sigma(\Delta)} &\text{as }n + 1 > \dim(\Sigma(A))
      \end{align*}
  \end{itemize}
  Hence, the desired equality holds in all cases.
\end{proof}

\begin{corollary}
  The regular operation set is suspendable.
\end{corollary}
\begin{proof}
  By \cref{prop:std-op}, it suffices to show that the standard
  operation set is suspendable, which is clear from the above proposition.
\end{proof}

The second restriction we put on operation sets is that there are
enough operations to create the standard coherences presented in
\cref{sec:basic-constructions}.

\begin{definition}
  An operation set \(\mathcal{O}\) \emph{contains the standard
  operations} if \(\Std \subseteq \mathcal{O}\).
\end{definition}

The groupoidal operation set clearly contains the standard
operations, and the regular operation set also does due to
\cref{prop:std-op}. The empty operation set does not contain the
standard operations. We end this section with the following
proposition about the support of terms in a disc.

\begin{proposition}
  For \(n \in \mathbb{N}\) the following two equations hold:
  \[ \bdry n - {D^{n+1}} = \Var(S^n) \cup \{d_n^-\} = \Var(D^n)
  \qquad \bdry n + {D^{n+1}} = \Var(S^n) \cup \{d_{n+1}^+\}\]
  Further, the following equations hold:
  \[\FV(U^n) = \Var(S^n) \qquad \Supp(d_n^-) = \Var(D^n) = \bdry n -
  {D^{n+1}} \qquad \Supp(d_n^+) = \bdry n + {D^{n+1}} \]
  again for any \(n \in \mathbb{N}\).
\end{proposition}
\begin{proof}
  The first equations follow by a simple case analysis, using that
  \(\bdry n - {D^n} = \Var(D^n)\) by
  \cref{lem:bdry-full,item:disc-prop-dim}. The free variables of
  \(U^n\) are easily calculated inductively, and the support of
  \(d_n^-\) and \(d_n^+\) are easy to compute using the first parts
  of the proposition, and that \(FV(U^n) \subseteq \Supp(d_n^-)\) and
  \(\FV(U^n) \subseteq \Supp(d_n^+)\) as the support of a term is
  downwards closed.

  These proofs are formalised in \module{Catt.Discs.Support}.
\end{proof}

\begin{corollary}
  \label{cor:disc-op}
  Both \((D^{n+1}, d_n^-, d_n^+)\) and \((D^n, d_n, d_n)\) are in
  \(\Std\) for each \(n\).
\end{corollary}

\section[The set of equality rules
\texorpdfstring{\(\mathcal{R}\)}{R}]{The set of equality rules
\texorpdfstring{\boldmath\(\mathcal{R}\)}{R}}
\label{sec:ruleset}

In \Cattr, the definitional equality relation is generated by a set
of rules \(\mathcal{R}\) formed of triples containing a context and
two terms in the context which should be made equal. In this section
we discuss some operations on these equality sets and properties that
they may have.

\begin{remark}
  In the formalisation the set of equality rules is defined similarly
  to the set of operations \(\mathcal{O}\). It is defined as a
  function that takes a context and two terms over that context and
  returns a type. It is therefore proof relevant in the same way as
  the operation sets.
\end{remark}

The equality rule sets inherit some operations and relations just by
being sets. We can easily form the empty equality set, which allows
us to recover the weak type theory \Catt, and given two equality sets
we can take their union, to get a type theory with equalities from
both sets (we note that the equality generated by a union is in
  general coarser than the union of the equalities generated by the
individual sets).

To aid readability when reasoning about typing and equality with
multiple distinct operations, we may subscript the turnstile symbol
in various judgements with the set of equality rules being used. For
example, we may write the judgements for typing of a term \(t\) in
the type theory generated from rules \(\mathcal{R}\) as
\[ \Gamma \vdash_{\mathcal{R}} t : A \]
and the corresponding judgement for the equality of two terms \(s\) and \(t\) as
\[ \Gamma \vdash_{\mathcal{R}} s = t \]
Equality rule sets can also be subsets of each other, leading to the
following lemma.

\begin{lemma}
  \label{lem:subset-lem}
  Let \(\mathcal{R}\) and \(\mathcal{S}\) be two equality rule sets
  and suppose that
  \[ \Gamma \vdash_{\mathcal{S}} s = t\]
  for all \((\Gamma,s,t) \in \mathcal{R}\) with \(\Gamma
  \vdash_{\mathcal{S}} s : A\) for some \(A : \Type_\Gamma\). Then
  the following inference rules hold:
  \begin{mathpar}
    \inferrule{\Gamma \vdash_{\mathcal{R}}}{\Gamma \vdash_{\mathcal{S}}} \and
    \inferrule{\Gamma \vdash_{\mathcal{R}} t : A}{\Gamma
    \vdash_{\mathcal{S}} t : A} \and
    \inferrule{\Gamma \vdash_{\mathcal{R}} A}{\Gamma
    \vdash_{\mathcal{S}} A} \and
    \inferrule{\Gamma \vdash_{\mathcal{R}} \sigma : \Delta}{\Gamma
    \vdash_{\mathcal{S}} \sigma : \Delta} \\
    \inferrule{\Gamma \vdash_{\mathcal{R}} s = t}{\Gamma
    \vdash_{\mathcal{S}} s = t} \and
    \inferrule{\Gamma \vdash_{\mathcal{R}} A = B}{\Gamma
    \vdash_{\mathcal{S}} A = B} \and
    \inferrule{\Gamma \vdash_{\mathcal{R}} \sigma = \tau}{\Gamma
    \vdash_{\mathcal{S}} \sigma = \tau}
  \end{mathpar}
  In particular these inference rules hold when \(\mathcal{R}
  \subseteq \mathcal{S}\).
\end{lemma}
\begin{proof}
  Follows from a simple induction. Details are given in the
  formalisation in module \module{Catt.Typing.Rule.Properties}.
\end{proof}
\begin{corollary}
  \label{cor:catt-to-r}
  Any context, term, type, or substitution that is well-formed in
  \Catt is also well-formed in \Cattr, for any equality set \(\mathcal{R}\).
\end{corollary}

Furthermore, we can immediately show that the application of a
substitution to piece of syntax that is well-formed in \Catt is well-formed.

\begin{lemma}
  \label{lem:sub-catt}
  Let \(\mathcal{R}\) be any equality rule set. Then the following
  inference rules hold for \(\sigma : \arr \Delta \star \Gamma\):
  \begin{mathpar}
    \inferrule{\Delta \vdash_\emptyset A \\ \Gamma
    \vdash_{\mathcal{R}} \sigma : \Delta}{\Gamma \vdash_{\mathcal{R}}
    A \sub \sigma }\and
    \inferrule{\Delta \vdash_\emptyset s : A \\ \Gamma
    \vdash_{\mathcal{R}} \sigma : \Delta}{\Gamma \vdash_{\mathcal{R}}
    s \sub \sigma : A \sub \sigma } \and
    \inferrule{\Delta \vdash_\emptyset \tau : \Theta \\ \Gamma
    \vdash_{\mathcal{R}} \sigma : \Delta}{\Gamma \vdash_{\mathcal{R}}
    \tau \bullet \sigma : \Theta }
  \end{mathpar}
  where the judgements with a subscript empty set are judgements in
  the theory generated by the empty rule sets (judgements in \Catt).
\end{lemma}
\begin{proof}
  Follows immediately from a mutual induction, using that any
  equality in \Catt is syntactic. The proof is formalised in
  \module{Catt.Typing.Properties.Base}.
\end{proof}

An arbitrary set \(\mathcal{R}\) has very few restrictions on the
equality relation it generates, and the terms that are well-formed
because of it. A rule set \(\mathcal{R}\) could identify terms of
different types, or identify two different variables (or even
identify all variables or terms). This makes it difficult to prove
much about the theory generated by an arbitrary set \(\mathcal{R}\).

To this end, we introduce certain conditions that these equality rule
sets can satisfy. The first three of these conditions put certain
closure properties on the set of rules \(\mathcal{R}\), and each
allow various constructions to be well-formed. We call theories that
satisfy these three properties \emph{tame theories} and introduce
these in \cref{sec:tame-theories}. In \cref{sec:further-conditions},
we introduce two more conditions which take the form of a property
that the generated equality must satisfy.

By introducing these conditions, we can prove various metatheoretic
properties about \Cattr in a modular and generic way. This will allow
the re-use of many constructions and proofs about the properties of
these constructions in \cref{cha:cattstrict}, where two distinct type
theories for semistrict \(\infty\)-categories are given.

In the following subsections, we will also show that the rule set for
disc removal satisfies all these conditions. For all these
conditions, we will have that if the condition holds on
\(\mathcal{R}\) and on \(\mathcal{S}\) then it also holds on
\(\mathcal{R}\cup \mathcal{S}\), and so these conditions can be
proved individually for each rule set that is introduced. Further,
the empty set will satisfy all of these conditions vacuously, and so
all proofs and constructions in the section apply to \Catt.

\subsection{Tame theories}
\label{sec:tame-theories}

Here we introduce the three core conditions on the equality rule set
\(\mathcal{R}\) which we expect hold for any reasonable choice of rule set:
\begin{itemize}
  \item The \emph{weakening condition}, which allows weakening to be
    well-formed.
  \item The \emph{suspension condition}, which allows suspension to
    be well-formed.
  \item The \emph{substitution condition}, which implies that the
    application of substitution to terms, types, and other
    substitutions (as substitution composition) preserves typing and equality.
\end{itemize}
We call an equality rule set \emph{tame} if it satisfies all three of
these conditions, and call the corresponding theory \Cattr a \emph{tame theory}.

\paragraph{Weakening condition}

For the weakening operation to be well-formed, meaning that the
weakening of a well-formed piece of syntax is itself well-formed, the
following closure property must hold on the set of rules \(\mathcal{R}\).

\begin{definition}
  A set of rules \(\mathcal{R}\) satisfies the \emph{weakening
  condition} if for all \((\Gamma,s,t) \in \mathcal{R}\) we have:
  \[ ((\Gamma, (x : A)), \wk(s), \wk(t)) \in \mathcal{R} \]
  for all \(A : \Type_\Gamma\).
\end{definition}

The following proposition is immediately provable by mutual induction
on typing and equality. Its proof is given in
\module{Catt.Typing.Properties.Weakening}.

\begin{proposition}
  Let \(\mathcal{R}\) satisfy the weakening condition. Then the
  following inference rules are admissible in \Cattr.
  \begin{mathpar}
    \inferrule{\Gamma \vdash B}{\Gamma, (x : A) \vdash \wk(A)} \and
    \inferrule{\Gamma \vdash s : B}{\Gamma, (x : A) \vdash \wk(s) : \wk(B)} \and
    \inferrule{\Gamma \vdash \sigma : \Delta}{\Gamma, (x : A) \vdash
    \wk(\sigma) : \Delta}
  \end{mathpar}
  for types \(A,B : \Type_\Gamma\), term \(s : \Term_\Gamma\) and
  substitution \(\sigma : \arr \Delta C \Gamma\).
\end{proposition}

\begin{corollary}
  \label{cor:id-sub-ty}
  If \(\mathcal{R}\) satisfies the weakening condition then:
  \[ \Gamma \vdash \id_\Gamma : \Gamma \]
  for any \(\Gamma : \Ctx\).
\end{corollary}

Using only the above proposition we can immediately prove typing
properties for several constructions using discs.

\begin{lemma}
  \label{lem:disc-typing}
  Suppose the weakening condition holds. Then the following judgements hold:
  \[ S^n \vdash U^n \qquad S^n \vdash \qquad D^n \vdash \]
  For all \(n \in \mathbb{N}\). Further, the following inference
  rules are admissible:
  \begin{mathpar}
    \inferrule{\Gamma \vdash A \\ n = \dim(A)} {\Gamma \vdash \{A\} : S^n} \and
    \inferrule{\Gamma \vdash A \\ n = \dim(A) \\ \Gamma \vdash s : A}
    { \Gamma \vdash \{A,s\} : D^n} \\
    \inferrule{\Gamma \vdash \{A\} : S^n}{\Gamma \vdash A} \and
    \inferrule{\Gamma \vdash \{A,s\} : D^n}{\Gamma \vdash A} \and
    \inferrule{\Gamma \vdash \{A,s\} : D^n}{\Gamma \vdash s : A}
  \end{mathpar}
  For \(A : \Type_\Gamma\) and \(s : \Term_\Gamma\).
\end{lemma}
\begin{proof}
  The first three typing judgements follow from a simple mutual
  induction, making use of the typing of weakening. We prove that
  \(\Gamma \vdash \{A\} : S^n\) by induction on \(n\) and \(A\). The
  base case is trivial. For the inductive step we assume that
  \(\Gamma \vdash \arr s A t\), with \(n = \dim(A)\), and want to show that:
  \[ \Gamma \vdash \langle \{A\},s ,t \rangle : S^n, (d_{n+1}^- :
  U^n), (d_{n+1}^+ : \wk(U^n)) \]
  The judgement \(\Gamma \vdash \{A\} : S^n\) holds by inductive
  hypothesis, and so it remains to show that the following two judgements hold:
  \[ \Gamma \vdash s : U^n \sub {\{A\}} \qquad \Gamma \vdash t :
  \wk(U^n)\sub{\langle\{A\}, s\rangle} \]
  As \(\Gamma \vdash \arr s A t\), we know (by case analysis on the
  typing derivation) that \(\Gamma \vdash s : A\) and \(\Gamma \vdash
  t : A\). These judgements are sufficient to finish the proof, since
  \(A \equiv U^n \sub {\{A\}} \equiv \wk(U^n) \sub {\langle \{A\}, s
  \rangle}\) by \cref{item:disc-prop-sub-from} and the interaction of
  weakening with substitution application.

  To show that \(\Gamma \vdash A\) follows from \(\Gamma \vdash \{A\}
  : S^n\), we instead show that \(\Gamma \vdash U^n \sub {\{A\}}\),
  leveraging that typing is invariant under syntactic equality. The
  typing of \(U^n \sub {\{A\}}\) follows from \(U^n\) being
  well-formed in \Catt (as it is well-formed in any theory with the
  weakening property), and \cref{lem:sub-catt}. The second to last
  inference rule follows trivially from the preceding one. For the
  last rule, we get that \(\Gamma \vdash s : U^n\sub{\{A\}}\) by case
  analysis on \(\Gamma \vdash \{A,s\} : D^n\), and so we are finished
  by the invariance of typing rules under syntactic equality.
\end{proof}

If we further have that the set of operations includes the standard
operations then we get the following corollary.

\begin{corollary}
  \label{cor:id-typing}
  Suppose that \(\mathcal{O}\) contains the standard operations in
  addition to \(\mathcal{R}\) satisfying the weakening condition.
  Then the following are equivalent:
  \begin{itemize}
    \item \(\Gamma \vdash A\) and \(\Gamma \vdash s : A\),
    \item There exists some \(B: \Type_\Gamma\) such that \(\Gamma
      \vdash \id(A,t) : B\),
    \item \(\Gamma \vdash \id(A,t) : \arr t A t\).
  \end{itemize}
  If we further have that \(\dim(A) \neq 0\) then the following two
  conditions are also equivalent:
  \begin{itemize}
    \item There exists some \(B: \Type_\Gamma\) such that \(\Gamma
      \vdash \Coh {D^n} {\wk(U^n)} {\{A,t\}} : B\),
    \item \(\Gamma \vdash \Coh{D^n} {\wk(U^n)} {\{A,t\}} : A\).
  \end{itemize}
  where \(n = \dim(A)\).
\end{corollary}
\begin{proof}
  The proof follows from
  \cref{lem:disc-typing,item:disc-prop-sub-from,cor:disc-op}.
\end{proof}

We end this discussion with the following \lcnamecref{prop:dr-weak}.

\begin{proposition}
  \label{prop:dr-weak}
  The set \dr satisfies the weakening condition.
\end{proposition}
\begin{proof}
  It suffices to show that for all \(\Gamma : \Ctx\), \(A, B :
  \Type_\Gamma\), and \(t : \Term_\Gamma\) that:
  \[ ((\Gamma, (x : B)), \Coh {D^n} {\wk(U^n)} {\wk(\{A,t\})},
  \wk(t)) \in \dr \]
  when \(n = \dim(A)\). By \cref{item:disc-prop-wk}, \(\wk(\{A,t\})
  \equiv \{ \wk(A), \wk(t)\}\) and so the triple above is clearly
  contained in \dr.
\end{proof}

The semistrict type theories \Cattsu and \Cattsua (which will be
introduced in \cref{sec:cattsu,sec:cattsua}) will be generated by
equality rule sets that are the union of multiple smaller rule sets
(including disc removal). Since the weakening condition is clearly
preserved under unions, we will be able to show that the rule sets
generating \Cattsu and \Cattsua satisfy the weakening condition by
showing that it is satisfied by each individual component.

\paragraph{Suspension condition}

For suspension, we introduce the following condition, which is
similar to the corresponding condition for weakening.

\begin{definition}
  A set of equality rules \(\mathcal{R}\) satisfies the
  \emph{suspension condition} if
  \[ (\Sigma(\Gamma), \Sigma(s), \Sigma(t)) \in \mathcal{R} \]
  for all \((\Gamma,s,t) \in \mathcal{R}\).
\end{definition}

If the set of operations \(\mathcal{O}\) is suspendable, then this
condition is sufficient to show that the suspension of a well-formed
piece of syntax is well-formed.

\begin{proposition}
  Suppose \(\mathcal{O}\) is suspendable and \(\mathcal{R}\)
  satisfies the suspension condition. Then the following inference
  rules are admissible for \(\Gamma, \Delta, \Delta' : \Ctx\),
  \(A,B,C,D : \Type_\Gamma\), \(s,t : \Term_\Gamma\), \(\sigma : \arr
  \Delta C \Gamma\), and \(\tau : \arr {\Delta'} D \Gamma\).
  \begin{mathpar}
    \inferrule{\Gamma \vdash}{\Sigma(\Gamma) \vdash}\and
    \inferrule{\Gamma \vdash A}{\Sigma(\Gamma) \vdash \Sigma(A)}\and
    \inferrule{\Gamma \vdash s : A}{\Sigma(\Gamma) \vdash \Sigma(s) :
    \Sigma(A)}\and
    \inferrule{\Gamma \vdash \sigma : \Delta}{\Sigma(\Gamma) \vdash
    \Sigma'(\sigma) : \Delta}\\
    \inferrule{\Gamma \vdash A = B}{\Sigma(\Gamma) \vdash \Sigma(A) =
    \Sigma(B)}\and
    \inferrule{\Gamma \vdash s = t}{\Sigma(\Gamma) \vdash \Sigma(s) =
    \Sigma(t)}\and
    \inferrule{\Gamma \vdash \sigma = \tau}{\Sigma'(\Gamma) \vdash
    \Sigma'(\sigma) = \Sigma(\tau)}
  \end{mathpar}
  For all \(\mu : \arr \Delta {\arr s A t} \Gamma\) and \(\mu' : \arr
  {\Delta'} {\arr {s'} {A'} {t'}} {\Gamma'}\) the following two rules
  are admissible:
  \begin{mathpar}
    \inferrule{\Gamma \vdash \mu : \Delta}{\Gamma \vdash \unrestrict
    \mu : \Sigma(\Delta)} \and
    \inferrule{\Gamma \vdash \mu = \mu'}{\Gamma \vdash \unrestrict
    \mu = \unrestrict \mu'}
  \end{mathpar}
  and so the inference rules
  \begin{mathpar}
    \inferrule{\Gamma \vdash \sigma : \Delta}{\Sigma(\Gamma) \vdash
    \Sigma(\sigma) : \Sigma(\Delta)} \and
    \inferrule{\Gamma \vdash \sigma = \tau} {\Sigma(\Gamma) \vdash
    \Sigma(\sigma) = \Sigma(\tau)}
  \end{mathpar}
  hold for \(\sigma : \arr \Delta \star \Gamma\) and \(\tau : \arr
  {\Delta'} \star \Gamma\).
\end{proposition}
\begin{proof}
  The rules concerning the unrestriction operation follow by simple
  induction on the typing judgement or equality in the premise, and
  in fact do not need the suspension condition.

  The remainder of the rules follow from a routine mutual induction
  on all typing and equality rules, which can be found in
  \module{Catt.Suspension.Typing}. The suspendability of the
  operation set is used for the case involving the typing rule for
  coherences, which also makes use of \cref{lem:susp-vs-prop}. In
  this case, the functoriality of suspension is used to show that the
  coherence has the correct type. The suspension condition is used
  for the rule constructor of the equality of terms.
\end{proof}

Similarly to the weakening condition, the suspension condition is
closed under unions of rule sets, and we can show it is satisfied by
\dr, with a similar proof to the proof for weakening.

\begin{proposition}
  \label{prop:dr-susp}
  The set \dr satisfies the suspension condition.
\end{proposition}
\begin{proof}
  It is sufficient to prove that for all \(\Gamma : \Ctx\), \(A :
  \Type_\Gamma\), and \(t : \Term_\Gamma\) that:
  \[(\Sigma(\Gamma), \Coh {\Sigma(D^n)} {\Sigma(\wk(U^n))}
  {\Sigma(\{A,t\})}, \Sigma(t)) \in \dr\]
  when \(n = \dim(A)\). By \cref{item:disc-prop-susp}, we get that
  \(\Sigma(D^n) \equiv D^{n+1}\) and \(\Sigma(\wk(U^n)) \equiv
  \wk(\Sigma(U^n)) \equiv \wk(U^{n+1})\). By
  \cref{item:disc-prop-sub-susp}, \(\Sigma(\{A,t\}) \equiv
  \{\Sigma(A),\Sigma(t)\}\). Therefore, it is sufficient to show that:
  \[(\Sigma(\Gamma), \Coh {D^{n+1}} {\wk(U^{n+1})}
  {\{\Sigma(A),\Sigma(t)\}}, \Sigma(t)) \in \dr\]
  which is clear as \(\dim(\Sigma(A)) = \dim(A) + 1 = n+1\).
\end{proof}

\paragraph{Substitution condition}

The substitution condition takes a slightly different form to the
previous two conditions. Instead of requiring that the rule set is
closed under application of any arbitrary substitution \(\sigma\), we
instead only ensure it is closed under well-formed substitutions.
This will not prevent us proving that typing is closed under the
application of substitutions, but will be critical in proving that
the supported rules construction, which will be given in
\cref{def:rule-with-supp} and is used for proving the support
condition, satisfies the substitution condition.

\begin{definition}
  An equality rule set \(\mathcal{R}\) satisfies the
  \emph{\(\mathcal{R}'\)-substitution condition} if:
  \[ (\Gamma, s \sub \sigma, t\sub \sigma) \in \mathcal{R} \]
  whenever \((\Delta, s, t) \in \mathcal{R}\) and \(\sigma : \arr
  \Delta \star \Gamma\) with \(\Gamma \vdash_{\mathcal{R}'} \sigma :
  \Delta\). We say the set \(\mathcal{R}\) satisfies the
  \emph{substitution condition} if it satisfies the
  \(\mathcal{R}\)-substitution condition.
\end{definition}

We make two comments about this definition:
\begin{itemize}
  \item We only close under substitutions with type part \(\star\).
    It will still be possible that typing is preserved by arbitrary
    (well-formed) substitutions when combined with the suspension condition.
  \item We introduce a second rule set \(\mathcal{R}'\) in the
    definition, which is only used for the typing premise of the
    substitution \(\sigma\). The reason for this is that the
    substitution condition is not closed under unions, and so we will
    instead prove that certain rule sets satisfy the
    \(\mathcal{R}'\)-substitution condition for an arbitrary
    \(\mathcal{R}'\), a condition which is closed under unions.
\end{itemize}
The substitution condition allows us to give the next proposition.

\begin{proposition}
  \label{prop:sub-prop-1}
  Suppose \(\mathcal{R}\) satisfies the substitution condition. For
  any \(\sigma : \arr \Delta \star \Gamma\), the following rules are admissible:
  \begin{mathpar}
    \inferrule{\Delta \vdash A \\ \Gamma \vdash \sigma :
    \Delta}{\Gamma \vdash A \sub \sigma}\and
    \inferrule{\Delta \vdash s : A \\ \Gamma \vdash \sigma :
    \Delta}{\Gamma \vdash s \sub \sigma : A \sub \sigma}\and
    \inferrule{\Delta \vdash \tau : \Theta \\ \Gamma \vdash \sigma :
    \Delta}{\Gamma \vdash \tau \bullet \sigma : \Theta}\\
    \inferrule{\Delta \vdash A = B\\ \Gamma \vdash \sigma :
    \Delta}{\Gamma \vdash A \sub \sigma = B \sub \sigma}\and
    \inferrule{\Delta \vdash s = t \\ \Gamma \vdash \sigma :
    \Delta}{\Gamma \vdash s \sub \sigma = t \sub \sigma}\and
    \inferrule{\Delta \vdash \tau = \mu \\ \Gamma \vdash \sigma :
    \Delta}{\Gamma \vdash \tau \bullet \sigma = \mu \bullet \sigma}
  \end{mathpar}
  If \(\mathcal{R}\) additionally satisfies the suspension
  conditions, then all the above rules are admissible for any
  substitution \(\sigma : \arr \Delta B \Gamma\).
\end{proposition}
\begin{proof}
  The proof for a non-extended substitution is given by another
  routine mutual induction in
  \module{Catt.Typing.Properties.Substitution}. For an arbitrary
  substitution \(\sigma : \arr \Delta B \Gamma\), we also proceed by
  mutual induction, but for the application of the substitution to an
  equality of terms \(s\) and \(t\) we further split on \(B\). If \(B
  = \star\), then the proof for non-extended substitutions can be
  used. Otherwise, we have:
  \begin{align*}
    s \sub \sigma &\equiv \Sigma s \sub {\unrestrict \sigma}\\
    &= \Sigma t \sub {\unrestrict \sigma}\\
    &\equiv t \sub \sigma
  \end{align*}
  with the non-syntactic equality following from the preservation of
  equality by suspension and inductive hypothesis. The proofs that
  the extended versions of these rules are admissible are found in
  \module{Catt.Typing.Properties.Substitution.Suspended}.
\end{proof}

We also prove that application of substitution respects equality in
its second argument, which does not in fact need the substitution
condition. This is also proved by a simple mutual induction in
\module{Catt.Typing.Properties.Substitution}.

\begin{proposition}
  \label{prop:sub-prop-2}
  The following inference rules are admissible:
  \begin{mathpar}
    \inferrule{\Gamma \vdash \sigma = \tau}{\Gamma \vdash s \sub
    \sigma = s \sub \tau}\and
    \inferrule{\Gamma \vdash \sigma = \tau}{\Gamma \vdash A \sub
    \sigma = A \sub \tau}\and
    \inferrule{\Gamma \vdash \sigma = \tau}{\Gamma \vdash \mu \bullet
    \sigma = \mu \bullet \tau}
  \end{mathpar}
  for substitutions \(\sigma : \arr \Delta A \Gamma\), \(\tau : \arr
  \Delta B \Gamma\), and \(\mu : \arr \Theta C \Delta\), term \(s :
  \Term_\Delta\), and type \(A : \Type_\Delta\).
\end{proposition}

This allows us to define a category of well-formed syntax in \Cattr,
which is well-defined by the two preceding definitions.

\begin{definition}
  Suppose \(\mathcal{R}\) satisfies the substitution and weakening
  conditions. Then we can define the \emph{syntactic category} of
  \Cattr, which by an abuse of notation we call
  \(\mathsf{Catt}_{\mathcal{R}}\), to have:
  \begin{itemize}
    \item Objects given by contexts \(\Gamma\) where \(\Gamma \vdash\).
    \item Morphisms \(\Delta \to \Gamma\) given by substitutions
      \(\sigma : \arr \Delta \star \Gamma\) where \(\Gamma \vdash
      \sigma : \Delta\) quotiented by the relation which equates
      substitutions \(\sigma\) and \(\tau\) when \(\Gamma \vdash
      \sigma = \tau\).
    \item The identity morphism \(\Gamma \to \Gamma\) given by \(\id_\Gamma\).
    \item Composition is given by \(\tau \circ \sigma = \sigma \bullet \tau\).
  \end{itemize}
  By \cref{cor:id-sub-ty}, the identity substitution is a
  well-defined morphism, and the above two propositions prove that
  composition is well-defined. Composition satisfies associativity
  and unitality by \cref{prop:categorical}.
\end{definition}

By taking the weakening of the identity substitution \(\id_\Gamma :
\Gamma \to \Gamma\), we get a substitution:
\[ \proj_{\Gamma} = \wk(\id_\Gamma) : \Gamma \to \Gamma, (x : A)\]
which includes \(\Gamma\) into \(\Gamma, x : A\). It can be checked
(and is given by
  \func{Catt.Syntax.Properties}{apply-project-is-wk-tm} in the
formalisation) that applying this substitution to a term is the same
operation as weakening the term. Using this, the following can be proved:

\begin{lemma}
  Suppose \(\mathcal{R}\) satisfies the substitution condition. Then
  it also satisfies the weakening condition.
\end{lemma}
\begin{proof}
  For \((\Gamma,s ,t) \in \mathcal{R}\) and \(A : \Type_\Gamma\), we
  must prove that:
  \[ ((\Gamma,(x:A)),\wk(s),\wk(t)) \equiv ((\Gamma, (x : A)), s \sub
  {\proj_{\Gamma}}, t \sub {\proj_{\Gamma}}) \in \mathcal{R} \]
  which will follow from the substitution condition if it can be proved that
  \[ \Gamma, x : A \vdash_{\mathcal{R}} \proj_\Gamma : \Gamma \]
  holds. This judgement is easy to derive when \(\mathcal{R}\)
  satisfies the weakening condition, but this is what we are trying
  to prove. Instead, since \(\emptyset\) trivially satisfies the
  weakening condition, \(\proj_\Gamma\) is well-formed in \Catt, and
  so the derivation above follows from \cref{cor:catt-to-r}.
\end{proof}

We lastly show that \dr also satisfies the substitution condition.

\begin{proposition}
  \label{prop:dr-sub}
  The set \dr satisfies the \(\mathcal{R}\)-substitution condition
  for any equality set \(\mathcal{R}\).
\end{proposition}
\begin{proof}
  The proof is similar to \cref{prop:dr-weak,prop:dr-susp}, and
  follows from the equality \(\{A,t\} \bullet \sigma \equiv \{A \sub
  \sigma, t \sub \sigma\}\) which holds by \cref{item:disc-prop-sub-sub} .
\end{proof}

\begin{remark}
  The proof of the substitution condition for \dr makes no use of the
  typing of \(\sigma\). In fact this premise is only necessary for
  the supported rules construction which will be given in
  \cref{def:rule-with-supp}
\end{remark}

\paragraph{Tameness}

We can now define tameness.

\begin{definition}
  An equality rule set \(\mathcal{R}\) is tame if it satisfies the
  weakening, substitution, and suspension conditions. An operation
  set \(\mathcal{O}\) is tame if it is suspendable and contains the
  standard operations. A theory generated by \(\mathcal{R}\) and
  \(\mathcal{O}\) is tame if both \(\mathcal{R}\) and \(\mathcal{O}\) are.
\end{definition}

\begin{proposition}
  The set \dr is tame.
\end{proposition}

In the formalisation, each module is parameterised by the various
conditions that the module needs, and where possible we avoid using
extra unnecessary conditions. Given that every theory we will
consider in this thesis is tame, and that it is hard to imagine a
sensible theory that isn't tame, the argument could be made that the
effort put into making distinctions between these conditions is
wasted or at least unnecessary.

The case for including the weakening condition is especially
unconvincing as it is implied by the substitution condition which
likely holds in any theory of significant interest. It is however
included here as it is used in the formalisation, where its
introduction is an artefact of the natural progression of this research.

To this end, from \cref{sec:operations-catt}, we will assume that the
theory we are working over is tame, and build a library of
constructions and results that work in any tame theory, even when
some results may not need all the conditions above.

Since we have limited use for proving properties about theories that
do not satisfy the substitution condition, we could have instead
enforced that all theories respect substitution by adding a
constructor to the (term) equality relation that takes an equality
\(\Delta \vdash s = t\) and typing relation \(\Gamma \vdash \sigma :
\Delta\) to an equality \(\Gamma \vdash s \sub \sigma = t \sub
\sigma\). This may remove some overhead of setting up the weakening
and substitution conditions. It would also allow more minimal
equality rule sets to be given, as a rule set such as disc removal
could be given by
\[ \{(D^n, \Coh {D^n} {\wk(U^n)} {\id_{D^n}}, d_n) \mid n \in \mathbb{N}\} \]
On the other hand, including the extra constructor would effectively
add an extra case to each inductive proof, and it is less clear how
to minimise some of the equality rules that will be introduced in
\cref{sec:operations-catt}. Taking either approach would likely lead
to a similar development of the theory.

\subsection{Further conditions}
\label{sec:further-conditions}

Knowing that the theory we are working in is tame will be sufficient
for giving most of the constructions and proofs in
\cref{sec:operations-catt}. Here we introduce some extra conditions
that instead serve to aid in the proof of metatheoretic properties of
the generated theory. These conditions take the form of predicates on
each rule in the equality rule sets, rather than being closure
properties as the conditions for tameness were.

\paragraph{Support condition}

The support of a term plays a central role in classifying the
operations of the theory (see \cref{sec:support}). Although it is
known that support is respected by syntactic equality, we have not
yet shown it is preserved by definitional equality. The following
condition allows this to be proved.

\begin{definition}
  A set \(\mathcal{R}\) satisfies the \emph{\(\mathcal{R}'\)-support
  condition} for an equality set \(\mathcal{R}'\) when:
  \[ \Gamma \vdash_{\mathcal{R}'} s : A \implies \Supp(s) = \Supp(t) \]
  for each \((\Gamma,s,t) \in \mathcal{R}\) and \(A : \Type_\Gamma\).
  A set \(\mathcal{R}\) satisfies the \emph{support condition} if it
  satisfies the \(\mathcal{R}\)-support condition.
\end{definition}

The use of support instead of free variables in this definition is
critical, as we do not expect the free variables of a piece of syntax
to be preserved by equality in general. As an example, we would like
to have the equality:
\[ D^1 \vdash \Coh {D^1} {U^1} {\id_{D^1}} = d_1 \]
given by disc removal, yet the free variables of each side are not
equal (though the support of each side is).

We also draw attention to typing premise. Without this, the left-hand
side of each equality rule is too unconstrained (at least with how
the equality rules are currently presented), and this condition would
fail to hold on the equality sets we introduce in this thesis. Having
this typing premise come from a separate rule set \(\mathcal{R}'\)
allows the support condition to be preserved by unions of equality
sets, similar to the substitution condition.

From the support condition, we immediately get the following
proposition, proved by mutual induction.

\begin{proposition}
  \label{prop:supp-prop}
  Let \(\mathcal{R}\) satisfy the support condition. Then the
  following rules are admissible:
  \begin{mathpar}
    \inferrule{\Gamma \vdash s = t}{\Supp(s) = \Supp(t)}\and
    \inferrule{\Gamma \vdash A = B}{\Supp(A) = \Supp(B)}\and
    \inferrule{\Gamma \vdash \sigma = \tau}{\Supp(\sigma) = \Supp(\tau)}
  \end{mathpar}
  For \(s,t: \Term_\Gamma\), \(A,B : \Type_\Gamma\) and substitutions
  \(\sigma : \arr \Delta A \Gamma\) and \(\tau : \arr \Theta B \Gamma\).
\end{proposition}

In traditional presentations of \Catt, \(\FV(t) \cup \FV(A)\) is used
instead of \(\Supp(t)\) for a term \(t\) of type \(A\). Equipped with
the support condition we can now show that these are the same.

\begin{lemma}
  The following hold when \(\mathcal{R}\) satisfies the support condition:
  \begin{lemmaenum}
  \item \(\Supp(A) = \FV(A)\) when \(\Gamma \vdash A\),
  \item \label{item:supp-sub-char} \(\Supp(\sigma) = \FV(\sigma)\)
    when \(\Gamma \vdash \sigma : \Delta\),
  \item \label{item:supp-tm-char-1} \(\Supp(t) = \Supp(A) \cup
    \FV(t)\) when \(\Gamma \vdash t : A\),
  \item \label{item:supp-tm-char-2} \(\Supp(t) = \FV(A) \cup \FV(t) =
    \Supp(A) \cup \Supp(t)\) when \(\Gamma \vdash t : A\) and
    \(\Gamma \vdash A\).
  \end{lemmaenum}
\end{lemma}
\begin{proof}
  All properties are proven by a single mutual induction on the
  typing derivations in the premises.
  \begin{enumerate}[(i)]
    \item Suppose \(\Gamma \vdash A\). If \(A \equiv \star\) then
      \(\Supp(A) = \FV(A) = \emptyset\). Instead, suppose \(A \equiv
      \arr s B t\). Then we have that \(\Gamma \vdash B\), \(\Gamma
      \vdash s : B\), and \(\Gamma \vdash t : B\) and so:
      \begin{align*}
        \Supp(A) &= \Supp(B) \cup \Supp(s) \cup \Supp(t)\\
        &= \FV(B) \cup (\FV(B) \cup \FV(s)) \cup (\FV(B) \cup \FV(t))&(*)\\
        &= \FV(B) \cup \FV(s) \cup \FV(t)\\
        &= \FV(A)
      \end{align*}
      where the equality \((*)\) is derived from the inductive
      hypothesis for (i) applied to \(B\) and the inductive
      hypothesis for (iv) applied to \(s\) and \(t\).
    \item Suppose \(\Gamma \vdash \sigma : \Delta\). If \(\sigma
      \equiv \langle A \rangle\) then \(\Gamma \vdash A\) and so:
      \[\Supp(\sigma) = \Supp(A) = \FV(A) = \FV(\sigma)\]
      If instead \(\sigma \equiv \langle \tau, t \rangle\) and
      \(\Delta = \Theta, (x : A)\) then \(\Gamma \vdash \tau :
      \Theta\) and \(\Gamma \vdash t : A \sub \tau\) and so:
      \begin{align*}
        \Supp(\sigma) &= \Supp(\tau) \cup \Supp(t)\\
        &= \Supp(\tau) \cup (\Supp(A \sub \tau) \cup \FV(t))&(*)\\
        &= \DC_\Gamma(\FV(\tau) \cup \FV(A \sub \tau)) \cup \FV(t)\\
        &= \Supp(\tau) \cup \FV(t)&\text{as }\FV(A \sub \tau)
        \subseteq \FV(\tau)\\
        &= \FV(\tau) \cup \FV(t)&(\dagger)\\
        &= \FV(\sigma)
      \end{align*}
      where the equality \((*)\) is derived from the inductive
      hypothesis for (iii) applied to \(t\) and the equality
      \((\dagger)\) is derived from the inductive hypothesis for (ii)
      applied to \(\tau\).
    \item Suppose \(\Gamma \vdash t : A\). We then split on the
      constructor used for the typing derivation:

      If the derivation is the result of a conversion rule applied to
      \(\Gamma \vdash t : B\) and \(\Gamma \vdash A = B\), then
      inductive hypothesis gives \(\Supp(t) = \Supp(B) \cup \FV(t)\)
      and \cref{prop:supp-prop} gives \(\Supp(A) = \Supp(B)\) and so
      \(\Supp(t) = \Supp(A) \cup \FV(t)\) as required.

      If the derivation is derived from the typing rule for
      variables, then a simple induction on the context \(\Gamma\),
      using that \(\Supp(\wk(A)) = \Supp(A)\), gives the required result.

      If the derivation is given by the typing rule for coherences
      then \(t \equiv \Coh \Delta B \sigma\), \(\Gamma \vdash \sigma
      : \Delta\), and \(A \equiv B \sub \sigma\). Therefore,
      \begin{align*}
        \Supp(t) &= \Supp(\sigma)\\
        &= \DC_\Gamma(\FV(B \sub \sigma) \cup \FV(\sigma))&\text{as
        }\FV(B \sub \sigma) \subseteq \FV(\sigma)\\
        &= \Supp(A) \cup \Supp(\sigma)\\
        &= \Supp(A) \cup \FV(\sigma)&(*)\\
        &= \Supp(A) \cup \FV(t)
      \end{align*}
      where the equality \((*)\) is the result of applying the
      inductive hypothesis for (ii) to \(\sigma\).
    \item If \(\Gamma \vdash t : A\) and \(\Gamma \vdash A\) then:
      \[ \Supp(t) = \Supp(A) \cup \FV(t) = \FV(A) \cup \FV(t) \]
      trivially follows from (i) and (iii) and:
      \[ \Supp(t) = \DC_\Gamma(\Supp(t)) = \DC_\Gamma(\FV(A) \cup
      \FV(t)) = \Supp(A) \cup \Supp(t) \]
      with the first equality resulting from the idempotency of the
      downwards closure operator.
  \end{enumerate}
  This proof is formalised in \module{Catt.Typing.Properties.Support}.
\end{proof}
\begin{corollary}
  \label{cor:dc-sub}
  Let \(\mathcal{R}\) satisfy the support condition and suppose
  \(\Gamma \vdash \sigma : \Delta\). Then the following equality holds:
  \[ \DC_\Gamma(V \sub \sigma) = \DC_\Delta(V) \sub \sigma \]
  for all \(V \subseteq \Var(\Delta)\); downwards closure commutes
  with the application of \(\sigma\) to variable sets.
\end{corollary}
\begin{proof}
  Proceed by induction on \(\Delta\). If \(\Delta \equiv \emptyset\)
  then the equation is trivial. Therefore, assume \(\Delta \equiv
  \Theta, (x : A)\) and so \(\sigma \equiv \langle \tau , t \rangle\)
  with \(\Gamma \vdash \tau : \Theta\) and \(\Gamma \vdash t : A \sub
  \tau\) by case analysis. We now split on whether \(x \in V\).

  If \(x \not\in V\) then \(\DC_\Gamma(V \sub \sigma) = \DC_\Gamma(V
  \sub \tau) = \DC_\Theta(V) \sub \tau = \DC_\Delta(V) \sub \tau\)
  with the second equality due to inductive hypothesis. Otherwise,
  \(x \in V\) and so letting \(U = V \setminus \{x\}\) we get the equality:
  \begin{align*}
    \DC_\Gamma(V \sub \sigma) &= \DC_\Gamma(U \sub \tau \cup \FV(t))\\
    &= \DC_\Gamma(U \sub \tau) \cup \Supp(t)\\
    &= \DC_\Gamma(U \sub \tau) \cup \Supp(A \sub \tau) \cup \FV(t)&(\dagger)\\
    &= \DC_\Gamma(U \sub \tau) \cup \DC_\Gamma(\FV(A) \sub \tau) \cup \FV(t) \\
    &= \DC_\Gamma(U \sub \tau \cup \FV(A) \sub \tau)  \cup \FV(t)\\
    &= \DC_\Gamma((U \cup \FV(A)) \sub \tau) \cup \FV(t)\\
    &= \DC_\Theta(U \cup \FV(A)) \sub \tau \cup \FV(t)&(*)\\
    &= (\{x\} \cup \DC_\Theta(U \cup \FV(A))) \sub \sigma \\
    &= \DC_\Delta(V) \sub \sigma
  \end{align*}
  where equality \((*)\) is by inductive hypothesis and equality
  \((\dagger)\) is by \cref{item:supp-tm-char-1}.
\end{proof}

Unfortunately, proving that the support condition holds for most
equality rule sets is not as trivial as the proofs for the tameness
properties. Consider the case for disc removal, which gives rise to the equality
\[ \Gamma \vdash \Coh {D^n} {\wk(U^n)} {\{A,t\}} = t \]
To prove the support condition for this case we need to show that:
\[ \Supp(\{A,t\}) = \Supp(t) \]
where we can assume that \(\Gamma \vdash t : A\). Intuitively this
should hold, as the support of a substitution should be equal to the
support of the locally maximal arguments, and if the derivation
\(\Gamma \vdash t : A\) held in \Catt, we would be able to prove
this. However, this proof (and intuition) relies on the derivation
\(\Gamma \vdash_{\mathcal{R}} t : A\) holding in a theory generated
by \(\mathcal{R}\) where \(\mathcal{R}\) already satisfies the
support condition, without which the typing derivation offers little utility.

We therefore introduce a proof strategy for showing that the support
condition holds. The key insight of this strategy is to prove by
induction that every equality and every typing derivation in the
system is well-behaved with respect to support. Then, for the case of
an equality \(\Gamma \vdash s = t\) arising from a rule \((\Gamma, s,
t)\), we have \(\Gamma \vdash s : A\) as a premise and so by
inductive hypothesis can assume that this typing derivation is
well-behaved with respect to support.

We formalise this with the following definition, called the
\emph{supported rules} construction:
\begin{definition}
  \label{def:rule-with-supp}
  Let \(\mathcal{R}\) be some equality rule set. The \emph{supported
  rules} construction applied to \(\mathcal{R}\) produces the
  equality rule set \(\mathcal{R}_{\mathsf{S}}\), given by:
  \[ \mathcal{R}_{\mathsf{S}} = \{ (\Gamma, s, t) \in \mathcal{R}
  \mid \Supp(s) = \Supp(t)\} \]
  The rule set \(\mathcal{R}_{\mathsf{S}}\) satisfies the support
  condition by construction.
\end{definition}

The proof strategy then proceeds as follows: to prove that
\(\mathcal{R}\) satisfies the support condition, we instead prove
that \(\mathcal{R}\) satisfies the
\(\mathcal{R}_{\mathsf{S}}\)-support condition, leveraging that
\(\mathcal{R}_\mathsf{S}\) itself satisfies the support condition.
The proof is then completed by the following lemma:
\begin{lemma}
  \label{lem:proof-strat-supp}
  Let \(\mathcal{R}\) be an equality rule set that satisfies the
  \(\mathcal{R}_{\mathsf{S}}\)-support condition. Then the following
  inference rules are admissible:
  \begin{mathpar}
    \inferrule{\Gamma \vdash_\mathcal{R} A}{\Gamma
    \vdash_{\mathcal{R}_\mathsf{S}} A}\and
    \inferrule{\Gamma \vdash_\mathcal{R} s : A}{\Gamma
    \vdash_{\mathcal{R}_\mathsf{S}} s : A}\and
    \inferrule{\Gamma \vdash_\mathcal{R} \sigma : \Delta}{\Gamma
    \vdash_{\mathcal{R}_\mathsf{S}} \sigma : \Delta}\and
    \inferrule{\Gamma \vdash_\mathcal{R} A = B}{\Gamma
    \vdash_{\mathcal{R}_\mathsf{S}} A = B}\and
    \inferrule{\Gamma \vdash_\mathcal{R} s = t}{\Gamma
    \vdash_{\mathcal{R}_\mathsf{S}} s = t}\and
    \inferrule{\Gamma \vdash_\mathcal{R} \sigma = \tau}{\Gamma
    \vdash_{\mathcal{R}_\mathsf{S}} \sigma = \tau}
  \end{mathpar}
  and hence \(\mathcal{R}\) satisfies the support condition.
\end{lemma}
\begin{proof}
  The inference rules are all proven using a mutual induction on all
  typing and equality rules, using that \(\mathcal{R}\) satisfies the
  \(\mathcal{R}_\mathsf{S}\)-support condition in the case where the
  equality \(\Gamma \vdash s = t\) is derived from a rule \((\Gamma,
  s, t) \in \mathcal{R}\). This induction is formalised in
  \module{Catt.Support.Typing}.

  The set \(\mathcal{R}\) then satisfies the support condition as if
  \((\Gamma,s,t) \in \mathcal{R}\) and \(\Gamma \vdash_{\mathcal{R}}
  s : A\), then \(\Gamma \vdash_{\mathcal{R}_{\mathsf{S}}} s : A\)
  holds by the first part of the lemma and so \(\Supp(s) = \Supp(t)\)
  as \(\mathcal{R}\) is already known to satisfy the
  \(\mathcal{R}_{\mathsf{S}}\)-support condition.
\end{proof}

\begin{remark}
  The original motivation for parameterising \Catt by an arbitrary
  set of equality rules \(\mathcal{R}\) was not to share proofs
  between \Cattsu and \Cattsua but was to be able to state the
  supported rules construction.
\end{remark}

To be able to prove that \(\mathcal{R}\) satisfies the
\(\mathcal{R}_{\mathsf{S}}\)-support condition, we will commonly need
to know that \(\mathcal{R}_{\mathsf{S}}\) satisfies various tameness
conditions, which are given by the next lemma.

\begin{lemma}
  \label{lem:supp-sat-conds}
  Let \(\mathcal{R}\) be any equality set. Then
  \(\mathcal{R}_{\mathsf{S}}\) satisfies the weakening, suspension,
  and substitution conditions if \(\mathcal{R}\) respects the
  corresponding condition.
\end{lemma}
\begin{proof}
  Let \((\Gamma, s, t) \in \mathcal{R}\) be an arbitrary rule. To
  show \(\mathcal{R}_{\mathsf{S}}\) satisfies the weakening condition
  we need to show that:
  \[ (\Gamma, s, t) \in \mathcal{R}_{\mathsf{S}} \implies ((\Gamma,
  (x : A)), \wk(s), \wk(t)) \in \mathcal{R}_{\mathsf{S}} \]
  By assumption, \((\Gamma, \wk(s), \wk(t)) \in \mathcal{R}\) and by
  the premise of the implication we have \(\Supp(s) = \Supp(t)\).
  From this it follows that \(\Supp(\wk(s)) = \Supp(\wk(t))\) and so
  the conclusion of the implication holds.

  The case for suspension is similar except we need to use the equality:
  \[ \Supp(\Sigma(s)) = \Sigma(\Supp(s)) = \Sigma(\Supp(t)) =
  \Supp(\Sigma(t)) \]
  derived from \cref{lem:susp-vs-prop} and \(\Supp(s) = \Supp(t)\)
  from the premise of the implication.

  For the substitution condition we need to show that:
  \[ \Supp(s) = \Supp(t) \implies \Supp(s \sub \sigma) = \Supp(t \sub \sigma) \]
  under the assumption that \(\Delta \vdash_{\mathcal{R}_\mathsf{S}}
  \sigma : \Gamma\). Since \(\mathcal{R}_\mathsf{S}\) satisfies the
  support rule, we can use \cref{cor:dc-sub} to get:
  \[ \Supp(s \sub \sigma) = \DC_\Gamma(\FV(s) \sub \sigma) = \Supp(s)
    \sub \sigma = \Supp(t) \sub \sigma = \DC_\Gamma(\FV(t) \sub \sigma)
  = \Supp(t \sub \sigma) \]
  as required.
\end{proof}

We now prove the appropriate support condition for disc removal.

\begin{proposition}
  \label{prop:dr-supp}
  Let \(\mathcal{R}\) satisfy the support and weakening conditions.
  Then the set \(\dr\) satisfies the \(\mathcal{R}\)-support condition.
\end{proposition}
\begin{proof}
  It is sufficient to prove that given \(s : \Term_\Gamma\), \(A :
  \Type_\Gamma\), and \(n = \dim(A)\) that:
  \[\Gamma \vdash_{\mathcal{R}} \Coh {D^n} {\wk(U^n)} {\{A,t\}} : B
  \implies \Supp(\{A,t\}) = \Supp(t) \]
  Assume the premise of the implication. Then \(\Gamma
  \vdash_{\mathcal{R}} \{A,t\} : D^n\) by case analysis on the typing
  derivation and so \(\Gamma \vdash_{\mathcal{R}} A\) and \(\Gamma
  \vdash_{\mathcal{R}} t : A\) by \cref{lem:disc-typing} as
  \(\mathcal{R}\) satisfies the weakening condition.

  By a simple induction, it can be shown that \(\Supp(\{A,t\}) =
  \Supp(A) \cup \Supp(t)\). By \cref{item:supp-tm-char-2} we have
  \(\Supp(t) = \Supp(A) \cup \Supp(t)\) as \(\mathcal{R}\) satisfies
  the support condition and so \(\Supp(\{A,t\}) = \Supp(t)\) as required.
\end{proof}

\paragraph{Preservation condition}

Our last condition allows us to prove preservation, the property that
typing is preserved by equality.

\begin{definition}
  A set \(\mathcal{R}\) satisfies the
  \emph{\(\mathcal{R}'\)-preservation condition} for an equality set
  \(\mathcal{R}'\) when:
  \[ \Gamma \vdash_{\mathcal{R}'} s : A \implies \Gamma
  \vdash_{\mathcal{R}'} t : A \]
  for each \((\Gamma, s, t) \in \mathcal{R}\) and \(A :
  \Type_\Gamma\). The set \(\mathcal{R}\) satisfies the
  \emph{preservation condition} if it satisfies the
  \(\mathcal{R}\)-preservation condition.
\end{definition}

When a rule set \(\mathcal{R}\) has all the properties presented in
this section, we are able to show preservation for the generated theory.

\begin{proposition}
  Let \(\mathcal{R}\) satisfy the support condition and preservation
  condition, as well as being tame. Then the following inference
  rules are admissible:
  \begin{mathpar}
    \inferrule{\Gamma \vdash A\\ \Gamma \vdash A = B}{\Gamma \vdash B}\and
    \inferrule{\Gamma \vdash s : A\\ \Gamma \vdash s = t \\ \Gamma
    \vdash A = B}{\Gamma \vdash t : B}\and
    \inferrule{\Gamma \vdash \sigma : \Delta\\ \Gamma \vdash \sigma =
    \tau}{\Gamma \vdash \tau : \Delta}
  \end{mathpar}
  for \(A, B : \Type_\Gamma\), \(s,t : \Term_\Gamma\), \(\sigma :
  \arr \Delta A \Gamma\), and \(\tau : \arr \Delta B \Gamma\).
\end{proposition}
\begin{proof}
  We prove the following bidirectional versions of the inference
  rules by mutual induction on the equality derivation:
  \begin{alignat*}{5}
    &\Gamma \vdash A = B &&\implies (\Gamma \vdash A \iff \Gamma \vdash B)\\
    &\Gamma \vdash s = t &&\implies (\forall A.\ \Gamma \vdash s : A
    \iff \Gamma \vdash t : A)\\
    &\Gamma \vdash \sigma = \tau &&\implies (\Gamma \vdash \sigma :
    \Delta \iff \Gamma \vdash \tau : \Delta)
  \end{alignat*}
  which imply the inference rules of the proposition are admissible
  (using the conversion rule for the second rule).

  The only non-trivial cases are for the statement for terms. We
  split on the equality derivation \(\Gamma \vdash s = t\). The cases
  for reflexivity on variables and transitivity are also trivial. The
  case for symmetry follows from the symmetry of the ``if and only
  if'' relation.

  Now suppose the equality is of the form \(\Coh \Delta A \sigma =
  \Coh \Delta B \tau\) and is derived from the equality rule for
  coherences from equalities \(\Delta \vdash A = B\) and \(\Gamma
  \vdash \sigma = \tau\). We prove the first direction, with the
  second following symmetrically. We therefore assume we have a
  typing derivation \(\Gamma \vdash \Coh \Delta A \sigma : C\), and
  will induct on this derivation to construction a derivation of
  \(\Gamma \vdash \Coh \Delta B \tau : C\).
  \begin{itemize}
    \item If the derivation is constructed with the conversion rule
      from \(\Gamma \vdash \Coh \Delta A \sigma : D\) and \(\Gamma
      \vdash D = C\), then we get a derivation \(\Gamma \vdash \Coh
      \Delta B \tau : D\) by inductive hypothesis and can apply the
      conversion rule to get a derivation \(\Gamma \vdash \Coh \Delta
      B \tau : C\).
    \item If instead the derivation is constructed with the coherence
      rule then \(C \equiv A \sub \sigma\) and  \(A \equiv \arr s
      {A'} t\) and therefore \(B \equiv \arr {u} {B'} {v}\) with
      \(\Delta \vdash s = u\) and \(\Delta \vdash t = v\). We also
      have that \(\Delta \vdash_{\mathsf{ps}}\), \((\Delta, \Supp(s),
      \Supp(t)) \in \mathcal{O}\), \(\Delta \vdash A\), and \(\Gamma
      \vdash \sigma : \Delta\). By the inductive hypothesis on the
      equality, we have \(\Delta \vdash B\) and \(\Gamma \vdash \tau
      : \Delta\). By \cref{prop:supp-prop}, \(\Supp(s) = \Supp(u)\)
      and \(\Supp(t) = \Supp(v)\) and so \((\Delta, \Supp(u),
      \Supp(v)) \in \mathcal{O}\). Hence, by the coherence rule we
      have \(\Gamma \vdash \Coh \Delta B \tau : B \sub \tau\). By
      \cref{prop:sub-prop-1,prop:sub-prop-2}, \(\Gamma \vdash A \sub
      \sigma = B \sub \tau\) and so by the conversion rule we obtain
      a derivation \(\Gamma \vdash \Coh \Delta B \tau : C\).
  \end{itemize}

  Finally, suppose the equality is derived from \textsc{rule}, such
  that \((\Gamma, s,t) \in \mathcal{R}\) and \(\Gamma \vdash s : A\).
  If \(\Gamma \vdash s : B\), then the preservation condition gives a
  derivation \(\Gamma \vdash t : B\). Conversely, if \(\Gamma \vdash
  t : B\), then we need to show that \(\Gamma \vdash A = B\). By
  applying the preservation condition to the derivation \(\Gamma
  \vdash s : A\), we get a derivation \(\Gamma \vdash t : A\). Then
  by \cref{lem:ty-unique}, we have \(\Gamma \vdash A = B\) and so the
  proof is complete by applying the conversion rule to the derivation
  \(\Gamma \vdash s : A\).
\end{proof}

As with the other conditions, we end this section by showing that \dr
satisfies the preservation condition.

\begin{proposition}
  \label{prop:dr-preserve}
  Suppose \(\mathcal{R}\) satisfies the weakening condition, and the
  set of operations \(\mathcal{O}\) contains the standard operations.
  Then \dr satisfies the \(\mathcal{R}\)-preservation condition.
\end{proposition}
\begin{proof}
  Take \((\Gamma, \Coh {D^n} {\wk(U^n)} {\{A,t\}}, t) \in \dr\) and
  suppose \(\Gamma \vdash \Coh {D^n} {U^n} {\{A,t\}} : B\). Then by
  \cref{lem:ty-unique}:
  \[\Gamma \vdash B = \wk(U^n) \sub {\{A,t\}} \equiv A\]
  By \cref{lem:disc-typing}, \(\Gamma \vdash t : A\) and so by the
  conversion rule \(\Gamma \vdash t : B\) as required.
\end{proof}

\subsection{Endo-coherence removal}
\label{sec:ecr}

We conclude this chapter with a second example of a family of
equality rules called \emph{endo-coherence removal}. As suggested by
the name, these equalities simplify a class of terms known as endo-coherences.

\begin{definition}
  An \emph{endo-coherence} is a coherence term \(\Coh \Delta {\arr s
  A s} \sigma\).
\end{definition}

If we consider the (ps-context):
\[ \Delta = (x : \star) (y : \star) (f : \arr x \star y) (z : \star)
(g : \arr y \star z) \]
then we see that there are two distinct endo-coherences with source
and target \(f * g\), the identity on \(f * g\) and the ``fake
identity'' \(\Coh \Delta {f*g \to f*g} {\id_\Delta}\). In the type
theories \Cattsu and \Cattsua introduced in
\cref{sec:cattsu,sec:cattsua}, identities will be privileged, and
these fake identities will be reduced to the true identity.

More generally, for each term \(t\) there is a canonical
endo-coherence with source and target \(t\), the identity on \(t\).
Endo-coherence removal simplifies any other endo-coherence on that
term to an identity. It makes the following rule admissible:
\begin{mathpar}
  \inferrule{\Delta \vdash_{\mathsf{ps}} \\ \Delta \vdash A \\ \Delta
    \vdash s : A \\ \Supp(s) = \Var(\Delta) \\ \Gamma \vdash \sigma :
  \Delta}{\Gamma \vdash \Coh \Delta {\arr s A s} \sigma = \id(A \sub
  \sigma,s \sub \sigma)}\textsc{ecr}
\end{mathpar}
Endo-coherence removal can be assembled into the following equality rule set.

\begin{definition}
  The \emph{endo-coherence removal set}, \ecr, is the set consisting
  of the triples:
  \[ \Gamma, \Coh \Delta {\arr s A s} \sigma, \id(A\sub \sigma, s
  \sub \sigma) \]
  for contexts \(\Gamma\) and \(\Delta\), \(A : \Type_\Delta\), \(s :
  \Term_\Delta\), and \(\sigma : \arr \Delta \star \Gamma\).

  A set of rules \(\mathcal{R}\) \emph{contains endo-coherence
  removal} if \(\ecr \subseteq \mathcal{R}\). We say that
  \(\mathcal{R}\) \emph{has endo-coherence removal} if the rule
  \textsc{ecr} holds in the generated theory.
\end{definition}

The set \ecr satisfies all the conditions introduced in this chapter,
as proven in the next proposition, which concludes this chapter.

\begin{proposition}
  \label{prop:ecr-props}
  Suppose the set of operations \(\mathcal{O}\) contains the standard
  operations. Then the set \ecr satisfies the following properties:
  \begin{lemmaenum}
  \item The set \ecr satisfies the weakening condition.
  \item The set \ecr satisfies the suspension condition.
  \item The set \ecr satisfies the \(\mathcal{R}\)-substitution
    condition, for any equality set \(\mathcal{R}\).
  \item \label{item:ecr-supp} The set \ecr satisfies the
    \(\mathcal{R}\)-support condition, for any equality set
    \(\mathcal{R}\) satisfying the support condition.
  \item \label{item:ecr-preserve} The set \ecr satisfies the
    \(\mathcal{R}\)-preservation condition, for any equality set
    \(\mathcal{R}\) satisfying the weakening and substitution conditions.
  \end{lemmaenum}
\end{proposition}
\begin{proof}
  Suppose \((\Gamma, \Coh \Delta {\arr s A s} \sigma, \id(A \sub
  \sigma, s \sub \sigma)) \in \ecr\). To show that the substitution
  holds, we suppose that \(\tau : \arr \Gamma \star \Theta\), and
  then must prove that:
  \[ (\Theta, \Coh \Delta {\arr s A s} {\sigma \bullet \tau}, \id(A
  \sub \sigma, s \sub \sigma) \sub \tau) \in \ecr \]
  It is immediate that:
  \[ (\Theta, \Coh \Delta {\arr s A s} {\sigma \bullet \tau}, \id(A
  \sub {\sigma \bullet \tau}, s \sub {\sigma \bullet \tau})) \in \ecr \]
  and so it suffices to prove that \(\id(A \sub \sigma, s \sub
    \sigma) \sub \tau \equiv \id(A \sub {\sigma \bullet \tau},s \sub
  {\sigma \bullet \tau})\), but this follows from
  \cref{item:disc-prop-sub-sub,prop:categorical}. The weakening
  condition then follows from the substitution condition.

  For the suspension condition, it must be shown that:
  \[ (\Sigma(\Gamma), \Coh {\Sigma(\Delta)} {\arr {\Sigma(s)}
      {\Sigma(A)} {\Sigma(s)}} {\Sigma(\sigma)}, \Sigma(\id(A\sub \sigma,
  s \sub \sigma))) \in \ecr \]
  and so it suffices to show that \(\Supp(\Sigma(s)) =
  \Var(\Sigma(\Delta))\), which follows from \(\Supp(\Sigma(s)) =
  \Sigma(\Supp(s))\), and
  \[ \Sigma(\id(A \sub \sigma, s \sub \sigma)) \equiv \id(\Sigma(A)
  \sub {\Sigma(\sigma)}, \Sigma(s) \sub {\Sigma(\sigma)}) \]
  which follows from the functoriality of suspension and
  \cref{item:disc-prop-sub-susp,item:disc-prop-susp}.

  For the support condition, assume that \(\Gamma
  \vdash_{\mathcal{R}} \Coh \Delta {\arr s A s} \sigma : B\) for some
  \(B : \Type_\Gamma\) and that \(\mathcal{R}\) satisfies the support
  condition. Then:
  \begin{align*}
    \Supp(\Coh \Delta {\arr s A s} \sigma) &= \Supp(\sigma)\\
    &= \FV(\sigma)&\text{by \cref{item:supp-sub-char}}\\
    &= \Var(\Delta) \sub \sigma \\
    &= \Supp(s) \sub \sigma&\text{by assumption}\\
    &= (\Supp(A) \cup \Supp(s)) \sub \sigma &\text{by
    \cref{item:supp-tm-char-2}}\\
    &= \DC_\Delta(\FV(A) \cup \FV(s)) \sub \sigma\\
    &= \DC_\Gamma(\FV(A) \sub \sigma \cup \FV(s) \sub
    \sigma)&\text{by \cref{cor:dc-sub}}\\
    &= \DC_\Gamma(\FV(A \sub \sigma) \cup \FV(s \sub
    \sigma))&\text{by \cref{prop:vs-sub}}\\
    &= \Supp(A \sub \sigma) \cup \Supp(s \sub \sigma)\\
    &= \Supp(\id(A \sub \sigma, s \sub \sigma))
  \end{align*}
  as required.

  Lastly for the preservation condition, let \(\mathcal{R}\) satisfy
  the weakening and substitution conditions, and assume \(\Gamma
  \vdash \Coh \Delta {\arr s A s} {\sigma} : B\). By deconstructing
  the typing derivation, we must have that \(\Delta \vdash A\),
  \(\Delta \vdash s : A\), and \(\Gamma \vdash \sigma : \Delta\).
  Therefore, by \cref{prop:sub-prop-1}, \(\Gamma \vdash A \sub
  \sigma\) and \(\Gamma \vdash s \sub \sigma : A \sub \sigma\).
  Hence, by \cref{cor:id-typing}, \(\Gamma \vdash \id(A \sub \sigma,
  s \sub \sigma) : (\arr s A s) \sub \sigma\). It remains to prove
  that \(\Gamma \vdash (\arr s A s) \sub \sigma = B\), but this is
  immediate from \cref{lem:ty-unique}, applied to the derivation
  \(\Gamma \vdash \Coh \Delta {\arr s A s} \sigma : B\).
\end{proof}

\begin{figure}[t]
  \centering
  %\includegraphics[height=\textheight - 25pt]{test.pdf}
  \caption{Dependency graph of Agda formalisation.}
  \label{fig:dep-graph}
\end{figure}

\chapter{Constructions in \texorpdfstring{\boldmath\Cattr}{Cattr}}
\label{sec:operations-catt}

This chapter will investigate some more involved constructions that
can be given in the type theory \Cattr. These constructions will be
central to defining the reductions that underpin the type theories
\Cattsu and \Cattsua which appear in \cref{cha:cattstrict}. We will
give a definition of each construction, describe under what
conditions it is well-formed, and state various properties describing
the behaviour of the construction and its interaction with other constructions.

For this chapter we will assume that we are working in a tame theory,
as described in \cref{sec:tame-theories}. This means that all proofs
in this section will hold in any variant of \Cattr such that the
equality set \(\mathcal{R}\) satisfies the weakening, substitution,
and suspension conditions, and the set of operations \(\mathcal{O}\)
is suspendable and contains the standard operations. We will also use
all the relevant proofs from \cref{sec:catt-with-equality}, without
explaining exactly what condition of the set \(\mathcal{R}\) is being
used.

The formalisation is commonly more specific when specifying which
conditions are necessary for each module, for example omitting the
suspension condition when it is not needed for a specific
construction, but for the body of this text we ignore these
distinctions and simply assume that every theory we work with will be
tame, as will be case for all theories introduced in \cref{cha:cattstrict}.

This chapter builds up to the following two constructions, that can
be viewed as meta-operations on \Cattr.
\begin{itemize}
  \item The \emph{pruning} operation will be introduced in
    \cref{sec:pruning} and is the main component of the type theory
    \Cattsu, defined in \cref{sec:cattsu}, a type theory for strictly
    unital \(\infty\)-categories. Pruning removes unnecessary
    identities from a term, simplifying the resulting term in the process.
  \item The \emph{insertion} operation will be introduced in
    \cref{sec:insertion}. It powers the type theory \Cattsua, a type
    theory for strictly unital and associative \(\infty\)-categories.
    Insertion merges certain arguments to a coherence into the body
    of the coherence itself, effectively ``inserting'' the argument
    into the head term. It can be viewed as a generalisation of
    pruning, but is a more complex construction.
\end{itemize}

Both pruning and insertion perform more radical modifications to the
structure of a term than disc removal and endo-coherence removal, the
equality rules we have seen so far. Pruning and insertion modify the
pasting diagram in the coherence at the head of the term they act on.
In this chapter, more combinatorial descriptions of pasting diagrams
will be introduced to enable the pasting diagrams involved in these
constructions to be constructed by induction.

The pruning construction identifies locally maximal arguments of a
coherence that are syntactically identities, and removes these
arguments from the term, while also removing the component of the
pasting diagram in the coherence which corresponds to this argument.
Pruning could be applied to the term \(f * g * \id\), a ternary
composite, to remove the identity argument and convert the ternary
composite to a binary composite, returning the term \(f*g\).

Insertion does not just simply remove parts of a term, but flattens
the structure of a term, moving data from a locally maximal argument
into the head term. The motivating example for insertion is the term
\(f * (g * h)\), a binary composite where one of the locally maximal
arguments is itself a binary composite. Under insertion, the inner
composite \(g * h\) is merged with the outer binary composite to form
a single ternary composite \(f * g * h\).

When a locally maximal argument is an identity, it will always be
insertable, and the result of inserting the identity into the head
term will be similar to pruning the same argument, motivating the
viewpoint that insertion is a generalisation of pruning. At the end
of this chapter, this relationship will be made precise.

Insertion again performs more radical changes to the head coherence
of the term than pruning, and needs to be able to merge two pasting
diagrams into one along a locally maximal argument. The operation on
pasting diagrams is best understood as an operation on \emph{trees},
an alternative characterisation of pasting diagrams which will be
introduced in \cref{sec:trees}.

Although the definition of these trees is simple, to be able to use
them effectively we must be able to describe their relationship to
the \Catt contexts they represent. It will also be necessary to
describe the morphisms between these trees, which correspond to
substitutions between the underlying contexts, and the composition of
such morphisms.

Certain constructions on trees will not compute nicely with the
syntax in \Catt. We therefore introduce a new notion of
\emph{structured term}, an alternative syntax for \Catt which allows
more complex representations of terms over contexts derived from
trees. Structured terms effectively retain more information about how
they are constructed, allowing constructions to compute on them in
ways that are not possible on the raw syntax of \Catt. This
representation of terms will be crucial in the formalisation, as it
aids the proof assistant in simplifying various constructions. These
structured terms are defined in \cref{sec:structured-terms}.

Finally, \cref{sec:insertion} defines the constructions used in the
insertion operation, using the structured syntax from the preceding
section. In this section, many properties of insertion are stated,
including a universal property that it satisfies.

\section{Pruning}
\label{sec:pruning}

Pruning drives the strictly unital behaviour of \Cattsu. Unitality in
\(\infty\)-categories is the property that the identity acts as a
unit with respect to composition, so that composing with the unit is
equivalent to the original term. If an \(\infty\)-category is
strictly unital, then it exhibits this behaviour up to equality
rather than equivalence.

For \Catt, strict unitality means that a composition containing an
identity as one of its arguments should be definitionally equal to
the term with this argument removed. Pruning is the operation that
removes an argument from a composition, taking a term such as \(f * g
* \id\) to \(f * g\), or \(\id * f\) to the unary composite on \(f\).
In the presence of strict units, it is also desirable to simplify the
higher-dimensional data that witnessed the (weak) unitality in \Catt.
For example, the left unitor on \(f\), given by the term:
\[ \Coh {(x : \star), (y : \star), (f : \arr x \star y)} {\arr
{\id(x) * f} {} {f}} {\id} \]
which witnesses that composing on the left with an identity is
equivalent to the original term, can be simplified to the identity on
\(f\), and the triangle equations which govern the coherence laws for
the unitors can also trivialise. For this reason, pruning is defined
to be able to apply to any term which has identities as a locally
maximal argument. We review the definition of a locally maximal argument below.

\begin{definition}
  In a context \(\Gamma\), a \emph{locally maximal variable} is a
  variable \(x\) of \(\Gamma\) that does not appear in the source or
  target of any other variable of \(\Gamma\). Equivalently, \(x\) is
  locally maximal when:
  \[ x \not\in \Supp(y) \]
  for any \(y \neq x \in \Var(\Gamma)\). Given a substitution
  \(\sigma : \Delta \to \Gamma\), a \emph{locally maximal argument}
  of \(\sigma\) is a term \(x \sub \sigma\) where \(x\) is a locally
  maximal variable of \(\Delta\).
\end{definition}

\begin{example}
  \label{ex:lm}
  Consider the pasting diagram given by the following diagram:
  % https://q.uiver.app/#q=WzAsMyxbMCwwLCJ4Il0sWzIsMCwieSJdLFszLDAsInoiXSxbMCwxLCJmIiwwLHsiY3VydmUiOi01fV0sWzAsMSwiaCIsMix7ImN1cnZlIjo1fV0sWzAsMSwiZyIsMV0sWzEsMiwiaiJdLFszLDUsIlxcYWxwaGEiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzUsNCwiXFxiZXRhIiwwLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dXQ== % tex-fmt: skip
  \[
    \begin{tikzcd}
      x && y & z
      \arrow[""{name=0, anchor=center, inner sep=0}, "f"',
      curve={height=30pt}, from=1-1, to=1-3]
      \arrow[""{name=1, anchor=center, inner sep=0}, "h",
      curve={height=-30pt}, from=1-1, to=1-3]
      \arrow[""{name=2, anchor=center, inner sep=0},
      "g"{description}, from=1-1, to=1-3]
      \arrow["j", from=1-3, to=1-4]
      \arrow["\alpha", shorten <=3pt, shorten >=5pt, Rightarrow, from=0, to=2]
      \arrow["\beta", shorten <=5pt, shorten >=3pt, Rightarrow, from=2, to=1]
    \end{tikzcd}
  \]
  which corresponds to the \Catt context (written to highlight the
  dimension of each term):
  \begin{alignat*}{3}
    \Theta ={} &(x : \star),\\
    &(y : \star),{}&&(f : x \to y),\\
    &&&(g : x \to y),{}&&(\alpha : f \to g),\\
    &&&(h : x \to y),&&(\beta : g \to h),\\
    &(z : \star),&&(j : y \to z)
  \end{alignat*}
  The locally maximal variables of \(\Theta\) are \(\alpha\),
  \(\beta\), and \(j\). Note that \(j\) is locally maximal, despite
  not being of maximal dimension in the context. Pruning the context
  \(\Theta\) along locally maximal variable \(\alpha\) removes the
  variables \(\alpha\) and \(g\) from the context, and must amend the
  type of \(\beta\) so that its source is \(f\).
\end{example}

To perform the pruning construction, we start with a coherence term
\(\Coh \Delta A \sigma : \Term_\Gamma\), and assume that some locally
maximal argument of \(\sigma\) is an identity, that is \(x \sub
\sigma \equiv \id(B,t)\) for some locally maximal variable \(x\),
type \(B : \Type_\Gamma\), and term \(t : \Term_\Gamma\). We then
construct the following:
\begin{itemize}
  \item A new pasting diagram \(\Delta \sslash x\), corresponding to
    \(\Delta\) with the variable \(x\) and its target removed.
  \item A new set of arguments \(\sigma \sslash x\), consisting of
    the same terms as \(\sigma\) except those corresponding to \(x\)
    and its target.
  \item A projection substitution \(\pi_x : \Delta \to \Delta \sslash
    x\), from which a type \(A \sub {\pi_x} : \Type_{\Delta \sslash
    x}\) can be obtained. This projection sends \(x\) to the identity
    on its source, the target of \(x\) to the source of \(x\), and
    every other variable to itself.
\end{itemize}
We note that the source and target of the locally maximal variable
\(x\) are well-defined as \(x\) must be sent by \(\sigma\) to an
identity, which cannot be zero dimensional.

\subsection{Dyck words}

To be able to easily reason about the structures involved in pruning,
we wish to define them by induction. To do this we introduce a
different presentation of pasting diagrams called \emph{Dyck words},
which have a simpler inductive structure. Dyck words more directly
encode the structure of the pasting diagram, and will allow us to
give an inductive characterisation of the locally maximal variables
of the associated context.

\begin{definition}
  The set of \emph{Dyck words}, \(\Dyck_d\) of trailing dimension
  \(d\) consists of lists of ``up'' and ``down'' moves according to
  the following rules.
  \begin{mathpar}
    \inferrule{ }{\circleddash : \Dyck_0} \and
    \inferrule{d : \mathbb{N} \\\mathcal{D} : \Dyck_d}{\mathcal{D}
    \Uparrow : \Dyck_{d + 1}} \and
    \inferrule{d : \mathbb{N} \\ \mathcal{D} : \Dyck_{d +
    1}}{\mathcal{D} \Downarrow : \Dyck_d}
  \end{mathpar}
  In any prefix of a Dyck word \(D : \Dyck_d\), the number of ``up''
  moves (given by constructor \(\Uparrow\)) must be greater than or
  equal to the number of ``down'' moves (given by constructor
  \(\Downarrow\)). The difference between the number of each move is
  given by the trailing dimension \(d\).
\end{definition}

Dyck words can be given a visual interpretation as a \emph{mountain
diagram}. To obtain such a diagram we start on the left-hand side,
and draw a continuous line by drawing an upwards sloping segment for
each \(\Uparrow\) in the word, and a downwards sloping line for each
\(\Downarrow\) in the word. An example of such a diagram is given in
\cref{fig:mountain}.

\begin{figure}[ht]
  \centering
  \[
    \begin{tikzcd}[column sep = small, cells={inner sep = 0}, arrows={no head}]
      && \bullet && \bullet \\
      & \bullet && \bullet && \bullet && \bullet \\
      \bullet &&&&&& \bullet && \bullet
      \arrow[from=3-1, to=2-2]
      \arrow[from=2-2, to=1-3]
      \arrow[from=1-3, to=2-4]
      \arrow[from=2-4, to=1-5]
      \arrow[from=1-5, to=2-6]
      \arrow[from=2-6, to=3-7]
      \arrow[from=3-7, to=2-8]
      \arrow[from=2-8, to=3-9]
  \end{tikzcd}\]
  \caption[Mountain diagram]{Mountain diagram for \(\circleddash
      \Uparrow\, \Uparrow\, \Downarrow\, \Uparrow\, \Downarrow\,
  \Downarrow\, \Uparrow\, \Downarrow\, : \Dyck_0\).}
  \label{fig:mountain}
\end{figure}

The rules \(\circleddash\), \(\Uparrow\), and \(\Downarrow\) directly
correspond to the rules \textsc{pss}, \textsc{pse}, and \textsc{psd}
that generate the typing judgement for ps-contexts. From a Dyck word,
we can directly construct this context by induction.

\begin{definition}
  For a Dyck word \(\mathcal{D} : \Dyck_d\), its associated context
  \(\lfloor \mathcal{D} \rfloor\), associated type
  \(\ty_{\mathcal{D}} : \Type_{\lfloor \mathcal{D} \rfloor}\), and
  associated term \(\tm_{\mathcal{D}} : \Term_{\lfloor \mathcal{D}
  \rfloor}\) are defined by mutual induction on \(\mathcal{D}\):
  \begin{align*}
    \lfloor \circleddash \rfloor &= (x : \star)\\
    \lfloor \mathcal{D} \Uparrow \rfloor &= \lfloor \mathcal{D}
    \rfloor, (y_{\mathcal{D}} : \ty_{\mathcal{D}}), (f_{\mathcal{D}}
      : \arr {\wk(\tm_{\mathcal{D}})} {\wk(\ty_{\mathcal{D}})}
    {y_{\mathcal{D}}})\\
    \lfloor \mathcal{D} \Downarrow \rfloor &= \lfloor \mathcal{D}
    \rfloor\\[10pt]
    \ty_{\circleddash} &= \star\\
    \ty_{\mathcal{D}\Uparrow} &= \arr{\wk(\wk(\tm_{\mathcal{D}}))}
    {\wk(\wk(\ty_{\mathcal{D}}))} {y_{\mathcal{D}}}\\
    \ty_{\mathcal{D} \Downarrow} &=
    \base(\ty_{\mathcal{D}})&\text{where }\base(\arr s A t) = A\\[10pt]
    \tm_{\circleddash} &= x\\
    \tm_{\mathcal{D}\Uparrow} &= f_{\mathcal{D}}\\
    \tm_{\mathcal{D}\Downarrow} &=
    \tgt(\ty_{\mathcal{D}})&\text{where }\tgt(\arr s A t) = t
  \end{align*}
  The variable names given here are used to avoid ambiguity in the
  definition. As we consider contexts up to \(\alpha\)-equality, we
  may freely change these variable names. The \(\tgt\) and \(\base\)
  operations are well-defined here as it may be checked by a simple
  induction that \(\dim(\ty_{\mathcal{D}}) = d\) for \(\mathcal{D} :
  \Dyck_d\), ensuring that we only apply \(\tgt\) and \(\base\) to
  types of strictly positive dimension.
\end{definition}

The tight correspondence between the rules used to construct Dyck
words and ps-contexts allow an easy proof that the contexts
associated to Dyck words are in fact pasting diagrams.

\begin{lemma}
  \label{lem:dyck-typing}
  For a Dyck word \(\mathcal{D} : \Dyck_d\), its associated context,
  type, and term are all well-formed:
  \[ \lfloor \mathcal{D} \rfloor \vdash \qquad \lfloor \mathcal{D}
    \rfloor \vdash \ty_{\mathcal{D}} \qquad \lfloor \mathcal{D} \rfloor
  \vdash \tm_{\mathcal{D}} : \ty_{\mathcal{D}} \]
  In addition to being a well-formed context, the context associated
  to a Dyck word is a ps-context; the following judgement holds:
  \[ \lfloor \mathcal{D} \rfloor \vdash_{\mathsf{ps}}
  \tm_{\mathcal{D}} : \ty_{\mathcal{D}} \]
  and so if \(\mathcal{D} : \Dyck_0\), we have \(\lfloor \mathcal{D}
  \rfloor \vdash_{\mathsf{ps}}\). Further, all ps-contexts are the
  associated context of a Dyck word.
\end{lemma}
\begin{proof}
  Due to the similarity of the rules for ps-contexts and Dyck words,
  this follows quickly from simple inductions, which are given in the
  formalisation. The proofs for the typing judgements appear in
  \module{Catt.Dyck.Typing} and the proofs for the ps-context
  judgements appear in \module{Catt.Dyck.Pasting}.
\end{proof}

The locally maximal variables in the context associated to a Dyck
word correspond exactly to the points in the word where there is an
upwards move followed immediately by a downwards move, creating a
peak in the mountain diagram. These peaks can be given an inductive
characterisation.

\begin{definition}
  Let \(\mathcal{D} : \Dyck_d\) be a Dyck word. A \emph{peak} of
  \(\mathcal{D}\), \(p : \Peak_{\mathcal{D}}\) is inductively defined
  by the following rules:
  \begin{mathpar}
    \inferrule{d \in \mathbb{N} \\ \mathcal{D} : \Dyck_d}{\mathcal{D}
    \UDPeak : \mathcal{D} \Uparrow\,\Downarrow}\and
    \inferrule{d \in \mathbb{N} \\ \mathcal{D} : \Dyck_d \\ p :
    \Peak_{\mathcal{D}}}{p \UpPeak : \Peak_{\mathsf{D}} \Uparrow}\and
    \inferrule{d \in \mathbb{N} \\ \mathcal{D} : \Dyck_{d+1} \\ p :
    \Peak_{\mathcal{D}}}{p \DownPeak : \Peak_{\mathcal{D} \Downarrow}}
  \end{mathpar}
  From each peak \(p : \Peak_{\mathcal{D}}\), a term \(\lfloor p
  \rfloor\) of \(\lfloor \mathcal{D} \rfloor\) can be inductively defined by:
  \[ \lfloor \mathcal{D} \UDPeak \rfloor = f_\mathcal{D} \qquad
    \lfloor p \UpPeak \rfloor = \wk(\wk \lfloor p \rfloor) \qquad
  \lfloor p \DownPeak \rfloor = \lfloor p \rfloor\]
  The term \(\lfloor p \rfloor\) is a locally maximal variable of
  \(\lfloor \mathcal{D} \rfloor\).
\end{definition}

\begin{example}
  \label{ex:dyck-peaks}
  Recall the ps-context \(\Theta\) from \cref{ex:lm}. This context is
  the associated context of the Dyck word:
  \[ \circleddash \Uparrow\, \Uparrow\, \Downarrow\, \Uparrow\,
  \Downarrow\, \Downarrow\, \Uparrow\, \Downarrow\]
  for which the mountain diagram is given in \cref{fig:mountain}. The
  three locally maximal variables \(\alpha\), \(\beta\), and \(j\)
  correspond to the peaks:
  \[ \circleddash \Uparrow\, \UDPeak\, \UpPeak\, \DownPeak\,
    \DownPeak\, \UpPeak\, \DownPeak \qquad \circleddash \Uparrow\,
    \Uparrow\, \Downarrow\, \UDPeak\, \DownPeak\, \UpPeak\, \DownPeak
    \qquad \circleddash \Uparrow\, \Uparrow\, \Downarrow\, \Uparrow\,
  \Downarrow\, \Downarrow\, \UDPeak \]
  which themselves correspond to the three peaks of the mountain
  diagram, with the height of each peak corresponding to the
  dimension of each locally maximal variable.
\end{example}

As all disc contexts are pasting diagrams, and hence are the
associated context of a Dyck word.

\begin{definition}
  Let \(\mathcal{D}^n\) be the Dyck word with \(n\) upwards moves
  followed by \(n\) downwards moves. The equality \( \lfloor
  \mathcal{D}^n \rfloor \equiv D^n\) follows from a trivial
  induction. If \(n > 0\), There is a unique peak of
  \(\mathcal{D}^n\) with associated term \(d_n\).
\end{definition}

We lastly show that a Dyck word can be suspended, which is expected
as ps-contexts are closed under suspension. The various constructions
associated to a suspended Dyck word are equal to the same
constructions on the unsuspended Dyck word.

\begin{lemma}
  Dyck words are closed under suspension. We define the suspension of
  a Dyck word \(\mathcal{D} : \Dyck_d\) to be the Dyck word
  \(\Sigma(\mathcal{D}) : \Dyck_{d+1}\) which is obtained by
  inserting an additional up move to the start of the work, or can
  alternatively be inductively defined by:
  \[ \Sigma(\circleddash) = \circleddash \Uparrow \qquad
    \Sigma(\mathcal{D}\Uparrow) = \Sigma(\mathcal{D})\Uparrow \qquad
  \Sigma(\mathcal{D}\Downarrow) = \Sigma(\mathcal{D})\Downarrow \]
  The following equalities hold:
  \[ \lfloor \Sigma(\mathcal{D}) \rfloor = \Sigma(\lfloor \mathcal{D}
    \rfloor) \qquad \ty_{\Sigma(\mathcal{D})} =
    \Sigma(\ty_{\mathcal{D}}) \qquad \tm_{\Sigma(\mathcal{D})} =
  \Sigma(\tm_{\mathcal{D}}) \]
  for each Dyck word \(\mathcal{D}\). For each peak \(p :
  \Peak_{\mathcal{D}}\), there is an associated peak \(\Sigma(p) :
  \Peak_{\Sigma(\mathcal{D})}\) which is defined similarly.
\end{lemma}
\begin{proof}
  These properties are all proved by straight forward induction on
  \(\mathcal{D}\). The formalised proofs appear in
  \module{Catt.Dyck.Properties}.
\end{proof}

The Dyck words presented in this section can be viewed as a more
direct syntax for pasting contexts, which allow induction to be
easily performed. For this reason, most of the properties of Dyck
words follow from routine inductions, and hence are relegated to the
formalisation. The key contribution of this (sub)section is the
characterisation of locally maximal variables as peaks, which have an
easy inductive definition due to the simplicity of Dyck words.

\begin{remark}
  All locally maximal variables of ps-contexts are identified with
  peaks, except for the unique variable of the singleton context.
  This discrepancy will make no difference for pruning, as a
  \(0\)-dimensional variable could never have been sent to an
  identity and so would never have been a candidate for pruning.
\end{remark}

\subsection{The pruning construction}
\label{sec:prune-construction}
Equipped with Dyck words, and a classification of locally maximal
variables as peaks, we are now able to define each of the
constructions used in the pruning operation.

\begin{definition}
  Let \(\mathcal{D} : \Dyck_d\) be a Dyck word, and \(p :
  \Peak_{\mathcal{D}}\) be a peak of \(\mathcal{D}\). The pruned Dyck
  word \(\mathcal{D} \sslash p : \Dyck_d\) and substitution \(\pi_p :
    \lfloor \mathcal{D}\rfloor \to \lfloor \mathcal{D} \sslash p
  \rfloor\) are then defined inductively on the peak \(p\) by the
  following equations:
  \begin{align*}
    \mathcal{D} \Uparrow\, \Downarrow \sslash \mathcal{D} \UDPeak &=
    \mathcal{D}\\
    \mathcal{D} \Uparrow \sslash p \UpPeak &= (\mathcal{D} \sslash p)
    \Uparrow \\
    \mathcal{D} \Downarrow \sslash p \DownPeak &= (\mathcal{D}
    \sslash p) \Downarrow \\[10pt]
    \pi_{\mathcal{D}\UDPeak} &= \langle \id_{\lfloor \mathcal{D}
    \rfloor} , \tm_{\mathcal{D}}, \id(\ty_{\mathcal{D}},
    \tm_{\mathcal{D}}) \rangle\\
    \pi_{p \UpPeak} &= \langle \wk(\wk(\pi_p)) , y_{\mathcal{D}},
    f_{\mathcal{D}} \rangle\\
    \pi_{p \DownPeak} &= \pi_p\\
    \intertext{If we further have a substitution \(\sigma : \arr
      {\lfloor \mathcal{D} \rfloor} \star \Gamma\) for some context
      \(\Gamma\), then the pruned substitution \(\sigma \sslash p :
    \arr {\lfloor \mathcal{D} \sslash p \rfloor} \star \Gamma\) can be formed:}
    \langle \sigma, s, t \rangle \sslash \mathcal{D}\UDPeak &= \sigma \\
    \langle \sigma, s, t \rangle \sslash p \UpPeak &= \langle \sigma
    \sslash p, s, t \rangle \\
    \sigma \sslash p \DownPeak &= \sigma \sslash p
  \end{align*}
\end{definition}

Each peak in a Dyck word corresponds to a consecutive upwards arrow
and downwards arrow. Pruning this peak corresponds removing these two
arrows, which does not change the trailing dimension of the Dyck
word. The effect on the mountain diagram representation can be seen
in \cref{fig:prune}.

\begin{figure}[ht]
  \centering
% https://q.uiver.app/#q=WzAsMTcsWzAsMiwiXFxidWxsZXQiXSxbMSwxLCJcXGJ1bGxldCJdLFsyLDAsIlxcYnVsbGV0Il0sWzMsMSwiXFxidWxsZXQiXSxbNCwwLCJcXGJ1bGxldCJdLFs1LDEsIlxcYnVsbGV0Il0sWzYsMiwiXFxidWxsZXQiXSxbNywxLCJcXGJ1bGxldCJdLFs4LDIsIlxcYnVsbGV0Il0sWzksMSwiXFxyaWdodHNxdWlnYXJyb3ciXSxbMTAsMiwiXFxidWxsZXQiXSxbMTEsMSwiXFxidWxsZXQiXSxbMTIsMCwiXFxidWxsZXQiXSxbMTMsMSwiXFxidWxsZXQiXSxbMTQsMiwiXFxidWxsZXQiXSxbMTUsMSwiXFxidWxsZXQiXSxbMTYsMiwiXFxidWxsZXQiXSxbMCwxXSxbMSwyLCIiLDAseyJjb2xvdXIiOlswLDYwLDYwXX1dLFsyLDMsIiIsMCx7ImNvbG91ciI6WzAsNjAsNjBdfV0sWzMsNF0sWzQsNV0sWzUsNl0sWzYsN10sWzcsOF0sWzEwLDExXSxbMTEsMTJdLFsxMiwxM10sWzEzLDE0XSxbMTQsMTVdLFsxNSwxNl1d % tex-fmt: skip
  \[
    \begin{tikzcd}[column sep = small, cells={inner sep = 0}, arrows={no head}]
      && |[color={rgb,255:red,204;green,0;blue,14}]|\bullet &&
      \bullet &&&&&&&& \bullet \\
      & \bullet && \bullet && \bullet && \bullet && \rightsquigarrow
      && \bullet && \bullet && \bullet \\
      \bullet &&&&&& \bullet && \bullet && \bullet &&&& \bullet && \bullet
      \arrow[from=3-1, to=2-2]
      \arrow[color={Diag2}, from=2-2, to=1-3]
      \arrow[color={Diag2}, from=1-3, to=2-4]
      \arrow[from=2-4, to=1-5]
      \arrow[from=1-5, to=2-6]
      \arrow[from=2-6, to=3-7]
      \arrow[from=3-7, to=2-8]
      \arrow[from=2-8, to=3-9]
      \arrow[from=3-11, to=2-12]
      \arrow[from=2-12, to=1-13]
      \arrow[from=1-13, to=2-14]
      \arrow[from=2-14, to=3-15]
      \arrow[from=3-15, to=2-16]
      \arrow[from=2-16, to=3-17]
  \end{tikzcd}\]
  \caption[Pruning]{Pruning of peak \(\circleddash \Uparrow\,
  \UDPeak\, \UpPeak\, \DownPeak\, \DownPeak\, \UpPeak\, \DownPeak\).}
  \label{fig:prune}
\end{figure}

When a peak is pruned the locally maximal variable and its target are
removed from the associated context. The substitution
\(\pi_{\mathcal{D} \UDPeak}\) simply maps these two variables to
\(\id(\ty_{\mathcal{D}},\tm_{\mathcal{D}})\) and
\(\tm_{\mathcal{D}}\), where the Dyck term \(\tm_{\mathcal{D}}\) is
the source of the locally maximal variable. Pruning a substitution
simply removes the terms corresponding to the removed variables in
the associated context.

\begin{example}
  Let \(\Gamma = (x : \star), (f : \arr x \star x)\) and consider the
  term \(f * \id(x)\), which is given by:
  \[ \Coh {(a : \star), (b : \star), (c : a \to b), (d : \star), (e :
  b \to d)} {a \to d} {\langle x, x, f, x, \id(\star,x) \rangle} \]
  The context in this coherence is the associated context of the Dyck
  word \(\circleddash \Uparrow\,\Downarrow\,\Uparrow\,\Downarrow\)
  which has a peak \(\circleddash \Uparrow\,\Downarrow\,\UDPeak\),
  which corresponds to the locally maximal variable \(e\). Since
  \(e\) is sent to an identity by the substitution, pruning can be
  applied to get:
  \begin{align*}
    \circleddash \Uparrow\,\Downarrow\,\Uparrow\,\Downarrow \sslash
    \circleddash \Uparrow\,\Downarrow\,\UDPeak &= \circleddash
    \Uparrow\, \Downarrow\\
    \pi_{\circleddash \Uparrow\,\Downarrow\,\UDPeak} &= \langle a, b,
    c, b, \id(\star,b) \rangle\\
    \langle x, x, f, x, \id(\star,x) \rangle \sslash \circleddash
    \Uparrow\,\Downarrow\,\UDPeak &= \langle x,x,f\rangle
  \end{align*}
  Which results in the term:
  \[ \Coh {(a : \star), (b : \star), (c : a \to b)} {(a \to d) \sub
    {\langle a, b, c, b, \id(\star,b) \rangle} } {\langle x, x, f
    \rangle} \equiv \Coh {(a : \star), (b : \star), (c : a \to b)} {(a
  \to b)} {\langle x, x, f \rangle} \]
  which is the unary composite on \(f\). In the presence of disc
  removal, this term could further simplify to the variable \(f\).
\end{example}

With these constructions, we can define the pruning rule.

\begin{definition}
  A term \(t\) \emph{is an identity} if \(t \equiv \id(A,s)\) for
  some type \(A\) and some term \(s\). The \emph{pruning rule set},
  \prune, is the set consisting of the triples:
  \[ (\Gamma, \Coh {\lfloor \mathcal{D} \rfloor} {A} \sigma, \Coh
      {\lfloor \mathcal{D} \sslash p \rfloor} {A \sub {\pi_p}} {\sigma
  \sslash p}) \]
  for each Dyck word \(\mathcal{D} : \Dyck_0\), peak \(p :
  \Peak_{\mathcal{D}}\), type \(A : \Type_{\lfloor \mathcal{D}
  \rfloor}\), and substitution \(\sigma : \arr {\lfloor \mathcal{D}
  \rfloor} \star \Gamma\) where \(\lfloor p \rfloor \sub \sigma\) is
  an identity.

  A set of rules \(\mathcal{R}\) \emph{contains pruning} if \(\prune
  \subseteq \mathcal{R}\). Pruning makes the following rule admissible:
  \begin{mathpar}
    \inferrule{\mathcal{D} : \Dyck_0 \\ p : \Peak_{\mathcal{D}}
      \\ \lfloor \mathcal{D} \rfloor \vdash A \\ \Gamma \vdash \sigma :
      \lfloor \mathcal{D} \rfloor \\\\ (\lfloor \mathcal{D} \rfloor,
      \Supp(\src(A)), \tgt(A)) \in \mathcal{O}\\ \lfloor p \rfloor \sub
    \sigma \text{ is an identity}}{\Gamma \vdash \Coh {\lfloor
      \mathcal{D} \rfloor} A \sigma = \Coh {\lfloor \mathcal{D} \sslash
    p} {A \sub {\pi_p}} {\sigma \sslash p}}\textsc{prune}
  \end{mathpar}
  The set \(\mathcal{R}\) \emph{has pruning} if the rule
  \textsc{prune} holds in the generated theory.
\end{definition}

\subsection{Properties of pruning}

We start with the aim of proving that each construction involved in
pruning satisfies the expected typing judgements. To do this the
following lemma will be necessary, which describes the interaction of
the Dyck word construction with pruning.

\begin{lemma}
  \label{lem:dyck-prune-prop}
  Let \(\mathcal{D} : \Dyck_d\) be a Dyck word. Then the following
  equations hold:
  \begin{align*}
    \ty_{\mathcal{D}} \sub{\pi_p} &\equiv \ty_{\mathcal{D} \sslash p}\\
    \tm_{\mathcal{D}} \sub{\pi_p} &\equiv \tm_{\mathcal{D} \sslash p}
  \end{align*}
  for any peak \(p : \Peak_{\mathcal{D}}\) of \(\mathcal{D}\).
\end{lemma}
\begin{proof}
  The proof proceeds by an induction on the peak \(p\), proving both
  equations simultaneously. Both equations hold by routine
  calculations given in \module{Catt.Dyck.Pruning.Properties} by the
  functions \func{Catt.Dyck.Pruning.Properties}{dyck-type-prune} and
  \func{Catt.Dyck.Pruning.Properties}{dyck-term-prune}.
\end{proof}

This allows the main typing properties of this section to be given.

\begin{proposition}
  \label{prop:prune-ty}
  Let \(\mathcal{D} : \Dyck_d\) be a Dyck word and let \(p :
  \Peak_{\mathcal{D}}\) be a peak of this word. Then:
  \[ \lfloor \mathcal{D} \sslash p \rfloor \vdash \pi_p : \lfloor
  \mathcal{D} \rfloor \]
  Given a substitution \(\sigma\) with \(\Gamma \vdash \sigma :
  \lfloor \mathcal{D} \rfloor\), where \(\lfloor p \rfloor \sub
  \sigma\) is an identity, the equality and typing judgements:
  \[ \Gamma \vdash \sigma = \pi_p \bullet (\sigma \sslash p) \qquad
  \Gamma \vdash \sigma : \lfloor \mathcal{D} \sslash p \rfloor \]
  hold.
\end{proposition}
\begin{proof}
  We prove each judgement holds in turn by induction on the peak
  \(p\). For the judgement:
  \[ \lfloor \mathcal{D} \sslash p \rfloor \vdash \pi_p : \lfloor
  \mathcal{D} \rfloor \]
  the case when the peak is of the form \(p\DownPeak\) is trivial.
  The case for when it is of the form \(\mathcal{D}\UDPeak\) easily
  follows from \cref{lem:dyck-typing,cor:id-typing}. For the case
  where the peak is of the form \(p\UpPeak\), it must be shown that:
  \[ \Delta \vdash \langle \wk(\wk(\pi_p)), y, f \rangle : \lfloor
    \mathcal{D} \rfloor, (y : \ty_{\mathcal{D}}), (f : \arr
  {\wk(\tm_{\mathcal{D}})} {\wk(\ty_{\mathcal{D}})} y) \]
  where \(\Delta = \lfloor \mathcal{D} \sslash p \rfloor, (y :
    \ty_{\mathcal{D} \sslash p}), (f : \arr{\wk(\tm_{\mathcal{D}\sslash
  p})} {\wk(\ty_{\mathcal{D}\sslash p})} {y})\). This requires proofs of:
  \begin{align*}
    \Delta &\vdash \wk(\wk(\pi_p)) : \lfloor \mathcal{D} \rfloor\\
    \Delta &\vdash y : \ty_{\mathcal{D}} \sub {\pi_p}\\
    \Delta &\vdash f : (\arr {\wk(\tm_{\mathcal{D}})}
    {\wk(\ty_{\mathcal{D}})} y) \sub {\langle \wk(\pi_p), y \rangle}
  \end{align*}
  The first part follows from inductive hypothesis (and typing of
  weakening). The other two judgements follow from some calculation
  and \cref{lem:dyck-prune-prop}.

  For the second judgement:
  \[ \Gamma \vdash \sigma = \pi_p \bullet (\sigma \sslash p)\]
  The \(p \DownPeak\) case is again trivial. The \(p \UpPeak\) case
  follows easily from properties of weakening and the inductive
  hypothesis. For the \(\mathcal{D} \UDPeak\) case we suppose the
  substitution is of the form \(\langle \sigma, s, \id(A,t) \rangle\)
  and are required to show that:
  \[ \Gamma \vdash \langle \id_{\mathcal{D}}, \tm_{\mathcal{D}},
    \id(\ty_{\mathcal{D}}, \tm_{\mathcal{D}})\rangle \bullet \sigma =
  \langle \sigma, s, \id(A,t) \rangle \]
  It is immediate that \(\id_{\mathcal{D}} \bullet \sigma \equiv
  \sigma\) and so it remains to show that \(\Gamma \vdash
  \tm_{\mathcal{D}} \sub \sigma = s\) and \(\Gamma \vdash
  \id(\ty_{\mathcal{D}},\tm_{\mathcal{D}}) \sub \sigma = \id(A,t)\).
  By deconstructing the typing derivation of \(\langle \sigma, s,
  \id(A,t) \rangle\), we have:
  \[ \Gamma \vdash \id(A,t) : (\arr{\wk(\tm_{\mathcal{D}})}
  {\wk(\ty_{\mathcal{D}})} {y}) \sub {\langle \sigma ,s \rangle} \]
  By \cref{cor:id-typing} and the uniqueness of typing, we must have:
  \[ \Gamma \vdash \arr t A t = (\arr{\wk(\tm_{\mathcal{D}})}
    {\wk(\ty_{\mathcal{D}})} {y}) \sub {\langle \sigma ,s \rangle}
    \equiv \arr {\tm_{\mathcal{D}} \sub \sigma} {\ty_{\mathcal{D}} \sub
  \sigma} {s} \]
  and so \(A = \ty_{\mathcal{D}} \sub \sigma\) and \(s = t =
  \tm_{\mathcal{D}} \sub \sigma\). The equality
  \(\id(\ty_{\mathcal{D}}, \tm_{\mathcal{D}}) = \id(A,t)\) follows as
  equality is respected by the identity construction, which can be
  proved by a simple induction.

  Lastly, we consider the judgement:
  \[ \Gamma \vdash \sigma \sslash p : \lfloor \mathcal{D} \sslash p \rfloor \]
  The only difficult case is for the peak \(p \UpPeak\), where we can
  assume that the substitution is of the form \(\langle \sigma, s,
  t\rangle\), such that:
  \[ \langle \sigma, s, t\rangle \sslash p \UpPeak \equiv \langle
  \sigma \sslash p, s, t\rangle\]
  Typing for \(\sigma \sslash p\) follows from inductive hypothesis,
  and the typing for \(s\) and \(t\) follow from applying conversion
  rules to the corresponding parts of the typing derivation for
  \(\langle \sigma, s, t \rangle\). After some computation, the
  following equalities are needed for these conversion rules:
  \begin{align*}
    \Gamma &\vdash \tm_{\mathcal{D}} \sub \sigma = \tm_{\mathcal{D}
    \sslash p} \sub {\sigma \sslash p}\\
    \Gamma &\vdash \ty_{\mathcal{D}} \sub \sigma = \ty_{\mathcal{D}
    \sslash p} \sub {\sigma \sslash p}
  \end{align*}
  The first is given by:
  \begin{align*}
    \tm_{\mathcal{D}} \sub \sigma &= \tm_{\mathcal{D}} \sub {\pi_p
    \bullet (\sigma \sslash p)}\\
    &\equiv \tm_{\mathcal{D}} \sub {\pi_p} \sub {\sigma \sslash p}\\
    &\equiv \tm_{\mathcal{D} \sslash p} \sub {\sigma \sslash p}
  \end{align*}
  and the second follows similarly, completing the proof.
\end{proof}

We next show that pruning has the expected properties on the Dyck
words \(\mathcal{D}^n\), which correspond to disc contexts.

\begin{proposition}
  \label{prop:prune-disc}
  Let \(n > 0\), and \(p\) be the unique peak of \(\mathcal{D}^n\). Then:
  \[ \mathcal{D}^n \sslash p \equiv \mathcal{D} \qquad \{\arr s A
  t,u\} \sslash p \equiv \{A,s\}\]
  for all \(A,s,t,u\) where \(\dim(A) = n - 1\).
\end{proposition}
\begin{proof}
  Both properties are immediate.
\end{proof}

We now turn our attention to proving that the pruning equality set
satisfies all the conditions from \cref{sec:ruleset}. We begin with
the tameness conditions, omitting the weakening condition, as it
follows from the substitution condition.

\begin{proposition}
  \label{prop:prune-tame}
  For all \(\mathcal{D} : \Dyck_d\) and peaks \(p :
  \Peak_{\mathcal{D}}\), and substitutions \(\sigma : \lfloor
  \mathcal{D} \rfloor \to \Delta\) and \(\tau : \Delta \to \Gamma\)
  the following equality holds:
  \[ (\sigma \sslash p) \bullet \tau \equiv (\sigma \bullet \tau) \sslash p \]
  Hence, the set \prune satisfies the \(\mathcal{R}\)-substitution
  condition for any equality set \(\mathcal{R}\), and so also
  satisfies the weakening condition.

  Furthermore, the following equalities hold:
  \[\Sigma(\mathcal{D}) \sslash \Sigma(p) = \Sigma(\mathcal{D}
    \sslash p) \qquad \pi_{\Sigma(p)} \equiv \Sigma(\pi_p) \qquad
  \Sigma(\sigma \sslash p) \equiv \Sigma(\sigma) \sslash \Sigma(p)\]
  Therefore, the set \prune also satisfies the suspension condition,
  making the equality set \prune tame.
\end{proposition}
\begin{proof}
  The proofs of each syntactic equality are easily proved by
  induction on the peak \(p\). Their proofs are given in the
  formalisation in \module{Catt.Dyck.Pruning.Properties} as
  \func{Catt.Dyck.Pruning.Properties}{//s-sub},
  \func{Catt.Dyck.Pruning.Properties}{prune-susp-peak},
  \funcn{Catt.Dyck.Pruning.Properties}{susp-π}{susp-\(\pi\)}, and
  \func{Catt.Dyck.Pruning.Properties}{susp-//s}.
\end{proof}

To show that the support property holds, we must prove that
\(\Supp(\sigma) = \Supp(\sigma \sslash p)\). We aim to do this by
observing that \(\Supp(\sigma) = \Supp(\pi_p \bullet (\sigma \sslash
p))\) and that \(\Supp(\pi_p \bullet (\sigma \sslash p)) =
\Supp(\sigma \sslash p)\). By employing the proof strategy for the
support condition introduced in \cref{sec:further-conditions}, the
first will follow from the equality \(\sigma = \pi_p \bullet (\sigma
\sslash p)\), which we can assume holds in a theory which satisfies
the support condition. For the second we need the following lemma.

\begin{lemma}
  \label{lem:pi-bdry}
  For all \(n : \mathbb{N}\), \(\epsilon \in \{-,+\}\), \(\mathcal{D}
  : \Dyck_d\), and \(p : \Peak_{\mathcal{D}}\):
  \[ \bdry n \epsilon {\lfloor \mathcal{D} \rfloor} \sub {\pi_p} =
  \bdry n \epsilon {\lfloor \mathcal{D} \sslash p \rfloor} \]
  and so \(\Supp(\pi_p) = \Var(\lfloor \mathcal{D} \sslash p \rfloor)\).
\end{lemma}
\begin{proof}
  The main equation in this lemma is given by a long and technical
  induction on the peak \(p\). The details of this induction appear
  in the formalisation in the function
  \funcn{Catt.Dyck.Pruning.Support}{π-boundary-vs}{\(\pi\)-boundary-vs}
  which appears in the module \module{Catt.Dyck.Pruning.Support}.

  The equation \(\Supp(\pi_p) = \Var(\lfloor \mathcal{D} \sslash p
  \rfloor)\) follows from \cref{prop:vs-sub,lem:bdry-full}, by
  setting \(n = \dim(\lfloor \mathcal{D} \rfloor)\).
\end{proof}

We are now ready to prove that the support condition holds.

\begin{proposition}
  \label{prop:prune-supp}
  Let \(\mathcal{R}\) be a tame equality rule set that satisfies the
  support condition. Then the set \prune satisfies the
  \(\mathcal{R}\)-support condition.
\end{proposition}
\begin{proof}
  It suffices to prove that:
  \[ \Supp(\Coh {\lfloor \mathcal{D} \rfloor} {A} \sigma) =
    \Supp(\Coh {\lfloor \mathcal{D} \sslash p \rfloor} {A \sub {\pi_p}}
  {\sigma \sslash p}) \]
  for \(\mathcal{D} : \Dyck_0\), \(p : \Peak_{\mathcal{D}}\), type
  \(A\), and substitution \(\sigma : \lfloor \mathcal{D} \rfloor \to
  \Gamma\), where \(\lfloor p \rfloor \sub \sigma\) is an identity
  and \(\Gamma \vdash_{\mathcal{R}} \Coh {\lfloor \mathcal{D}
  \rfloor} {A} \sigma : B\) for some \(B\). By inspection of the
  typing derivation we obtain an instance of the judgement \(\Gamma
  \vdash_{\mathcal{R}} \sigma : \lfloor \mathcal{D} \rfloor\), and so:
  \begin{align*}
    \Supp(\Coh {\lfloor \mathcal{D} \rfloor} {A} \sigma) &= \Supp(\sigma)\\
    &= \Supp(\pi_p \bullet (\sigma \sslash p))&(*)\\
    &= \Supp(\pi_p) \sub {\sigma \sslash p}\\
    &= \Var{\lfloor \mathcal{D} \sslash p \rfloor} \sub {\sigma
    \sslash p}&\text{by \cref{lem:pi-bdry}}\\
    &= \Supp(\sigma \sslash p) \\
    &= \Supp(\Coh {\lfloor \mathcal{D} \sslash p \rfloor} {A \sub
    {\pi_p}} {\sigma \sslash p})
  \end{align*}
  where equality \((*)\) is derived by applying \cref{prop:supp-prop}
  to the equality
  \[\Gamma \vdash_{\mathcal{R}} \sigma = \pi_p \bullet (\sigma \sslash p)\]
  from \cref{prop:prune-ty}.
\end{proof}

To prove that the preservation condition holds, it is necessary to
show that the type \(A \sub{\pi_p}\) created by pruning is a valid
operation. This cannot be deduced from any of the conditions that
have been imposed on the operation set \(\mathcal{O}\) so far.
Therefore, we introduce the following additional condition.

\begin{definition}
  An operation set \(\mathcal{O}\) \emph{supports pruning} if for all
  \(\mathcal{D} : \Dyck_0\), \(p : \Peak_{\mathcal{D}}\), and
  variable sets \(U,V \subseteq \Var(\lfloor \mathcal{D} \rfloor)\) we have:
  \[ (\lfloor \mathcal{D} \sslash p \rfloor, U \sub{\pi_p}, V
  \sub{\pi_p}) \in \mathcal{O} \]
  whenever \((\lfloor \mathcal{D} \rfloor, U , V) \in \mathcal{O}\).
\end{definition}

The globular operation set trivially supports pruning. From
\cref{lem:pi-bdry,prop:std-op}, it can be proved that the regular
operation set supports pruning. We can now prove that the
preservation condition holds.

\begin{proposition}
  \label{prop:prune-preserve}
  Let \(\mathcal{R}\) be a tame equality rule set and suppose the
  operation set \(\mathcal{O}\) supports pruning. Then the set \prune
  satisfies the \(\mathcal{R}\)-preservation condition.
\end{proposition}
\begin{proof}
  Let \(\mathcal{D} : \Dyck_d\) be a Dyck word and \(p :
  \Peak_{\mathcal{D}}\) be a peak of \(\mathcal{D}\). Further suppose
  \(\arr s A t : \Type_{\lfloor \mathcal{D} \rfloor}\), and \(\sigma
  : \lfloor \mathcal{D} \rfloor \to \Gamma\) such that \(\lfloor p
  \rfloor \sub \sigma\) is an identity and:
  \[ \Gamma \vdash_{\mathcal{R}} \Coh {\lfloor \mathcal{D} \rfloor}
  {\arr s A t} \sigma : B\]
  for some type \(B : \Type_\Gamma\). By inspection on this typing
  derivation we have:
  \[ \lfloor \mathcal{D} \rfloor \vdash_{\mathcal{R}} A \qquad \Gamma
    \vdash_{\mathcal{R}} \sigma \lfloor \mathcal{D} \rfloor \qquad
    (\lfloor \mathcal{D} \rfloor, \Supp(s), \Supp(t)) \in \mathcal{O}
  \qquad \Gamma \vdash_{\mathcal{R}} B = (\arr s A t) \sub \sigma\]
  and so by \cref{prop:prune-ty}, we have:
  \[\lfloor \mathcal{D} \sslash p \rfloor \vdash_{\mathcal{R}} \pi_p
    : \lfloor \mathcal{D} \rfloor \qquad \Gamma \vdash_{\mathcal{R}}
  \sigma \sslash p : \lfloor \mathcal{D} \sslash p \rfloor\]
  therefore, as \(\mathcal{O}\) supports pruning, the following judgement holds:
  \[\Gamma \vdash_{\mathcal{R}} \Coh {\lfloor \mathcal{D} \sslash p
    \rfloor} {(\arr s A t) \sub {\pi_p}} {\sigma \sslash p} : (\arr s A
  t) \sub {\pi_p} \sub {\sigma \sslash p}\]
  and so by applying the conversion rule, it suffices to show that
  \[ \Gamma \vdash_{\mathcal{R}} B = (\arr s A t) \sub {\pi_p} \sub
  {\sigma \sslash p}\]
  but this follows from the equality \(B = (\arr s A t) \sub \sigma\)
  and the equality \(\sigma = \pi_p \bullet (\sigma \sslash p)\) from
  \cref{prop:prune-ty}.
\end{proof}

We end this section with a property of pruning that will be required
to prove confluence. Suppose we have a Dyck word \(\mathcal{D}\) and
two distinct peaks \(p, q : \Peak_{\mathcal{D}}\). Then both peaks
can be pruned from \(\mathcal{D}\) in either order. Consider the
example below on the Dyck word from \cref{ex:dyck-peaks}.
\[
  \begin{tikzcd}[column sep = 0.7em, row sep = scriptsize,
    cells={inner sep = 0,shape=circle,anchor=center}, arrows={no head}]
    &&&&&&&&&&& \bullet \\
    &&&&&&&&&& \bullet && \bullet && |[color=Diag1]|\bullet \\
    &&&&&&&&& \bullet &&&& \bullet && \bullet \\
    && |[color=Diag2]|\bullet && \bullet &&&& |[color=Diag2,
    rotate=35]|\mathclap{\rightsquigarrow} &&&&&&&& |[color=Diag1,
    rotate=-35]|\mathclap{\rightsquigarrow} && \bullet \\
    & \bullet && \bullet && \bullet && |[color=Diag1]|\bullet
    &&&&&&&&&& \bullet && \bullet \\
    \bullet &&&&&& \bullet && \bullet &&&&&&&& \bullet &&&& \bullet \\
    &&&&&&&& |[color=Diag1, rotate=-35]|\mathclap{\rightsquigarrow}
    &&& |[color=Diag2]|\bullet && \bullet &&& |[color=Diag2,
    rotate=35]|\mathclap{\rightsquigarrow} \\
    &&&&&&&&&& \bullet && \bullet && \bullet \\
    &&&&&&&&& \bullet &&&&&& \bullet
    \arrow[from=6-1, to=5-2]
    \arrow[color=Diag2, from=5-2, to=4-3]
    \arrow[color=Diag2, from=4-3, to=5-4]
    \arrow[from=5-4, to=4-5]
    \arrow[from=4-5, to=5-6]
    \arrow[from=5-6, to=6-7]
    \arrow[color=Diag1, from=6-7, to=5-8]
    \arrow[color=Diag1, from=5-8, to=6-9]
    \arrow[from=3-10, to=2-11]
    \arrow[from=2-11, to=1-12]
    \arrow[from=1-12, to=2-13]
    \arrow[from=2-13, to=3-14]
    \arrow[color=Diag1, from=3-14, to=2-15]
    \arrow[color=Diag1, from=2-15, to=3-16]
    \arrow[from=6-17, to=5-18]
    \arrow[from=5-18, to=4-19]
    \arrow[from=4-19, to=5-20]
    \arrow[from=5-20, to=6-21]
    \arrow[from=9-10, to=8-11]
    \arrow[color=Diag2, from=8-11, to=7-12]
    \arrow[color=Diag2, from=7-12, to=8-13]
    \arrow[from=8-13, to=7-14]
    \arrow[from=7-14, to=8-15]
    \arrow[from=8-15, to=9-16]
  \end{tikzcd}
\]
The following proposition proves that both peaks of the Dyck word can
be pruned, and that the order in which this is done does not matter.

\begin{proposition}
  \label{prop:prune-conf}
  Suppose \(\mathcal{D} : \Dyck_d\) is a Dyck word and let \(p\) and
  \(q\) be two distinct peaks of \(\mathcal{D}\). Then there is a
  peak \(q_{p}\) of \(\mathcal{D} \sslash p\) such that:
  \[ \lfloor q_{p} \rfloor \equiv \lfloor q \rfloor \sub {\pi_{p}}\]
  and a similar peak \(p_{q}\) of \(\mathcal{D} \sslash q\).
  Furthermore, the following equations hold syntactically:
  \begin{mathpar}
    (\mathcal{D} \sslash p) \sslash q_{p} = (\mathcal{D} \sslash q)
    \sslash p_{q} \and
    \pi_p \bullet \pi_{q_p} \equiv \pi_q \bullet \pi_{p_q} \and
    (\sigma \sslash p) \sslash q_{p} = (\sigma \sslash q) \sslash p_{q}
  \end{mathpar}
  where the last equation holds for any \(\sigma : \lfloor
  \mathcal{D} \rfloor \to \Gamma\).
\end{proposition}
\begin{proof}
  All proofs proceed by a simultaneous induction on both the peaks
  \(p\) and \(q\), and are given in
  \module{Catt.Dyck.Pruning.Properties} in the formalisation. The
  construction of the peak \(q_p\) is given by the function
  \func{Catt.Dyck.Pruning.Properties}{prune-peak}, the equality
  \(\lfloor q_p \rfloor \equiv \lfloor q \rfloor \sub {\pi_p}\) is
  given by \func{Catt.Dyck.Pruning.Properties}{prune-peak-prop}, and
  the remaining three equations are given by
  \func{Catt.Dyck.Pruning.Properties}{prune-conf},
  \funcn{Catt.Dyck.Pruning.Properties}{π-conf}{\(\pi\)-conf}, and
  \func{Catt.Dyck.Pruning.Properties}{prune-sub-conf}.
\end{proof}

\section{Trees}
\label{sec:trees}

During the next sections we build up to defining the insertion
operation. This operation performs larger modifications to pasting
diagrams than the pruning operation, and we will again want to
represent pasting diagrams differently to make the definition in
\cref{sec:insertion} as natural as possible. It is well known that
pasting diagrams correspond to planar rooted trees
\cite{Weber2004,leinster2004higher,batanin1998monoidal}, which we
will simply refer to as \emph{trees} and can be defined as follows.

\begin{definition}
  A \emph{tree} \(T : \Tree\) is inductively defined to be a
  (possibly empty) list of trees.
\end{definition}

Throughout this section we will make use of standard operations and
notations for lists. A list that contains the elements \(x_i\) for
\(i\) from \(0\) to \(n\) will be written in square bracket notation
as \([x_0,x_1,x_2,\dots,x_n]\). Further, we use the notation \(\emp\)
for the empty list and \(\doubleplus\) for the concatenation of
lists, which is associative and has the empty list as its unit. We
will use the Agda-like notation of writing \(n :: ns\) for a list for
which the first element (the head) is \(n\) and the rest of the list
(the tail) is \(ns\). The length of a list will be given by the
operation \(\len\).

We will use the notation \(\Sigma(T) = [T]\), and call \(\Sigma(T)\)
the suspension of \(T\), for reasons that will become immediate once
the context generated by a tree has been defined in \cref{sec:tree-contexts}.

We note that it will be common to see expressions of the form \(S ::
T\) where \(S\) and \(T\) are both trees. It may seem as if this was
an error, and that a concatenation operation should have been given
instead, but in this case we are exploiting the identification of
trees and lists of trees to treat \(S\) as a tree (as an element of
the list) and \(T\) as a list of trees.

We now define some common operations on trees.

\begin{definition}
  \label{def:treetrunk}
  The \emph{depth} of a tree \(\dep(T)\) is \(0\) if \(T\) is empty
  or \(1 + \max_k{\dep(T_k)}\) if \(T = [T_0,\dots,T_n]\). For a tree
  \(T\), its \emph{trunk height}, \(\th(T)\), is \(1 + \th(T_0)\) if
  \(T = [T_0]\) and \(0\) otherwise. A tree is \emph{linear} if its
  trunk height equals its depth.

  Subtrees of a tree can be indexed by a list of natural numbers
  \(P\), giving a subtree \(T^P\) by letting \(T^{\emp} = T\) and
  \(T^{k::P} = {(T_k)}^P\) if \(T = [T_0, \dots, T_n]\).
\end{definition}

As these trees represent pasting diagrams, a context can be
associated to each one. To be able to make effective use of trees we
will need to understand this mapping to contexts, and the associated
constructions used in this mapping. One of these constructions is
suspension, which we have already seen. The second is an operation
known as the wedge sum, which will be introduced in
\cref{sec:wedge-sums}. Both these operations are mappings from
contexts to contexts which preserve ps-context derivations. We will
see in \cref{sec:tree-contexts} that a further result holds, that
these two operations (along with the singleton context) are
sufficient to generate all ps-contexts.

\begin{remark}
  In the formalisation, trees are defined in \module{Catt.Tree} and
  take a slightly different form to the trees defined above, and are
  actually defined to be a binary tree. This exploits an isomorphism
  between binary trees and trees with arbitrary (finite) branching.
  The constructors for the trees in the formalisation are called
  \(\mathsf{Sing}\), which stands for ``singleton'' and takes no
  arguments, and \(\mathsf{Join}\), which takes two trees as
  arguments. The isomorphism is generated from the following rules:
  \begin{mathpar}
    \inferrule{ }{\mathsf{Sing} \simeq \emp}\and
    \inferrule{S \simeq S' \\ T \simeq T'}{\mathsf{Join}(S,T) \simeq S' :: T'}
  \end{mathpar}
  Presenting trees in this way in the formalisation allows any
  induction to be done as a single induction over the constructors of
  a tree, instead of simultaneously inducting on the depth of the
  tree and on lists. We retain the standard notation of trees for
  this text for simplicity of notation. Under the above isomorphism,
  this has no effect on the formal development.
\end{remark}

\subsection{Wedge sums}
\label{sec:wedge-sums}

The wedge sum, just like suspension, is an operation inspired by a
similar operation on topological spaces. Given two spaces \(X\) and
\(Y\) and points \(x\) of \(X\) and \(y\) of \(Y\), the space \(X
\vee Y\) can be formed, by taking the disjoint union of \(X\) and
\(Y\), and identifying the points \(x\) and \(y\).

This construction satisfies a universal property: it is the colimit
of the following diagram:
\begin{equation}
  \label[diagram]{diag:wedge-colimit}
  \begin{tikzcd}
    X && Y \\
    & {\{*\}}
    \arrow["x", from=2-2, to=1-1]
    \arrow["y"', from=2-2, to=1-3]
  \end{tikzcd}
\end{equation}
where the arrows labelled \(x\) and \(y\) send the unique point \(*\)
to \(x\) and \(y\) respectively. Such a universal construction gives
rise to two inclusions:
\[\inc_X : X \to X \vee Y \qquad \inc_Y : Y \to X \vee Y\]
A similar colimit can be formed in the syntactic category of \Cattr.
Leveraging that the variables of a context are ordered, every
(non-empty) context in \Catt is naturally bipointed. For a context
\(\Gamma\), the first point is given by the first variable of the
context (which must have type \(\star\)), which we name
\(\fst(\Gamma)\), and the second point is given by the last
\(0\)-dimensional variable in the context, which we name
\(\snd(\Gamma)\). We therefore restrict the construction above to
when the chosen point for the left context \(\Gamma\) is
\(\fst(\Gamma)\) and the chosen point for the second context is
\(\snd(\Delta)\). This simplifies the construction, and will be the
only case we need for forming trees. We note that
\(\fst(\Sigma(\Gamma)) \equiv N\) and \(\snd(\Sigma(\Gamma)) \equiv
S\), as we will commonly take the wedge sums of suspended contexts.

\begin{definition}
  Let \(\Gamma\) and \(\Delta\) be non-empty contexts. We then
  mutually define the \emph{wedge sum} \(\Gamma \vee \Delta\) and
  inclusions \(\inc_\Gamma : \arr \Gamma \star {\Gamma \vee_t
  \Delta}\) and \(\inc_\Delta : \arr \Delta \star {\Gamma \vee
  \Delta}\) on the context \(\Delta\), noting that the base case is
  \(\Delta = (x : A)\) as \(\Delta\) is non-empty.
  \begin{align*}
    \Gamma \vee (x : A) &= \Gamma \\
    \Gamma \vee \Delta, (x : A) &= \Gamma \vee \Delta, (x : A \sub
    {\inc_\Delta})\\[10pt]
    \inc_\Gamma &= \wk^{n - 1}(\id_\Gamma) &\text{when \(\Delta\) has
    length \(n\)}\\[10pt]
    \inc_{(x : A)} &= \langle \snd(\Gamma) \rangle\\
    \inc_{\Delta, (x : A)} &= \langle \wk(\inc_\Delta), x \rangle
  \end{align*}
  If we further have substitutions \(\sigma : \arr \Gamma A \Theta\)
  and \(\tau : \arr \Delta A \Theta\), then we can define the
  substitution \( \sigma \vee \tau : \arr {\Gamma \vee \Delta} A
  \Theta\) again by induction on \(\Delta\):
  \begin{align*}
    \sigma \vee \langle A, s \rangle &= \sigma\\
    \sigma \vee \langle \tau, s \rangle &= \langle \sigma, s \rangle
  \end{align*}
  We note that no extra property is needed to define this universal
  map, though to show it is well-formed we will need that
  \(\snd(\Gamma) \sub \sigma = \fst(\Delta) \sub \tau\).
\end{definition}

We firstly prove some basic properties required for \(\Gamma \vee
\Delta\) to be the colimit of \cref{diag:wedge-colimit}.

\begin{lemma}
  \label{lem:wedge-sum-prop}
  Let \(\Gamma\) and \(\Delta\) be non-empty contexts. Then:
  \[ \inc_{\Gamma} \vee \inc_{\Delta} \equiv \id_{\Gamma \vee \Delta} \]
  Further, the following equations hold:
  \[ \inc_{\Gamma} \bullet (\sigma \vee \tau) \equiv \sigma \qquad
  \inc_{\Delta} \bullet (\sigma \vee \tau) \equiv \tau \]
  for substitutions \(\sigma : \arr \Gamma A \Theta\) and \(\tau :
  \arr \Delta A \Theta\) where the second equality requires that
  \(\snd(\Gamma) \sub \sigma \equiv \fst(\Delta) \sub \tau\). Lastly:
  \[ (\sigma \vee \tau) \bullet \mu \equiv (\sigma \bullet \mu) \vee
  (\tau \bullet \mu) \]
  where \(\mu : \arr \Theta B {\Theta'}\) is another substitution.
\end{lemma}
\begin{proof}
  Proofs appear as \func{Catt.Wedge.Properties}{sub-from-wedge-prop},
  \func{Catt.Wedge.Properties}{sub-from-wedge-inc-left},
  \func{Catt.Wedge.Properties}{sub-from-wedge-inc-right}, and
  \func{Catt.Wedge.Properties}{sub-from-wedge-sub} in
  \module{Catt.Wedge.Properties}.
\end{proof}

To simplify definitions of substitutions between wedge sums of
contexts, we will write substitutions diagrammatically by specifying
the individual components. Consider the following diagram:
% https://q.uiver.app/?q=WzAsNixbMCwyLCJcXFNpZ21hXFxHYW1tYSJdLFsxLDIsIlxcdmVlIl0sWzIsMiwiXFxTaWdtYSBcXERlbHRhIl0sWzAsMCwiXFxTaWdtYSBcXEdhbW1hJyJdLFsyLDAsIlxcU2lnbWFcXERlbHRhJyJdLFsxLDAsIlxcdmVlIl0sWzAsMywiXFxTaWdtYSBcXHNpZ21hIl0sWzIsNCwiXFxTaWdtYSBcXHRhdSJdXQ== % tex-fmt: skip
\[
  \begin{tikzcd}[column sep=tiny, row sep=10pt]
    {\Gamma'} & \vee & {\Delta'} &\vee &{\Theta'} \\
    \\
    \Gamma & \vee & \Delta &
    \arrow["{\sigma}", from=3-1, to=1-1, pos=.4]
    \arrow["{\tau}", from=3-3, to=1-3, pos=.4]
  \end{tikzcd}
\]
which is generated from substitutions \(\sigma : \Gamma \to \Gamma'\)
and \(\tau : \Delta \to \Delta'\). A substitution \(\Gamma \vee
\Delta \to \Gamma' \vee \Delta' \vee \Theta'\) can be generated by
composing each arrow in the diagram with suitable inclusions so that
its target is \(\Gamma' \vee \Delta' \vee \Theta'\), and then using
the universal property of the wedge to map out of the source context.
In the diagram above the generated substitution is:
\[ ((\sigma \bullet \inc_{\Gamma'}\bullet \inc_{\Gamma' \vee
    \Delta'}) \vee (\tau \bullet \inc_{\Delta'}\bullet \inc_{\Gamma' \vee
\Delta'}))  \]
To ensure these definitions are unique, the following proposition is needed:
\begin{proposition}
  The wedge sum \(\vee\) is associative and has the singleton context
  \((x : \star)\) as its left and right unit. Given a context
  \(\Gamma\), the inclusions satisfy the following unitality properties:
  \[ \inc_{\Gamma} : \Gamma \to \Gamma \vee (x : \star) \equiv
    \id_\Gamma \qquad \inc_{\Gamma} : \Gamma \to (x : \star) \vee
  \Gamma \equiv \id_\Gamma \]
  and given substitutions \(\sigma : \arr \Gamma A \Xi\), \(\tau :
  \arr \Delta A \Xi\), and \(\mu : \arr \Theta A \Xi\) we have:
  \[ (\sigma \vee \tau) \vee \mu \equiv \sigma \vee (\tau \vee \mu)\]
  There is a unique way of including each of the contexts \(\Gamma\),
  \(\Delta\), and \(\Theta\) into \(\Gamma \vee \Delta \vee \Theta\),
  that is there is a unique substitution \(\Gamma \to \Gamma \vee
  \Delta \vee \Theta\) which is built from a composite of inclusions
  and similarly for \(\Delta\) and \(\Theta\).
\end{proposition}
\begin{proof}
  The proofs of these are given in \module{Catt.Wedge.Properties},
  and are all given by inducting on the right most context. The proof
  for the right unitality of \(\vee\) is omitted from the
  formalisation as it is immediate from the definitions.

  The uniqueness of inclusions substitutions is given by
  \begin{itemize}
    \item \func{Catt.Wedge.Properties}{wedge-inc-left-assoc}, which says:
      \begin{align*}
        \inc_{\Gamma} \bullet \inc_{\Gamma \vee \Delta} : \Gamma \to
        (\Gamma \vee \Delta) \vee \Theta &\equiv \inc_{\Gamma} :
        \Gamma \to \Gamma \vee (\Delta \vee \Theta)\\
        \intertext{
      \item \func{Catt.Wedge.Properties}{wedge-incs-assoc}, which says:}
        \inc_{\Delta} \bullet \inc_{\Gamma \vee \Delta} : \Delta \to
        (\Gamma \vee \Delta) \vee \Theta &\equiv \inc_{\Delta}
        \bullet \inc_{\Delta \vee \Theta} : \Delta \to \Gamma \vee
        (\Delta \vee \Theta)\\
        \intertext{
      \item \func{Catt.Wedge.Properties}{wedge-inc-right-assoc}, which says:}
        \inc_{\Theta} : \Theta \to (\Gamma \vee \Delta) \vee \Theta
        &\equiv \inc_{\Theta} \bullet \inc_{\Delta \vee \Theta} :
        \Theta \to \Gamma \vee (\Delta \vee \Theta)
      \end{align*}
  \end{itemize}
  We note that the definition of the wedge sum differs slightly in
  the formalisation, specifying a term \(t\) in \(\Gamma\) which
  takes the role of \(\snd(\Gamma)\), in order to give more
  computational control. By replacing the terms \(t\) in the
  formalisation by \(\snd(\Gamma)\) for the appropriate context
  \(\Gamma\), and noting that \(\snd(\Delta) \sub{\inc_{\Delta}}
  \equiv \snd(\Gamma \vee \Delta)\) (which can be proved by an easy
  induction), the results written here can be recovered.
\end{proof}

The previous proposition ensures that the diagrammatic notation for
substitutions between wedge sums uniquely defines a substitution. We
next show that all the constructions in this section have the
expected typing properties.

\begin{lemma}
  \label{lem:wedge-typing}
  The following inference rules are admissible in \Cattr:
  \begin{mathpar}
    \inferrule{\Gamma \vdash \\ \Delta \vdash}{\Gamma \vee \Delta \vdash}\and
    \inferrule{ }{\Gamma \vee \Delta \vdash \inc_{\Gamma} : \Gamma}\and
    \inferrule{ }{\Gamma \vee \Delta \vdash \inc_{\Delta} : \Delta}\and
    \inferrule{\Theta \vdash \snd(\Gamma) \sub \sigma = \fst(\Delta)
    \sub \tau}{\Theta \vdash \inc_{\Delta} \bullet (\sigma \vee \tau)
    = \tau}\and
    \inferrule{\Theta \vdash \sigma : \Gamma \\ \Theta \vdash \tau :
      \Gamma \\ \Theta \vdash \snd(\Gamma) \sub \sigma = \fst(\Delta)
    \sub \tau}{\Theta \vdash \sigma \vee \tau : \Gamma \vee \Delta}\and
    \inferrule{\Theta \vdash \sigma = \sigma'\\ \Theta \vdash \tau =
    \tau'}{\Theta \vdash \sigma \vee \tau = \sigma' \vee \tau'}
  \end{mathpar}
\end{lemma}
\begin{proof}
  All proofs are given in \module{Catt.Wedge.Typing}.
\end{proof}

We finally show that the wedge sum preserves pasting diagrams, the
property that wedge sums were initially introduced for.

\begin{proposition}
  \label{prop:wedge-ps}
  The wedge sum of two ps-contexts is a ps-context: If \(\Gamma
  \vdash_{\mathsf{ps}}\) and \(\Delta \vdash_{\mathsf{ps}}\), then
  \(\Gamma \vee \Delta \vdash_{\mathsf{ps}}\)
\end{proposition}
\begin{proof}
  It can first be proven that if the derivation \(\Gamma
  \vdash_{\mathsf{ps}}\) is generated by \(\Gamma
  \vdash_{\mathsf{ps}} x : \star\), then \(x \equiv \snd(\Gamma)\),
  by showing for all derivations \(\Gamma \vdash_{\mathsf{ps}} x : A
  \), where \(\dim(A) > 0\) that the \(0\)-target of the type \(A\)
  is \(\snd(\Gamma)\) by induction, and then case splitting on the
  original derivation. Then \(\Gamma \vdash_{\mathsf{ps}}\) implies
  that \(\Gamma \vdash_{\mathsf{ps}} \snd(\Gamma) : \star\).

  The statement of the proposition is then proven by induction on the
  following statement: If \(\Gamma \vdash_{\mathsf{ps}}\) and
  \(\Delta \vdash_{\mathsf{ps}} x : A\), then:
  \[ \Gamma \vee \Delta \vdash_{\mathsf{ps}} x \sub {\inc_{\Delta}} :
  A \sub {\inc_{\Delta}}\]
  The base case is given by the preceding paragraph, and the other
  cases follow from routine calculation.

  These proofs are given in \module{Catt.Wedge.Pasting}.
\end{proof}

We lastly give a version of the wedge sum construction for variable sets.

\begin{definition}
  Let \(\Gamma\) and \(\Delta\) be two non-empty contexts, and let
  \(U \subseteq \Var(\Gamma)\) and \(V \subseteq \Var(\Delta)\) be
  variable sets. Then define:
  \[U \vee V = U \sub {\inc_\Gamma} \cup V \sub {\inc_\Delta}\]
  to be a variable set of \(\Gamma \vee \Delta\).
\end{definition}

\subsection{Tree contexts}
\label{sec:tree-contexts}

We have now defined suspensions and wedge sums, and shown that both
operations preserve ps-contexts. This allows us to define the context
generated by a tree.

\begin{definition}
  For a tree \(T\), the context $\lfloor T \rfloor$ generated from it
  is defined recursively by:
  \[\lfloor \emp \rfloor = D^0 \qquad
    \lfloor [T_0,\dots,T_n] \rfloor = \bigvee\limits_{i = 0}^n
  \Sigma\lfloor T_i \rfloor\]
  It is immediate from this definition that \(\lfloor \Sigma(T)
  \rfloor \equiv \Sigma(\lfloor T \rfloor)\), \(\lfloor S \doubleplus
  T \rfloor \equiv \lfloor S \rfloor \vee \lfloor T \rfloor\), and
  that \(\dim(\lfloor T \rfloor) = \dep(T)\).
\end{definition}

We can immediately give some examples of trees and their associated
contexts. The context \(D^0\) is defined to be the context associated
to \(\emp\),
and so as \(D^{n+1} \equiv \Sigma(D^n)\), all the disc contexts can
easily be recovered from trees as \(D^n \equiv \lfloor \Sigma^n(\emp)
\rfloor\). Each tree \(\Sigma^n(\emp)\) is linear and has depth \(n\).

Trees can also be drawn graphically as follows: For a tree
\([T_0,\dots,T_n]\), first recursively draw the trees \(T_i\) and lay
these out in a horizontal line. Then a single point is drawn
underneath these subtrees which we call the root of the tree, and a
line is and drawn between the root of the tree and the root of each
subtree. An example is given in \cref{fig:tree-example}.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[every node/.style={scale=0.6},baseline=(x11.base)]
    \node [on grid](x01) {$\bullet$};
    \node [above left=0.5 and 0.3 of x01, on grid] (x11) {$\bullet$};
    \node [above left=0.5 and 0.25 of x11, on grid] (x21) {$\bullet$};
    \node [above right=0.5 and 0.25 of x11, on grid] (x22) {$\bullet$};
    \node [above right=0.5 and 0.3 of x01, on grid](x12) {$\bullet$};
    \draw (x01.center) to (x11.center);
    \draw (x01.center) to (x12.center);
    \draw (x11.center) to (x21.center);
    \draw (x11.center) to (x22.center);
  \end{tikzpicture}
  \qquad
  \begin{tikzcd}
    \bullet & \bullet & \bullet
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-30pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=30pt}, from=1-1, to=1-2]
    \arrow[""{name=2, anchor=center, inner sep=0}, from=1-1, to=1-2]
    \arrow[from=1-2, to=1-3]
    \arrow[shorten <=4pt, shorten >=4pt, Rightarrow, from=1, to=2]
    \arrow[shorten <=4pt, shorten >=4pt, Rightarrow, from=2, to=0]
  \end{tikzcd}
  \caption{The tree \([[\emp,\emp],\emp]\) and generated context.}
  \label{fig:tree-example}
\end{figure}

The context associated to a tree is clearly a pasting diagram, as the
context is built only using the singleton context, wedge sums, and
suspension. In fact, the set of contexts generated by trees is
exactly the set containing the singleton context, and closed under
wedge sums and suspensions. Further, it is proven in the
formalisation module \module{Catt.Dyck.FromTree} that all pasting
diagrams are generated by some tree, though this will not be needed
for any formal development of our type theories.

We next introduces \emph{paths}, which can be thought of as the
variables in a tree.

\begin{definition}
  Let \(T\) be a tree. \emph{Paths} \(p : \Path_T\) are non-empty
  lists of natural numbers of the form \(q \doubleplus [n]\) such
  that \(q\) indexes a subtree \(T^q\) of \(T\) and \(0 \leq n \leq
  \len(T^q) \).

  For path \(p : \Path_T\), we obtain a variable of \(\lfloor T
  \rfloor\) by recursion on \(p\) as follows:
  \begin{itemize}
    \item Suppose \(p = [n]\). Let \(T = [T_0,\dots,T_k]\). It is
      clear that \(\lfloor T \rfloor\) has exactly \(k+2\) variables
      of dimension \(0\), corresponding to (inclusion of) the first
      variable of each context \(\Sigma(\lfloor T_i \rfloor)\) as
      well as the variable corresponding to the inclusion of
      \(\snd(\Sigma(T_i))\). We then define \(\lfloor [n] \rfloor\)
      to be the \(n\)\textsuperscript{th} such variable, indexing from 0.
    \item Let \(p = k :: q\) and \(T = [T_0,\dots,T_k,\dots]\), where
      \(q\) is a path of \(T_k\). Then by recursion we have a
      variable \(\lfloor q \rfloor\) of \(\lfloor T_k \rfloor\). This
      gives a variable \(\Sigma(\lfloor q \rfloor)\) of
      \(\Sigma(\lfloor T_k \rfloor)\) which can be included into
      \(\lfloor T \rfloor\) by the appropriate inclusion to get
      \(\lfloor p \rfloor\).
  \end{itemize}
  We lastly define the set of \emph{maximal paths} \(\MaxPath_T\) of
  \(T\) to be paths \(p \doubleplus [0] \) such that \(T^p = \emp\).
  Such paths correspond to locally maximal variables of \(\lfloor T \rfloor\).
\end{definition}

We now turn our attention to substitutions from a tree context
\(\sigma : \lfloor T \rfloor \to \Gamma\). A substitution can be
viewed as a function from the variables of its source context to
terms of the target context. Therefore, a substitution \(\sigma :
\lfloor T \rfloor \to \Gamma\) acts on variables of \(\lfloor T
\rfloor\). However, we have seen that the more natural notion of a
variable in a tree context is a path. This motivates the following definition.

\begin{definition}
  A term-labelling \(L : T \to \Gamma\) from a tree \(T\) to context
  \(\Gamma\) is a pair containing a function \(\Path_T \to
  \Term_\Gamma\) and a type of \(\Gamma\). To apply the function
  component of a labelling to a path \(p\), we write \(L(p)\) or
  \(L[x_0,x_1,\dots]\) for a path \([x_0,x_1,\dots]\). The type
  component of the labelling is given by \(\ty(L)\).

  If \(T = [T_0,\dots,T_n]\), then there are natural projections \(
  L_i :T_i \to \Gamma\) given by \(L_i(p) = L(i :: p)\) and \(\ty(L)
  = \arr {L[i]} {\ty(L)} {L[i+1]}\) for \(0 \leq i \leq n\).
\end{definition}

For labellings to play the role of substitutions, a substitution
\(\lfloor L \rfloor : \arr {\lfloor T \rfloor} {\ty(L)} \Gamma\) will
be defined for each term-labelling \(L : T \to \Gamma\). A natural
way to define this substitution is by induction on the tree \(T\),
which motivates the use of extended substitutions. Suppose we start
with a labelling \(L : \arr {[T_0,\dots,T_n]} {} \Gamma\). To
proceed, we will apply the inductive hypothesis to obtain the substitutions:
\[ \lfloor L_i \rfloor : \arr {\lfloor T_i \rfloor} {\arr {L[i]}
{\ty(L)} {L[i+1]}} {\Gamma} \]
These substitutions are not regular (non-extended) substitutions,
even if \(L\) has associated type \(\star\), and so corresponds to a
regular substitution.

\begin{definition}
  Let \(L : T \to \Gamma\) be a term-labelling. We define the substitution:
  \[\lfloor L \rfloor : \arr {\lfloor T \rfloor} {\ty(L)} \Gamma\]
  by induction on the tree \(T\) as \(\langle \ty(L), L[0] \rangle\)
  if \(T = \emp\) and:
  \[ \unrestrict \lfloor L_0 \rfloor \vee \unrestrict \lfloor L_1
  \rfloor \vee \cdots \vee \unrestrict \lfloor L_n \rfloor \]
  if \(T = [T_0, \dots, T_n]\). Although it looks like the
  \(0\)-dimensional terms in the labelling are not used to generate
  the substitution, they appear in the types of the labellings
  \(L_i\), and so appear in the unrestricted substitutions.
\end{definition}

There are many ways of giving a more syntactic presentation of
labellings. Given a tree \(T = [T_0,\dots,T_n]\), a labelling \(L : T
\to \Gamma\) can be written as:
\[ t_0\{L_0\}t_1\{L_1\}t_2\cdots t_n\{L_n\}t_{n+1} : \ty(L) \]
where each \(t_i\) is the term \(L[i]\) and the sublabellings \(L_i\)
have been recursively put in this syntactic bracketing format
(omitting the type). The syntactic presentation contains all the
information of the original labelling, which can be recovered by
letting \(L[i] = t_i\) for each \(i\), \(L[i :: p] = L_i(p)\).

As an example, take the tree \(T = [[\emp,\emp], \emp]\) from
\cref{fig:tree-example}, and let:
\[\Gamma = (x : \star), (f : x \to x), (\alpha : f*f \to f)\]
Then we can define the labelling \(L : T \to \Gamma\) by:
\[ L = x\bigl\{f*f\{\alpha\}f\{\id(f)\}f\bigr\}x\{f\}x : \star \]
which sends the (maximal) paths \([0,0,0]\) to \(\alpha\),
\([0,1,0]\) to \(\id(f)\), and \([1,0]\) to \(f\), and has associated
substitution:
\[ \lfloor L \rfloor = \langle x,x,f*f,f,\alpha,f,\id(f),x,f \rangle\]
The curly brackets notation for labellings is used instead of a
typical round bracket notation to avoid clashes with notations that
already use round brackets, such as \(\id(f)\).

We finish this section by examining a boundary operation for trees.
We have already seen that for every ps-context \(\Gamma\) and \(n \in
\mathbb{N}\), there are the boundary variable sets:
\[\bdry n - \Gamma \qquad \bdry n + \Gamma\]
Since \(\lfloor T \rfloor\) is a ps-context for any tree \(T\), we
immediately obtain such boundary variable sets for \(\lfloor T
\rfloor\). However, by recalling the definitions for the wedge sum of
variable sets given in \cref{sec:wedge-sums} and the suspension of a
variable set given in \cref{sec:operation-properties}, a more natural
definition can be given.

\begin{definition}
  For any tree \(T : \Tree\), dimension \(n \in \mathbb{N}\), and
  \(\epsilon \in \{-,+\}\), we define the boundary set:
  \[\bdry n \epsilon T\]
  by induction on \(n\) and \(T\). If \(n = 0\), then we define:
  \[\bdry 0 - T = \FV(\fst(\lfloor T \rfloor)) \qquad \bdry 0 + T =
  \FV(\snd(\lfloor T \rfloor))\]
  Now suppose \(n\) is not \(0\). If the tree \(T\) is the singleton
  tree, then \(\bdry n \epsilon T = \Var(\lfloor T \rfloor)\). Now
  suppose that \(T = [T_0,\dots,T_n]\). We then define:
  \[ \bdry n \epsilon T = \bdry {n-1} \epsilon {T_0} \vee \cdots \vee
  \bdry {n-1} \epsilon {T_n}\]
  with the boundary sets \(\bdry n \epsilon {T_i}\) obtained by
  inductive hypothesis.
\end{definition}

In the formalisation module \module{Catt.Tree.Support}, we prove that
the boundary sets \(\bdry n \epsilon T\), the tree boundary, and
\(\bdry n \epsilon {\lfloor T \rfloor}\), the ps-context boundary,
coincide. Therefore:
\[ (\lfloor S \rfloor, \bdry n - T, \bdry n + T) \in \Std\]
for each \(n \geq \dep(S) - 1\).

\section{Structured syntax}
\label{sec:structured-terms}

We now introduce a new class of syntax named \emph{structured
syntax}. Terms over tree contexts are commonly built using several of
the standard constructions we have seen so far, such as paths,
labellings, suspensions, and inclusions. By recording which of these
constructions was used in the formation of a term, these terms can
compute more usefully, which we will exploit to prove more involved
lemmas about insertion in \cref{sec:insertion}. Structured syntax
will be our variation on the base syntax of \Catt which records these
constructions.

The key problem with the base syntax for \Catt is that
term-labellings are difficult to compose. We have so far considered
term-labellings of the form \(L : T \to \Gamma\), where \(\Gamma\) is
any arbitrary context, but there is no reason a labelling couldn't be
of the form \(M : S \to \lfloor T \rfloor\) for trees \(S\) and
\(T\). We would then hope to be able to compose these labellings to
get a labelling of the form:
\[ M \bullet L : S \to \Gamma \]
Such a labelling would need to send a path \(p: \Path_S\) to a term
of \(\Gamma\). The only reasonable way forward is to apply \(M\) to
\(p\) to get a term of \(\lfloor T \rfloor\), and then apply
\(\lfloor L \rfloor\) to this term to get a term of \(\Gamma\).
Unfortunately, for an arbitrary term \(t : \Term_{\lfloor T
\rfloor}\) and labelling \(L : T \to \Gamma\), the term:
\[ t \sub {\lfloor L \rfloor}\]
does not have nice computational properties. We examine two examples:
\begin{itemize}
  \item Suppose \(t\) was of the form \(\lfloor p \rfloor\) for some
    path \(p\). We then have:
    \[ \lfloor p \rfloor \sub {\lfloor L \rfloor} \equiv L(p)\]
    and would hope that this syntactic equality would fall out
    immediately, and that the left-hand side would reduce to the
    right-hand side in the formalisation. This is not the case
    however, and proving that such a syntactic equality holds is non-trivial.
  \item Suppose \(t \equiv \Sigma(s)\) and \(L = a\{L_1\}b : A\).
    Similar to the above case we would hope that the syntactic equality:
    \[ \Sigma(s) \sub {\lfloor a\{L_1\}b : A \rfloor} \equiv s \sub
    {\lfloor L_1 \rfloor}\]
    holds ``on the nose''. This however is not the case.
\end{itemize}

Structured terms alleviate these problems by recording that such a
term \(t\) was generated from a path or generated using suspension.
This allows the application of a labelling to a structured term to
use this information, for example letting the two syntactic
equalities above to hold by definition. If a labelling is the
``correct'' notion of substitution from a tree, then a structured
term is the ``correct'' notion of term in a tree.

\begin{definition}
  Let \(\U\) be a member of \(\Ctx \uplus \Tree\), either some
  context \(\Gamma\) or some tree \(T\). We then define the
  \emph{structured syntax} classes \(\STerm_\U\) of \emph{structured
  terms}, \(\SType_\U\) of \emph{structured types}, and
  \emph{(\(\STerm\)-)labellings} \(\arr S {} \U\) for some tree
  \(S\). The syntax classes for structured terms and types are
  generated by the following rules:
  \begin{mathpar}
    \inferrule{p : \Path_T}{\SPath(p) : \STerm_T}\and
    \inferrule{s : \STerm_{T_i}\\ 0 \leq i \leq n}{\Inc_i(s) :
    \STerm_{[T_0,\dots,T_n]}} \and
    \inferrule{S : \Tree\\ A : \SType_S \\ L : S \to \U}{\SCoh S A L
    : \STerm_\U}\and
    \inferrule{t : \Term_\Gamma}{\SOther(t) : \STerm_\Gamma} \\
    \inferrule{ }{\star : \SType_\U}\and
    \inferrule{s : \STerm_\U \\ A : \SType_\U \\ t: \STerm_\U}{\arr s
    A t : \SType_\U}
  \end{mathpar}
  Labellings \(L : S \to \U\) are defined as pairs of a function
  \(\Path_S \to \STerm_\U\) and structured type, similarly to
  term-labellings in \cref{sec:tree-contexts}. We note that the
  syntax for structured types is shared with the syntax for \Catt
  types, and will be careful to make it clear which syntax we are
  using when necessary.
\end{definition}

Each piece of structured syntax can be converted back into the base
syntax of \Catt, using many of the constructions already introduced.

\begin{definition}
  Suppose \(\U : \Ctx \uplus \Tree\). Define \(\lfloor \U \rfloor\)
  to be \(\Gamma\) if \(\U = \Gamma\) for some context \(\Gamma\) or
  \(\lfloor T \rfloor\) if \(\U = T\) for some tree \(T\). Now, for a
  structured term \(s : \STerm_\U\), a structured type \(A :
  \SType_\U\), or a labelling \(L : S \to \U\), we define:
  \[ \lfloor s \rfloor : \Term_{\lfloor \U \rfloor} \qquad \lfloor A
    \rfloor : \Type_{\lfloor \U \rfloor} \qquad \lfloor L \rfloor :
  \arr {\lfloor S \rfloor} {\lfloor \ty(L) \rfloor} {\lfloor \U \rfloor} \]
  by the equations:
  \begin{align*}
    \lfloor \SPath(p) \rfloor &= \lfloor p \rfloor\\
    \lfloor \Inc_i(s) \rfloor &= \Sigma(\lfloor s \rfloor) \sub
    {\inc_{\lfloor T_i \rfloor}}&\text{if }s : \STerm_{[T_0,\dots,T_n]}\\
    \lfloor \SCoh S A L \rfloor &= \Coh {\lfloor S \rfloor} {\lfloor
    A \rfloor} {\id_{\lfloor S \rfloor}} \sub {\lfloor L \rfloor}\\
    \lfloor \SOther(t) \rfloor &= t\\[10pt]
    \lfloor \star \rfloor &= \star\\
    \lfloor \arr s A t \rfloor &= \arr {\lfloor s \rfloor} {\lfloor A
    \rfloor} {\lfloor t \rfloor}
  \end{align*}
  and by defining \(\lfloor L \rfloor\) similarly to term labellings
  except \(\lfloor L \rfloor = \langle \lfloor \ty(L) \rfloor,
  \lfloor L[0] \rfloor \rangle\) for labellings \(L : {\emp} \to
  {\U}\) from the singleton tree. We refer to \(\lfloor a \rfloor\),
  \(\lfloor A \rfloor\) and \(\lfloor L \rfloor\) as the term, type,
  or substitution generated by \(a\),\(A\), or \(L\).
\end{definition}

For any tree \(T\), there is an \emph{identity labelling} \(\id_T\) given by:
\[ \id_T(p) = \SPath(p) \qquad \ty(\id_T) = \star\]
The function \func{Catt.Tree.Structured.Properties}{id-label-to-sub}
in the formalisation (see \module{Catt.Tree.Structured.Properties}) shows that:
\[\lfloor \id_T \rfloor = \id_{\lfloor T \rfloor}\]

The main motivation for introducing structured syntax was to be able
to define a composition of labellings, which we do now by defining
the application of a labelling to a structured term, structured type,
or another labelling.

\begin{definition}
  Let \(L : T \to \U\) be a labelling (with \(\U : \Ctx \uplus
  \Tree\)). We define the application of \(L\) to a structured term
  \(s : \STerm_{T}\), a structured type \(A : \SType_T\), and a
  labelling \(M : S \to T\) to give:
  \[ s \sub L : \STerm_\U \qquad A \sub L : \SType_\U \qquad M
  \bullet L : S \to \U\]
  These definitions are given by mutual recursion:
  \begin{align*}
    \SPath(p) \sub L &= L(p)\\
    \Inc_i(s) \sub L &= s \sub {L_i}\\
    \SCoh S A M \sub L &= \SCoh S A {M \bullet L}\\
    \SOther(t) \sub L &= t \sub {\lfloor L \rfloor}\\[10pt]
    \star \sub L &= B\\
    (\arr s A t) \sub L &= \arr {s \sub L} {A \sub L} {t \sub L}\\[10pt]
    (M \bullet L)(p) &= M(p) \sub L\\
    \ty(M \bullet L) &= \ty(M) \sub L
  \end{align*}
  It can easily be seen that these definitions satisfy the
  computational properties given at the start of the section.
\end{definition}

The main theorem of this section is that the application of a
labelling to a structured term is compatible with the map from
structured syntax to \Catt syntax.

\begin{theorem}
  \label{thm:structured-main}
  For any labelling \(L : T \to \U\) and structured term \(s :
  \STerm_T\), structured type \(A : \SType_T\), or labelling \(M : S
  \to T\), we have:
  \[ \lfloor s \sub L \rfloor \equiv \lfloor s \rfloor \sub {\lfloor
    L \rfloor} \qquad \lfloor A \sub L \rfloor \equiv \lfloor A \rfloor
    \sub {\lfloor L \rfloor} \qquad \lfloor M \bullet L \rfloor \equiv
  \lfloor M \rfloor \bullet \lfloor L \rfloor\]
\end{theorem}
\begin{proof}
  We proceed by proving all statements by mutual induction. Suppose
  \(s : \STerm_T\) is a structured term. We split on the form of \(s\):
  \begin{itemize}
    \item Suppose \(s\) is of the form \(\SCoh S A M\). Then \(s \sub
      L\) is \(\SCoh S A {M \bullet L}\) and so the required
      statement follows from the inductive hypothesis for labellings.
    \item Suppose \(s\) is of the form \(\SOther(t)\). Then \(\lfloor
        s \sub L \rfloor \equiv \lfloor \SOther (t \sub {\lfloor L
        \rfloor}) \rfloor \equiv t \sub {\lfloor L \rfloor} \equiv
      \lfloor s \rfloor \sub {\lfloor L \rfloor}\).
    \item Suppose \(T = [T_0,\dots, T_n]\) and \(s\) is of the form
      \(\Inc_i(t)\). Then:
      \begin{align*}
        \lfloor \Inc_i(t) \rfloor \sub {\lfloor L \rfloor} &\equiv
        \Sigma(t)\sub {\inc_{\lfloor T_i \rfloor}} \sub {\unrestrict
        \lfloor L_0 \rfloor \vee \cdots \vee \unrestrict \lfloor L_n \rfloor}\\
        &\equiv \Sigma(t)\sub {\inc_{\lfloor T_i \rfloor} \bullet
          (\unrestrict \lfloor L_0 \rfloor \vee \cdots \vee \unrestrict
        \lfloor L_n \rfloor)}\\
        &\equiv \Sigma(t) \sub {\unrestrict \lfloor L_i \rfloor}
        &\text{by \cref{lem:wedge-sum-prop}}\\
        &\equiv \lfloor t \rfloor \sub {\lfloor L_i \rfloor}\\
        &\equiv \lfloor t \sub {L_i} \rfloor &\text{by inductive hypothesis}\\
        &\equiv \lfloor \Inc_i(t) \sub L \rfloor
      \end{align*}
    \item Suppose \(s\) is of the form \(\SPath(p)\). Then if
      \(\lfloor p \rfloor\) is not a \(0\)-dimensional variable, then
      an argument similar to the preceding case can be made. If
      instead \(\lfloor p \rfloor\) is of the form \([k]\) and \(T =
      [T_0,\dots,T_n]\) then first suppose that \(k < n + 1\) such
      that \(\lfloor [k] \rfloor \equiv \fst(\lfloor T_k \rfloor)
      \sub {\inc_{\lfloor T_k \rfloor}}\). Then:
      \begin{align*}
        \lfloor [k] \rfloor \lfloor L \rfloor &\equiv \fst(\lfloor
        T_k \rfloor) \sub {\inc_{\lfloor T_k \rfloor}} \sub {\lfloor
          \unrestrict \lfloor L_0 \rfloor \vee \cdots \vee \unrestrict
        \lfloor L_n \rfloor \rfloor}\\
        &\equiv \fst(\lfloor T_k \rfloor) \sub {\unrestrict \lfloor
        L_k \rfloor}\\
        &= \lfloor L[k] \rfloor
      \end{align*}
      where the last equality follows from the labelling \(L_k\)
      having type component \(\ty(L_k) \equiv \arr {\lfloor L[k]
      \rfloor} {B} {\lfloor L[k+1] \rfloor}\). The case where \(k =
      n+1\) is similar to above using \(\snd(T_n)\) instead of
      \(\fst(T_k)\) (as there is no tree \(T_k\) in this case).
  \end{itemize}

  The case for structured types follows by a simple induction using
  the case for terms. We now consider the case for a label \(M : S
  \to T\). Suppose \(S = [S_0,\dots,S_n]\). Then:
  \begin{align*}
    \lfloor M \rfloor \bullet \lfloor L \rfloor &\equiv \left(
    \bigvee_i \unrestrict \lfloor M_i \rfloor  \right) \bullet
    \lfloor L \rfloor\\
    &\equiv \bigvee_i \unrestrict \lfloor M_i \rfloor \bullet \lfloor
    L \rfloor&\text{by \cref{lem:wedge-sum-prop}}\\
    &\equiv \bigvee_i \unrestrict \left(\lfloor M_i \rfloor \bullet
    \lfloor L \rfloor\right)\\
    &\equiv \bigvee_i \unrestrict \lfloor M_i \bullet L
    \rfloor&\text{by inductive hypothesis}\\
    &\equiv \lfloor M \bullet L \rfloor
  \end{align*}
  with the last line following from \((M \bullet L)_i\) and \(M_i
  \bullet L\) being the same labelling. This concludes all cases.
\end{proof}

Structured syntax is only used as computational aid for reasoning
about the base syntax of \Catt, and therefore the desired notion of
``syntactic'' equality of structured syntax is syntactic equality of
the underlying \Catt terms, that is we say \(s \equiv t\) for
structured terms \(s\) and \(t\) exactly when \(\lfloor s \rfloor
\equiv \lfloor t \rfloor\). On labellings \(L, M : T \to \U\) we can
instead provide the equality:
\[ L \equiv M \iff \ty(L) \equiv \ty(M) \land \forall (p :
\Path_T).\ L(p) \equiv M(p)\]
and by observing the proof of \cref{thm:structured-main}, we see that
this equality implies equality of the generated substitutions.

It is therefore possible to derive many properties for this equality
of structured terms simply by reducing all constructions used to the
corresponding \Catt constructions, and using the corresponding result
for the syntax of \Catt.

\begin{proposition}
  Composition of labellings is associative and has a left and right
  unit given by the identity labelling.
\end{proposition}
\begin{proof}
  Follows immediately from \cref{thm:structured-main}, the identity
  labelling generating the identity substitution, and the
  corresponding results for \Catt.
\end{proof}

Using this technique, every syntactic result about \Catt can be
transported to structured syntax. Further, it is easy to prove that
the equality relation is preserved by each constructor, for example
if \(L \equiv M\) and \(A \equiv B\), then \(\SCoh S A L \equiv \SCoh A B M\).

To extend this, we redefine some constructions we have seen for \Catt
in the previous sections, this time for structured terms.

\begin{definition}
  We define the suspension for a structured term \(a : \STerm_\U\),
  structured type \(A : \STerm_\U\), and restricted substitution for
  a labelling \(L : T \to \U\), giving structured term \(\Sigma(a) :
  \STerm_{\Sigma(\U)}\), structured type \(\Sigma(A) :
  \STerm_{\Sigma(\U)}\), and labelling \(\Sigma'(L) : T \to
  {\Sigma(\U)}\). These are all defined by mutual induction as follows:
  \begin{align*}
    \Sigma(a) &\equiv \Inc_0(a) &\text{if \(\U\) is a tree}\\
    \Sigma(\SCoh S A M) &\equiv \SCoh S A {\Sigma'(M)}&\text{if
    \(\U\) is a context}\\
    \Sigma(\SOther(t))&\equiv \SOther(\Sigma(t))\\[10pt]
    \Sigma(\star) &= \arr N \star S&\text{if \(\U\) is a context}\\
    \Sigma(\star) &= \arr {\SPath[0]} \star {\SPath[1]}&\text{otherwise}\\
    \Sigma(\arr s A t) &= \arr {\Sigma(s)} {\Sigma(A)} {\Sigma(t)}\\[10pt]
    \Sigma'(L)(p) &= \Sigma(L(p))\\
    \ty(\Sigma'(L)) &= \Sigma(\ty(L))
  \end{align*}
  We further define an unrestriction operation that takes a labelling
  of the form \(M : T \to \U\)  with \(\ty(M) \equiv \arr s A t\) and
  produces a labelling
  \[\unrestrict M : {\Sigma(T)} \to \U \equiv s\{M\}t : A\]
  This can be used to define the full suspension of a labelling as
  with \Catt substitutions by defining \(\Sigma(L)\) to be
  \(\unrestrict \Sigma'(L)\).
\end{definition}

A simple case analysis demonstrates that these constructions commute
with \(\lfloor \_ \rfloor\). They therefore inherit the properties of
the suspension on \Catt terms, types, and substitutions. We lastly
recover wedge sums for structured syntax.

\begin{definition}
  We have seen that the wedge sum of trees \(S\) and \(T\) is given
  by \(S \doubleplus T\). Letting \(S = [S_0,\dots,S_m]\) and \(T =
  [T_0,\dots,T_n]\), we further define inclusion labellings:
  \[ \inc_S : S \to {S \doubleplus T} \qquad \inc_T : T \to {S \doubleplus T}\]
  by the equations:
  \begin{align*}
    \inc_S([k])&\equiv \SPath[k] & \inc_S(k :: p) &\equiv \SPath(k ::
    p) & \ty(\inc_S) &\equiv \star\\
    \inc_T([k])&\equiv \SPath[m + k] & \inc_T(k :: p) &\equiv \SPath
    (m + k :: p) & \ty(\inc_T) &\equiv \star
  \end{align*}
  and finally, we suppose \(L : S \to \U\) and \(M : T \to \U\) are
  labellings of the form:
  \[ L \equiv s_0\{L_0\}s_1\cdots s_n\{L_n\}t_0 : A \qquad M \equiv
  t_0\{M_0\}t_1\cdots t_n\{M_n\}t_{n+1} : A \]
  and define their concatenation to be the labelling:
  \[ L\doubleplus M \equiv s_0\{L_0\}s_1\cdots
  s_n\{L_n\}t_0\{M_0\}t_1\cdots t_n\{M_n\}t_{n+1} : A \]
  where \(L \doubleplus M : {S \doubleplus T} \to \U\).
\end{definition}

Many properties of these constructions among others are given in the
formalisation module
\module{Catt.Tree.Structured.Construct.Properties}. In particular,
the diagrammatic notation for substitutions between wedge sums can be
repurposed to define labellings, which will be used to define certain
labellings in \cref{sec:insertion}.

It will be useful to be able to interpret all \Catt syntax as
structured syntax. For terms such a mapping is trivially given by the
\(\SOther\) constructor. For a type \(A\), a structured type \(\lceil
A \rceil\) can be formed by a simple induction, applying the
\(\SOther\) constructor to each term in the type. For substitutions,
we give the following definition.

\begin{definition}
  Let \(\sigma : \lfloor S \rfloor \to_A \Gamma\) be a substitution.
  We then define the labelling:
  \[ \lceil \sigma \rceil : S \to \Gamma \]
  by \(\lceil \sigma \rceil(p) = \SOther(\lfloor p \rfloor \sub
  \sigma)\) and \(\ty(\lceil \sigma \rceil) = \lceil A \rceil\).
\end{definition}

This construction is an inverse to taking generating a substitution
from a labelling.

\begin{proposition}
  Let \(\sigma : \lfloor S \rfloor \to_A \Gamma\) be a substitution.
  Then \(\lfloor \lceil \sigma \rceil \rfloor \equiv \sigma\).
  Further, for any labelling \(L : S \to \Gamma\), \(\lceil \lfloor L
  \rfloor \rceil \equiv L\).
\end{proposition}
\begin{proof}
  We note that every variable of \(\lfloor S \rfloor\) is given by
  \(\lfloor p \rfloor\) for some path \(p\). We then have the equality:
  \begin{equation*}
    \lfloor p \rfloor \sub {\lfloor \lceil \sigma \rceil \rfloor}
    \equiv \lfloor p \sub {\lceil \sigma \rceil} \rfloor \equiv
    \lfloor \SOther(\lfloor p \rfloor \sub \sigma) \rfloor \equiv
    \lfloor p \rfloor \sub \sigma
  \end{equation*}
  and so \(\sigma\) and \(\lfloor \lceil \sigma \rceil \rfloor\) have
  the same action on each variable and so are equal.

  Letting \(L : S \to \Gamma\) be a labelling. Then for any path \(p\):
  \[ \lceil \lfloor L \rfloor \rceil(p) \equiv \SOther(\lfloor p
  \rfloor \sub {\lfloor L \rfloor}) \equiv \SOther(\lfloor L(p) \rfloor) \]
  and so \(\lfloor \lceil \lfloor L \rfloor \rceil(p) \rfloor \equiv
  \lfloor L(p) \rfloor\). Therefore, \(L \equiv \lceil \lfloor L
  \rfloor \rceil\) by definition.
\end{proof}

\subsection{Typing and equality}
\label{sec:typing-struct-terms}

Similarly to the definition of syntactic equality for structured
syntax, we also want the equality rules for structured terms and
structured types to be inherited from the equality relations on their
generated terms, and so define:
\[ \U \vdash s = t \iff \lfloor \U \rfloor \vdash \lfloor s \rfloor =
  \lfloor t \rfloor \qquad \U \vdash A = B \iff \lfloor \U \rfloor
\vdash \lfloor A \rfloor = \lfloor B \rfloor\]

For labellings, (definitional) equality can be defined similarly to
the syntactic equality relation:
\[ \U \vdash L = M \iff \U \vdash \ty(L) = \ty(M) \land \forall (p :
\Path_T).\ \U \vdash L(p) = M(p)\]
Using \cref{lem:wedge-typing}, it can be proven by a simple induction
that equality of labellings (along with equality of their associated
types) induces equality of the generated substitutions.

We also want the typing rules for \(s : \STerm_\U\) and \(A :
\SType_\U\) to be inherited from the typing rules for \(\lfloor s
\rfloor\) and \(\lfloor A \rfloor\). We re-use the notation for each
typing judgement. For labellings, we introduce the following more
natural typing judgement:

\begin{definition}
  For a labelling \(L : T \to \U\), where \(\U : \Ctx \uplus \Tree\),
  we define the judgement:
  \[ \U \vdash L : T \]
  to mean that the labelling \(L\) is well-formed. This judgement is
  generated by the following rule:
  \begin{mathpar}
    \inferrule{\U \vdash L[0] : \ty(L)\quad \cdots\quad \U\vdash
      L[n+1] : \ty(L)\\\U\vdash L_0 : T_0\quad \cdots\quad\U\vdash L_n
    : T_n}{\U \vdash L : [T_0,\dots,T_n]}
  \end{mathpar}
\end{definition}

Paths \(p\) can be equipped with a canonical structured type,
\(\ty(p)\), as follows:
\begin{itemize}
  \item For paths \([k]\), \(\ty([k]) = \star\),
  \item For paths \(k :: p\) where \(p\) is a path, the type \(\ty(k
    :: p)\) is obtained by taking the type \(\ty(p)\), applying
    \(\Inc_k\) to each term, and replacing the \(\star\) type at its
    base by the type \(\arr {\SPath[k]} {\star} {\SPath[k+1]}\).
\end{itemize}
This can be used to prove that the identity labelling is well-formed.
\begin{proposition}
  Let \(S\) be a tree. Then \(S \vdash \id_S : S\).
\end{proposition}
\begin{proof}
  Let \(x\) be a list that indexes a subtree of \(S\), and define the
  labelling \(\mathsf{subtree}(x) : S^x \to x\) by
  \(\ty(\mathsf{subtree}(x)) = \ty(x \doubleplus [0])\) and
  \(\mathsf{subtree}(x)(p) = \SPath(x \doubleplus p)\).

  We then prove the more general result that \(S \vdash
  \mathsf{subtree}(x) : S^x\) for each \(x\), with the desired result
  following from the case \(x = \emp\). If \(S^x = \emp\), then the
  typing judgement follows from \(S \vdash S^x[0] : \ty(S^x[0])\).

  If \(S^x = [T_0, \dots, T_n]\) then we must show that \(S \vdash
  S^x[k] : \ty(S^x[0])\), which follows from the observation that
  \(\ty(S^x[0]) \equiv \ty(S^x[i])\) for any \(i\) as the definition
  does not use the last element of the path. We are also required to
  show that \(S \vdash S^x_i : T_i\), but \(T_i \equiv S^{x
  \doubleplus [i]}\) and \(S^x_i \equiv S^{x \doubleplus [i]}\), and
  so this follows from inductive hypothesis.
\end{proof}

From this typing judgement for labellings, one can obtain a
derivation of the typing judgement for the generated substitution.

\begin{proposition}
  Let \(L : T \to \U\), and suppose \(\U \vdash L : T\) and \(\U
  \vdash \ty(L)\). Then:
  \[ \lfloor \U \rfloor \vdash \lfloor L \rfloor : \lfloor T \rfloor\]
\end{proposition}
\begin{proof}
  We induct on the tree \(T\), splitting into cases on whether it is
  empty. If it is, then by case analysis on the judgement for label
  typing we get:
  \[ \U \vdash L[0] : \ty(L) \]
  Then, \(\lfloor L \rfloor \equiv \langle \lfloor A \rfloor, \lfloor
  L[0] \rfloor \rangle\), and so the following derivation can be obtained:
  \[
    \begin{prooftree}
      \infer0{\U \vdash A}
      \infer1{\lfloor \U \rfloor \vdash \lfloor A \rfloor}
      \infer1{\lfloor \U \rfloor \vdash \langle \lfloor A \rfloor
      \rangle : \emptyset}
      \infer0{\U \vdash L[0] : A}
      \infer1{\lfloor \U \rfloor \vdash \lfloor L[0] \rfloor :
      \lfloor A \rfloor}
      \infer2{\lfloor \U \rfloor \vdash \langle \lfloor A \rfloor,
      \lfloor L[0] \rfloor \rangle : \lfloor \emp \rfloor}
    \end{prooftree}
  \]

  Suppose instead that \(T = [T_0,\dots,T_n]\), such that:
  \[ \lfloor L \rfloor \equiv \unrestrict \lfloor L_0 \rfloor \vee
  \cdots \vee \unrestrict \lfloor L_n \rfloor\]
  From \(\U \vdash L : T\), we obtain \(\U \vdash L_i : T_i\) for
  each \(i \in \{0,\dots,n\}\). We further obtain \(\U \vdash L[k] :
  \ty(L)\) for \(0 \leq k \leq n+1\) and so:
  \[\ty(L_i) \equiv \arr {L[i]} {\ty(L)} {L[i+1]}\]
  is well-formed and so by inductive hypothesis we have \(\lfloor \U
  \rfloor \vdash \lfloor L_i \rfloor : \lfloor T_i \rfloor\). We have
  for each \(i\) that \(\lfloor \ty(L) \rfloor\) is not the type
  \(\star\) and so the unrestriction \(\unrestrict \lfloor L_i
  \rfloor\) is well-formed. Furthermore, by construction of the
  unrestriction we have:
  \[ \fst(\lfloor T_i \rfloor) \sub {\lfloor L_i \rfloor} \equiv
    \lfloor L[i] \rfloor \qquad \snd(\lfloor T_i \rfloor) \sub {\lfloor
  L_i \rfloor} \equiv \lfloor L[i+ 1] \rfloor\]
  and so by \cref{lem:wedge-typing}, the wedge sums are well-formed,
  completing the proof.
\end{proof}

It can be shown that the reverse implication also holds: if \(\lfloor
\U \rfloor \vdash \lfloor L \rfloor : \lfloor T \rfloor\) then \(\U
\vdash L : T\). This follows as a corollary from the following proposition.

\begin{proposition}
  Let \(\sigma : \arr {\lfloor T \rfloor} A \Gamma\) be a
  substitution with \(\Gamma \vdash \sigma : \lfloor S \rfloor\).
  Then for any \(L : S \to T\) we have:
  \[T \vdash L : S \implies \Gamma \vdash L \bullet \lceil \sigma \rceil : S\]
  and hence \(\Gamma \vdash \lceil  \sigma \rceil : T\) follows from
  letting \(L\) be the identity labelling.
\end{proposition}
\begin{proof}
  Let \(S = [S_0, \dots, S_n]\) (where we allow this list to be
  empty). By the definition of the typing for a labelling, it
  suffices to show that for each \(0 \leq i \leq n\) and \(0 \leq k
  \leq n + 1\) that:
  \[ S \vdash L[k] \bullet \lceil \sigma \rceil : \ty(L) \sub {\lceil
  \sigma \rceil} \qquad S \vdash (L \bullet \lceil \sigma \rceil)_i : S_i\]
  The second typing judgement follows directly from inductive
  hypothesis, as \((L \bullet \lceil \sigma \rceil)_i \equiv L_i
  \bullet \lceil \sigma \rceil\). By definition of typing for
  structured terms, the first judgement requires us to prove that:
  \[ \lfloor S \rfloor \vdash \lfloor L[k] \bullet \lceil \sigma
  \rceil \rfloor : \lfloor \ty(L) \sub {\lceil \sigma \rceil} \rfloor\]
  which is equivalent to:
  \[ \lfloor  S \rfloor \vdash \lfloor L[k] \rfloor \sub \sigma :
  \lfloor \ty(L) \rfloor \sub \sigma\]
  and so follows from typing being preserved by substitution.
\end{proof}

By these results, many of the properties enjoyed by the typing
judgements in \Cattr with a tame rule set \(\mathcal{R}\) also apply
to the typing judgements for structured terms.

The module \module{Catt.Tree.Structured.Typing.Properties} also
introduces many functions for constructing the typing judgements for
structured syntax. One such function is
\func{Catt.Tree.Structured.Typing.Properties}{TySCoh}, which
represents the admissibility of the following rule:
\begin{equation}
  \label[rule]{rule:scoh}
  \inferrule{S \vdash \arr s A t \\ \U \vdash L : S \\ \U \vdash
    \ty(L) \\ (\lfloor S \rfloor, \Supp(s), \Supp(t)) \in
  \mathcal{O}}{\U \vdash \SCoh S {\arr s A t} L}
\end{equation}
In keeping with the theme of this section, one could define
\(\Supp(s)\) as \(\Supp(\lfloor s \rfloor)\) for a structured term
\(s : \STerm_\U\). However, we choose not to do this, instead giving
a definition of support for structured syntax that leverages the
extra information available in the syntax.

\begin{definition}
  For a path \(p : \Path_T\), a structured term \(s : \STerm_\U\), a
  structured type \(A : \SType_\U\), and a labelling \(L : S \to
  \U\), we define their supports \(\Supp(p)\), \(\Supp(s)\),
  \(\Supp(A)\), and \(\Supp(L)\) by mutual recursion:
  \begin{align*}
    \Supp([n]) &= \{\lfloor [0] \rfloor\}\\
    \Supp(k :: p) &= \Sigma(\Supp(p)) \sub {\inc_{T_k}} &\text{where
    }T = [T_1,\dots,T_n]\\[10pt]
    \Supp(\SPath(p)) &= \Supp(p)\\
    \Supp(\Inc_i(s)) &= \Sigma(\Supp(s)) \sub
    {\inc_{T_k}}&\text{where }T = [T_1,\dots,T_n]\\
    \Supp(\SCoh S A L) &= \Supp(L) \cup \Supp(\ty(L))\\
    \Supp(\SOther(t)) &= \Supp(t)\\[10pt]
    \Supp(\star) &= \emptyset\\
    \Supp(\arr s A t) &= \Supp(s) \cup \Supp(A) \cup \Supp(t)\\
    \Supp(L) &= \bigcup_{i=0}^{n+1} \Supp(L[i]) \cup \bigcup_{i=0}^n\Supp(L_i)
  \end{align*}
\end{definition}

We note that each of these support definitions is naturally downwards
closed, and there is no need to apply a downwards closure operator as
was necessary for the support of \Catt syntax. By some routine
calculations given in the formalisation module
\module{Catt.Tree.Structured.Support}, these support definitions are
equivalent to taking the support of the generated piece of syntax.
More precisely, the equations:
\begin{mathpar}
  \Supp(p) = \Supp(\lfloor p \rfloor) \and \Supp(s) = \Supp(\lfloor s
  \rfloor) \and \Supp(A) = \Supp(\lfloor A \rfloor) \and \Supp(L)
  \cup \Supp(\ty(L)) = \Supp(\lfloor L \rfloor)
\end{mathpar}
for path \(p\), structured term \(s\), structured type \(A\), and
labelling \(L\).

By using this notion of support, we are able to avoid a lot of
``boilerplate'' proof. The above definition of support more closely
resembles the format of structured terms, and without this
definition, most proofs concerning the support of a structured term
would begin by simplifying a variable set similar to \(\Supp(\lfloor
s \rfloor)\) to one more similar to \(\Supp(s)\). Here, we instead
give this equivalence proof once.

We end this section by giving alternative equality relations for
labellings, which encapsulate the idea that a substitution is fully
determined by where it sends locally maximal variables. These
equalities are defined as follows for labellings \(L : T \to \U\) and
\(M : T \to \U\):
\begin{align*}
  L \equiv^{\max} M &\iff \forall (p : \MaxPath_T).\ L(p) \equiv M(p)\\
  \U \vdash L =^{\max} M &\iff \forall (p : \MaxPath_T).\ \U \vdash L(p) = M(p)
\end{align*}
and define two labels to be equal exactly when their action on
maximal paths is equal. The following theorem gives conditions for
when the standard equality relation can be recovered from these.

\begin{theorem}
  \label{thm:label-max-equality}
  Let \(L : S \to \U\) and \(M : S \to \U\) be labellings. Then the
  following rules are admissible:
  \begin{mathpar}
    \inferrule{\U \vdash L : S\\ \U \vdash M : S \\ L \equiv^{\max}
    M}{\U \vdash L = M}\and
    \inferrule{\U \vdash L : S\\ \U \vdash M : S \\ L \equiv^{\max}
    M}{\U \vdash \ty(L) = \ty(M)}
  \end{mathpar}
  If the equality rule set \(\mathcal{R}\) satisfies the preservation
  and support conditions, then the rules above are still admissible
  with \(\U \vdash L =^{\max} M\) replacing the syntactic equalities.
\end{theorem}
\begin{proof}
  We prove the results for the syntactic equality, with the results
  for the definitional equality following similarly, but using the
  preservation property instead of uniqueness of typing. We proceed
  by induction on the tree \(S\), proving the admissibility of both
  rules simultaneously.

  First suppose that \(S = \emp\). Then the path \([0] :
  \Path_{\emp}\) is maximal and so \(\U \vdash L = M\) follows by the
  reflexivity of equality. The second rule follows from the
  uniqueness of typing, as we get \(\U \vdash L[0] : \ty(L)\) and
  \(\U \vdash M[0] : \ty(M)\) from the premises.

  Now suppose that \(S = [S_0,\dots,S_n]\). By inductive hypothesis,
  the following judgements hold for each \(i \in \{0,\dots,n\}\):
  \[ \U \vdash L_i = M_i \qquad \U \vdash \arr {L[i]} {\ty(L)}
  {L[i+1]} = \arr {M[i]} {\ty(M)} {M[i+1]}\]
  From the equalities on types, we immediately get that \(\U \vdash
  \ty(L) = \ty(M)\) as is required for the admissibility of the
  second rule, and also get that \(\U \vdash L[i] = M[i]\) for each
  \(0 \leq i \leq n+1\), which along with equality on (sub)labellings
  above is sufficient to prove that:
  \[ \U \vdash L = M\]
  which witnesses the admissibility of the first rule.
\end{proof}

\subsection{Standard coherences}
\label{sec:standard-coherences}

In \cref{sec:background}, we gave a preliminary definition of
standard coherences, a definition of a canonical coherence over a
given pasting diagram. This diagram relies on inclusion substitutions
from the boundary of a pasting diagram into its source and target
variables, whose definition for ps-contexts can be unpleasant to work with.

In contrast, the \(n\)-boundary of a tree and its associated source
and target inclusions have a natural definition by induction on the
tree, where the source and target inclusions are given by labellings.
We give this definition below.

\begin{definition}
  Given dimension \(n \in \mathbb{N}\) and \(T : \Tree\), we define
  the \emph{\(n\)-boundary} of the tree \(\bound n T : \Tree\) by
  induction on \(n\) and \(T\):
  \begin{equation*}
    \bound 0 T = \emp \qquad
    \bound {n + 1} {[T_0, \dots, T_n]} = [\bound n {T_0}, \dots ,
    \bound n {T_n}]
  \end{equation*}
  We further define path-to-path functions \(\incbdpath n \epsilon T
  : \bound n T \to T\) for \(\epsilon \in \{-,+\}\) by induction:
  \begin{align*}
    \incbdpath 0 - T ([0]) &= [0]\\
    \incbdpath 0 + {[T_0, \dots, T_m]} ([0]) &= [m+1]\\
    \incbdpath {n+1} \epsilon {[T_0, \dots, T_m]} ([k]) &= [k]\\
    \incbdpath {n+1} \epsilon {[T_0,\dots, T_m]} (k :: p) &= [k ::
    \incbdpath {n+1} \epsilon {T_k} (p)]
  \end{align*}
  and then can define the \emph{source inclusion labelling} \(\incbd
  n + T : {\bound n T} \to T\) and \emph{target inclusion labelling}
  \(\incbd n + T : {\bound n T} \to T\) by:
  \[\incbd n \epsilon T(p) = \SPath(\incbdpath n \epsilon T(p))
  \qquad \ty(\incbd n \epsilon T) = \star\]
  for each \(n\) and \(\epsilon \in \{-,+\}\).
\end{definition}

In the module \module{Catt.Tree.Boundary.Typing}, it is proven that:
\[ T \vdash \incbd n \epsilon T : \bound n T\]
for all trees \(T\), \(n \in \mathbb{N}\), and \(\epsilon \in \{-,+\}\).

In \cref{sec:background}, the source and target variable sets were
defined to be support of the source and target inclusions. This can
now be justified by the following lemma.

\begin{lemma}
  For a dimension \(n \in \mathbb{N}\), \(T : \Tree\), and \(\epsilon
  \in \{-,+\}\) we have:
  \[ \Supp(\incbd n \epsilon T) = \bdry n \epsilon T \]
\end{lemma}
\begin{proof}
  The proof is given by the function
  \func{Catt.Tree.Boundary.Support}{tree-inc-label-supp} in the
  formalisation module \module{Catt.Tree.Boundary.Support} and
  proceeds by induction on \(n\) and \(T\).
\end{proof}

This definition also allows simple inductive proofs that the boundary
inclusions satisfy the globularity conditions, which we state in the
following proposition. These proofs are given in the formalisation
module \module{Catt.Tree.Boundary.Properties}.

\begin{proposition}
  \label{prop:bdry-glob}
  Let \(n \leq m\) and let \(T\) be a tree. Then:
  \[ \bound n {\bound m T} \equiv \bound n T\]
  Further, for \(\epsilon, \omega \in \{-,+\}\) we have:
  \[ \incbd n \epsilon {\bound m T} \bullet \incbd m \omega T \equiv
  \incbd n \epsilon T \]
  If instead \(n \geq \dep(T)\), then \(\bound n T \equiv T\) and
  \(\incbd n \epsilon T \equiv \id_T\).
\end{proposition}

Further, these constructions commute with suspension: The equalities
\(\Sigma(\bound n T) \equiv \bound {n+1} {\Sigma(T)}\) and
\(\Sigma(\incbd n \epsilon T) \equiv \incbd {n+1} \epsilon
{\Sigma(T)}\) hold by definition.

We now recall the definitions of standard type, standard coherence,
and standard term for a tree \(T\), which are given by mutual induction:
\begin{itemize}
  \item The \emph{standard type}, \(\stdty T n\), is an
    \(n\)-dimensional type where each component of the type is given
    by the standard term over the appropriate boundary of the tree
    \(T\), and then included back into \(T\) by applying the
    inclusion labelling.
  \item The \emph{standard coherence}, \(\stdcoh T n\), is the
    canonical dimension \(n\) coherence term over a tree \(T\). It is
    formed by a single coherence constructor over \(T\) with type
    given by the standard type, \(\stdty T n\).
  \item The \emph{standard term}, \(\stdtm T n\), is a variation on
    the standard coherence which does not introduce unnecessary unary
    composites. If \(T\) is linear (and so represents a disc
    context), and \(n = \dep(T)\), then \(\stdtm T n\) is simply
    given by the unique maximal path in \(T\). Otherwise, it is given
    by the standard coherence \(\stdcoh T n\).
\end{itemize}

At the end of \cref{sec:background} it was stated that
\(\Sigma(\stdtm T n) \equiv \stdtm {\Sigma(T)} {n + 1}\). Using this,
the standard term can instead be defined by letting \(\stdtm \emp 0\)
be \(\SPath([0])\), \(\stdtm {\Sigma(T)} {n+1}\) be \(\Sigma(\stdtm T
n)\), and \(\stdtm T n\) be \(\stdcoh T n\) otherwise, which avoids
the case split on the linearity of \(T\). We now define all three
constructions formally using structured syntax.

\begin{definition}
  We define the \(n\)-dimensional \emph{standard type} over a tree
  \(T\) as a structured type \(\stdty T n : \SType_T\), and the
  \(n\)-dimensional \emph{standard coherence} and \emph{standard
  term} over a tree \(T\) as structured terms \(\stdcoh T n, \stdtm T
  n : \STerm_T\) by mutual induction:
  \begin{align*}
    \stdty T 0 &= \star\\
    \stdty T {n + 1} &= \arr {\stdtm {\bound n T} n \sub {\incbd {n +
    1} - T}} {\stdty T n} {\stdtm {\bound n T} n \sub {\incbd {n+1} +
    T}}\\[10pt]
    \stdcoh T n &= \SCoh T {\stdty T n} {\id_T}\\[10pt]
    \stdtm T n &=
    \begin{cases*}
      \SPath([0])&if \(T = \emp\) and \(n = 0\)\\
      \Inc_0(\stdtm {T_0} {n-1})&if \(n \neq 0\) and \(T = [T_0]\)\\
      \stdcoh T n&\text{otherwise}
    \end{cases*}
  \end{align*}
  when \(n = \dep(T)\), we call the standard coherence \(\stdcoh T
  n\) the \emph{standard composite} of \(T\).
\end{definition}

We can immediately show that these standard construct commute with suspension.

\begin{lemma}
  \label{lem:std-susp}
  For tree \(T\) and \(n \in \mathbb{N}\), \(\Sigma(\stdty T n)
  \equiv \stdty {\Sigma(T)} {n+1}\) and \(\Sigma(\stdcoh T n) \equiv
  \stdcoh {\Sigma(T)} {n+1}\).
\end{lemma}
\begin{proof}
  We first consider the standard type. The case for \(n = 0\) follows
  immediately, so we let \(n > 0\). We then get for \(\epsilon \in \{-,+\}\):
  \begin{align*}
    \Sigma\left(\stdtm {\bound {n-1} T} {n-1} \sub {\incbd {n-1}
    \epsilon T}\right) &\equiv \Sigma(\stdtm {\bound {n-1} T} {n-1})
    \sub {\Sigma(\incbd{n-1} \epsilon T)}&\text{by functoriality of
    suspension}\\
    &\equiv \stdtm {\Sigma(\bound {n-1} T)} {n} \sub
    {\Sigma(\incbd{n-1} \epsilon T)}\\
    &\equiv \stdtm {\bound n {\Sigma(T)}} n \sub {\incbd n \epsilon {\Sigma(T)}}
  \end{align*}
  By inductive hypothesis \(\Sigma(\stdty T {n-1}) \equiv \stdty
  {\Sigma(T)} n\) and so
  \begin{align*}
    \Sigma(\stdty T n) &\equiv \arr {\Sigma\left(\stdtm {\bound {n-1}
    T} {n-1} \sub {\incbd {n-1} - T}\right)} {\Sigma(\stdty T {n-1})}
    {\Sigma\left(\stdtm {\bound {n-1} T} {n-1} \sub {\incbd {n-1} + T}\right)}\\
    &\equiv \arr {\stdtm {\bound n {\Sigma(T)}} {n} \sub {\incbd n -
    {\Sigma(T)}}} {\stdty {\Sigma(T)} {n}} {\stdtm {\bound n
    {\Sigma(T)}} {n} \sub {\incbd n + {\Sigma(T)}}}\\
    &\equiv \stdty {\Sigma(T)} {n + 1}
  \end{align*}
  as required.

  For the standard coherence we have:
  \[ \Sigma(\stdcoh T n) \equiv \SCoh {\Sigma(T)} {\Sigma(\stdty T
    n)} {\Sigma(\id_T)} \equiv \SCoh {\Sigma(T)} {\stdty {\Sigma(T)}
  {n+1}} {\id_{\Sigma(T)}} \equiv \stdcoh {\Sigma(T)} {n+1}\]
  following from the case for types.
\end{proof}

To prove that the standard constructions are well-formed, we give a
couple of lemmas. The first concerns the support of the standard term
and standard coherence.

\begin{lemma}
  \label{lem:std-supp}
  For a tree \(T\), dimension \(n \in \mathbb{N}\), and \(\epsilon
  \in \{-,+\}\), we have:
  \[ \Supp\left(\stdtm {\bound n T} n \sub {\incbd n \epsilon
    T}\right) = \bdry n \epsilon T \qquad \Supp\left(\stdcoh {\bound n
  T} n \sub {\incbd n \epsilon T}\right) = \bdry n \epsilon T\]
\end{lemma}
\begin{proof}
  The case for coherences follows from the definition and the equality
  \[\Supp(\incbd n \epsilon T) = \bdry n \epsilon T\]
  For the standard term, it suffices to consider cases where the
  standard term and standard coherence are not equal. If \(n = 0\),
  then \(\bound n T \equiv \emp\), and it suffices to prove that
  \(\Supp([m]) = \FV(\lfloor [m] \rfloor)\), but this is immediate
  because \(\Supp([m]) = \Supp(\lfloor [m] \rfloor)\) and \(\lfloor
  [m] \rfloor\) is a variable of type \(\star\) so its support is
  equal to its free variables.

  We therefore consider the case where \(n > 0\) and \(\len(\bound n
  T) = 1\). The only case where this happens is if \(\len(T) = 1\)
  too, so assume \(T \equiv [T_0]\)
  \begin{align*}
    \Supp\left(\stdtm {\bound n T} n \sub {\incbd n \epsilon T}
    \right) &= \Supp\left(\stdtm {\Sigma(\bound {n-1} {T_0})} n \sub
    {\Sigma\left(\incbd {n-1} \epsilon {T_0}\right)}  \right)\\
    &= \Supp\left(\Sigma\left( \stdtm {\bound {n-1} {T_0}} {n - 1}
    \right) \sub {\Sigma\left(\incbd {n-1} \epsilon {T_0}\right)}  \right)\\
    &= \Supp\left(\Sigma\left( \stdtm {\bound {n-1} {T_0}} {n - 1}
    \sub {\incbd {n-1} \epsilon {T_0}} \right) \right)\\
    &= \Sigma\left(\Supp\left( \stdtm {\bound {n-1} {T_0}} {n - 1}
    \sub {\incbd {n-1} \epsilon {T_0}} \right) \right)\\
    &= \Sigma\left( \bdry {n-1} \epsilon {T_0} \right)\\
    &= \bdry n \epsilon T
  \end{align*}
  as required.
\end{proof}

The second lemma gives a globularity condition for the standard type.

\begin{lemma}
  \label{lem:std-type-glob}
  Let \(T\) be a tree. Then:
  \[ \stdty T n \equiv \stdty {\bound m T} n \sub {\incbd m \epsilon T}\]
  for \(n \leq m\) and \(\epsilon \in \{-,+\}\).
\end{lemma}
\begin{proof}
  We induct on \(n\). If \(n = 0\) then both sides of the equation
  are the type \(\star\). We therefore consider the case for \(n +
  1\) and so we must prove:
  \begin{align*}
    \stdty T {n+1} &\equiv \arr {\stdtm {\bound n T} n \sub {\incbd n
    - T}} {\stdty T k} {\stdtm {\bound n T} n \sub {\incbd n + T}}\\
    &\equiv  \arr {\stdtm {\bound n {\bound m T}} n \sub {\incbd n -
    {\bound m T}} \sub {\incbd m \epsilon T}} {\stdty {\bound m T} n
    \sub {\incbd m \epsilon T}} {\stdtm {\bound n {\bound m T}} n
    \sub {\incbd n + {\bound m T}} \sub {\incbd m \epsilon T}}\\
    &\equiv \stdty {\bound m T} {n+1} \sub {\incbd m \epsilon T}
  \end{align*}
  The equality \({\stdty T n} \equiv {\stdty {\bound m T} n \sub
  {\incbd m \epsilon T}}\) follows by inductive hypothesis. Further,
  for \(\omega \in \{-,+\}\) we have by \cref{prop:bdry-glob}:
  \begin{align*}
    \stdtm {\bound n {\bound m T}} n \sub {\incbd n \omega {\bound m
    T}} \sub {\incbd n \epsilon T} &\equiv \stdtm {\bound n {\bound m
    T}} n \sub {\incbd n \omega {\bound m T} \bullet \incbd m \epsilon T} \\
    &\equiv \stdtm {\bound n T} n \sub {\incbd n - T}
  \end{align*}
  which completes the proof.
\end{proof}

We can now state and prove the typing properties of standard constructions.

\begin{proposition}
  \label{prop:standard-typing}
  Suppose that \(\mathcal{O}\) contains the standard operations. Then
  the following rules are admissible:
  \begin{mathpar}
    \inferrule{T : \Tree\\ n \in \mathbb{N}}{T \vdash \stdty T n}\and
    \inferrule{T : \Tree \\ n \neq 0\\ n \geq \dep(T)}{T \vdash
    \stdcoh T n : \stdty T n}\and
    \inferrule{T : \Tree \\ n \geq \dep(T)}{T \vdash \stdtm T n : \stdty T n}
  \end{mathpar}
\end{proposition}
\begin{proof}
  We prove that all three rules are admissible by mutual induction.
  First consider the cases for types. The case when \(n = 0\) is
  trivial, so we consider the case for \(n + 1\). We need to show that:
  \[ T \vdash \arr {\stdtm {\bound n T} n \sub {\incbd n - T}}
  {\stdty n T} {\stdtm {\bound n T} n \sub {\incbd n + T}}\]
  The inductive hypothesis on types gives that \(T \vdash \stdty n
  T\) and so we must show that:
  \[ T \vdash {\stdtm {\bound n T} n \sub {\incbd n \epsilon T}} : \stdty n T\]
  for \(\epsilon \in \{-,+\}\). By inductive hypothesis for terms, we
  have \(\bound n T \vdash \stdtm {\bound n T} n : \stdty {\bound n
  T} n\) as we have \(\dep(\bound n T) \leq n\). As \(T \vdash \incbd
  n \epsilon T : \bound n T\) we have that:
  \[ T \vdash {\stdtm {\bound n T} n \sub {\incbd n \epsilon T}} :
  \stdty {\bound n T} n \sub {\incbd n \epsilon T} \]
  and so by \cref{lem:std-type-glob}, this case is complete.

  For the standard coherence, we apply \cref{rule:scoh}, using the
  inductive hypothesis for types. To show that \((T, \src(\stdty T
  n), \tgt(\stdty T n)) \in \mathcal{O}\), we apply \cref{lem:std-supp}.

  For the standard term, like previous proofs it is sufficient to
  consider the cases where it is defined differently to the standard
  coherence. For \(n = 0\) we must have \(T = \emp\) by the condition
  on the depth of \(T\). Hence, \(\stdtm T n \equiv [0]\) which is
  well-formed as has type \(\star \equiv \stdty T n\) as required.

  We now consider \(\stdtm {\Sigma(T)} {n+1} \equiv \Sigma (\stdtm T
  n)\). By inductive hypothesis on dimension, \(T \vdash \stdtm T n :
  \stdty T n\) and so we immediately have that:
  \[ \Sigma(T) \vdash \stdtm {\Sigma(T)} {n + 1} : \Sigma(\stdty T n)\]
  and so the proof is complete by \cref{lem:std-supp}.
\end{proof}

The equality relations we have seen so far make heavy use of disc
contexts and associated terms and types. We therefore pause to
consider the form of these as structured syntax and to relate them to
the standard constructions presented in this section.

All disc contexts are the result of applying iterated suspensions to
the singleton context, and so it follows that disc contexts
correspond exactly to linear trees. By an abuse of notation we write:
\[ D^n = \Sigma^n(\emp)\]
As we further have that \(\Sigma(U^n) \equiv U^{n+1}\) for the sphere
type \(U^n\), it can be proved for a simple induction that:
\[U^n \equiv \lfloor \stdty {D^n} n \rfloor\]
As we have already noted, the maximal dimension term \(d_n :
\Term_{D^n}\) is given by \(\lfloor \stdtm {D^n} n \rfloor\). It is
also equal to the unique maximal path, \(p^n = \Sigma^n[0]\), which
is the list containing \(n+1\) zeros.

The only missing construction is an equivalent for the substitution
from a disc context. From a structured term \(s : \STerm_\U\) of type
\(A : \SType_\U\), there should be a labelling \(\{A,s\}\) from
\(D^n\) to \(\U\). This however proves more challenging to define as
trees and types have opposite inductive structure. For a labelling,
it is natural to specify the lower-dimensional terms first and fill
in higher-dimensional terms by induction, though when deconstructing
a type, we first receive the highest dimensional terms, only
receiving the lower-dimensional terms by further deconstructing the type.

To define the labelling \(\{A,t\}\), we define the extension of
labelling from a linear tree, which allows us to add
higher-dimensional terms to the labelling, and use this to define the
labelling from a linear tree.

\begin{definition}
  Let \(L : D^n \to \U\) be a labelling from a linear tree, and let
  \(s,t : \STerm_\U\) be structured terms. The \emph{extension} of
  \(L\) by \(s\) and \(t\), \(\ext(L,s,t)\), is defined inductively on \(n\) by:
  \begin{equation*}
    \ty(\ext(L,s,t)) = \ty(L) \qquad \ext(L,s,t) =
    \begin{cases*}
      L[0]\,\{t\}\,s &if \(n = 0\)\\
      L[0]\,\{\ext(L_0,s,t)\}\,L[1]&otherwise
    \end{cases*}
  \end{equation*}
  We then define the labelling \(\{A,t\}\) by induction on \(A\):
  \[ \{\star,t\} = (p \mapsto t) \qquad \{\arr s A t, u\} =
  \ext(\{A,s\},t,u) \qquad \ty(\{A,t\}) = \star\]
\end{definition}

These constructions all satisfy the expected typing judgements. More
precisely the following inference rules are admissible:
\begin{mathpar}
  \inferrule{\U \vdash L : D^n\\ \U \vdash s : \stdty {D^n} n \sub
  L\\ \U \vdash t : \arr {p^n \sub L} {\stdty {D^n} n \sub L} {s}}{\U
  \vdash \ext(L,s,t) : D^{n+1}}\\
  \inferrule{\U \vdash A \\ \U \vdash t : A}{\U \vdash \{A,t\} : D^{\dim(A)}}
\end{mathpar}
The admissibility of the above rules is routine to verify.

Using these constructions, we can recover structured term definitions
of the unary composite of a (structured) term \(t\) of type \(A\) of
dimension \(n\) as \( \stdcoh {D^n} n \sub {\{A,t\}}\) and can define
the identity of the same term \(t\) as \(\stdcoh {D^n} {n+1} \sub
{\{A,t\}}\). Therefore, the rules for disc removal and endo-coherence
removal can be rephrased in terms of structured syntax to get the
following rules:
\begin{mathpar}
  \inferrule{\U : \Ctx \uplus \Tree\\ \U \vdash A \\ \U \vdash t : A
  \\ \dim(A) = n > 0}{ \U \vdash \stdcoh {D^n} n \sub {\{A,t\}} =
  t}\textsc{dr'}\\
  \inferrule{\U : \Ctx \uplus \Tree\\ T : \Tree \\ L : \arr S \star
    \U\\ n = \dim(A)\\\\ T \vdash A \\ T \vdash s : A \\ \Supp(s) =
  \Var(T) \\ \U \vdash L : T}{\U \vdash \SCoh T {\arr s A s} L =
  \stdcoh {D^n} {n+1} \sub { \{A, s\} \bullet L}}\textsc{ecr'}
\end{mathpar}
which are admissible if the equality rule set \(\mathcal{R}\) has
disc removal or endo-coherence removal respectively.

We end this section with two further results that can be proven in
the presence of disc removal and endo-coherence removal. The first
states that disc removal is sufficient (and necessary) to unify
standard coherences and standard terms.

\begin{theorem}
  \label{thm:std-dr}
  The tame equality rule set \(\mathcal{R}\) has disc removal if and
  only if the rule:
  \begin{mathpar}
    \inferrule{T : \Tree \\ n \in \mathbb{N}\\ n \geq \dep(T) > 0}{T
    \vdash \stdcoh T n = \stdtm T n}
  \end{mathpar}
  is admissible.
\end{theorem}
\begin{proof}
  We note that \(\stdcoh T n\) and \(\stdtm T n\) only differ when
  \(T = D^n\). If \(\mathcal{R}\) has disc removal, then for each \(n
  \neq 0\) we have \(\stdcoh {D^n} n = \SPath(p^n) \equiv \stdtm
  {D^n} n\). Conversely, if \(\stdcoh T n = \stdtm T n\) when \(n >
  0\) or \(\dep(T) > 0\), then \(\stdcoh {D^n} n = \stdtm {D^n} n\)
  for any \(n > 0\). Then as \(\mathcal{R}\) is tame, we can apply
  the substitution \(\{A,t\}\) to both sides of the equation to get
  the statement of disc removal.
\end{proof}

Lastly, under the presence of endo-coherence removal, the standard
coherences \(\stdtm T n\) for which \(n > \dep(T)\) can be shown to
be equal to identities.

\begin{theorem}
  \label{thm:std-ecr}
  Suppose the equality rule set \(\mathcal{R}\) has endo-coherence
  removal. Let \(T\) be a tree and suppose \(n \geq \dep(T)\). Then:
  \[ T \vdash \stdcoh T {n+1} = \stdcoh {D^n} {n+1} \sub {\{\stdty T
  n, \stdtm T n\}} \]
\end{theorem}
\begin{proof}
  The following chain of equalities hold:
  \begin{align*}
    \stdcoh T {n+1} &\equiv \SCoh T {\arr {\stdtm {\bound n T} n \sub
      {\incbd n T -}} {\stdty T n} {\stdtm {\bound n T} n \sub {\incbd
    n T +}}} {\id_S}\\
    &\equiv \SCoh T {\arr {\stdtm T n} {\stdty T n} {\stdtm T n}}
    {\id_S}&\text{by \cref{prop:bdry-glob}}\\
    &= \stdcoh {D^n} {n+1} \sub {\{\stdty T n, \stdtm T n\}}&\text{by
    \textsc{ecr'}}
  \end{align*}
  where \textsc{ecr'} can be applied as \(\Supp(\stdtm T n) =
  \Var(\lfloor T \rfloor)\) by \cref{lem:std-supp}.
\end{proof}

Due to these two theorems, every standard term \(\stdtm T n\) with
\(n \geq \dep(T)\) is equal to either the unique variable of the
singleton context (when \(n = \dep(T) = 0\)), a standard composite
(when \(n = \dep(T) > 0\)) or an identity (when \(n > \dep(T)\)),
hence completely classifying the well-formed standard terms.

\section{Insertion}
\label{sec:insertion}

We now introduce \emph{insertion}, the construction that powers the
strictly associative behaviour of \Cattsua. Insertion incorporates
part of the structure of a locally maximal argument term into the
head coherence, simplifying the overall syntax of the term.

Consider the composite \(f * (g * h)\). This term has two locally
maximal arguments, \(f\) and \(g * h\), the second of which is a
(standard) coherence. Insertion allows us to merge these two
composites into one by ``inserting'' the pasting diagram of the inner
coherence into the pasting diagram of the outer coherence. In the
case above we will get that the term \(f * (g * h)\) is equal to the
ternary composite \(f * g * h\), a term with a single coherence. As
the term \((f * g) * h\) also reduces by insertion to the ternary
composite, we see that both sides of the associator become equal
under insertion. The action of insertion on these contexts is shown
in \cref{fig:insertion}.

\begin{figure}
  $$
  \begin{aligned}
    \begin{tikzpicture}
      \node (x) at (0,0)  {$x$};
      \node (y) at (1.5,0) {$y$};
      \node (z) at (3,0) {$z$};
      \draw [->] (x) to node [above, font=\small] {$f$} (y);
      \draw [->] (y) to node [above, font=\small] {$g*h$} (z);
      \begin{scope}[xshift=1.25cm, yshift=1.75cm, red]
        %\draw [fill=red!10, draw=none] (1,0.05) ellipse (1.2cm and .6cm);
        \draw [rounded corners, fill=red!7, draw=none] (-.25,-.35)
        rectangle +(2.5,1);
        \node (x2) at (0,0)  {$x'$};
        \node (y2) at (1,0) {$y'$};
        \node (z2) at (2,0) {$z'$};
        \draw [->] (x2) to node [above, font=\small] {$g$} (y2);
        \draw [->] (y2) to node [above, font=\small] {$h$} (z2);
      \end{scope}
      \draw [->, very thick, red] (2.25,1.25) to +(0,-.5);
    \end{tikzpicture}
  \end{aligned}
  \quad\leadsto\quad
  \begin{aligned}
    \begin{tikzpicture}
      \node (x) at (0,0)  {$x \vphantom'$};
      \node [red] (y) at (1,0) {$x'$};
      \node [red] (z) at (2,0) {$y'$};
      \node [red] (w) at (3,0) {$z'$};
      \begin{scope}[xshift=.5cm, yshift=1.5cm, red]
        \draw [rounded corners, fill=white, draw=none] (-.25,-.35)
        rectangle +(2.5,1);
      \end{scope}
      \draw [->] (x) to node [above, font=\small] {$f$} (y);
      \draw [->, red] (y) to node [above, font=\small] {$g$} (z);
      \draw [->, red] (z) to node [above, font=\small] {$h$} (w);
    \end{tikzpicture}
  \end{aligned}
  $$
  \caption{Insertion acting on the composite \(f * (g * h)\).}
  \label{fig:insertion}
\end{figure}

Insertion is an operation that is best understood with respect to
trees instead of ps-contexts. Insertion merges the structure of two
trees along a \emph{branch} of the first tree.

\begin{definition}
  Let \(S\) be a tree. A \emph{branch} of \(S\) is a non-empty list
  of natural numbers \(P\) which indexes a subtree \(S^P\) which is
  linear. From each branch \(P\), a maximal path \(\olsi P\) can be
  obtained by concatenating \(P\) with \(p^{\dep(S^P)}\), the unique
  maximal path of \(S^P\).

  For a branch \(P\), we further define the \emph{branch height},
  \(\bh(P)\), to be one less than the length of \(P\) (noting that
  branches are non-empty lists), and the \emph{leaf height},
  \(\lh(P)\), to be one less than the length of \(\olsi P\), which is
  equal to the dimension of \(\lfloor \hat P \rfloor\).
\end{definition}
While each branch \(P\) uniquely determines a maximal path \(\olsi
P\), the converse does not hold. There may be multiple branches of a
tree which correspond to the same maximal path. Consider the tree \(T
= [[[[\emp],\emp],\emp]]\). This has two distinct branches \(P =
[0,0,0]\) and \(Q = [0,0,0,0]\) which both correspond to the maximal
path \([0,0,0,0,0]\). We graphically depict these branches below by
drawing them in blue.

\[ P =
  \begin{tikzpicture}[yscale=0.7,every node/.append
    style={scale=0.6},baseline=(x21.base)]
    \node [on grid] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (0,1) (x11) {$\bullet$};
    \node [on grid] at (-0.5,2)(x21){$\bullet$};
    \node [on grid] at (0.5,2) (x22){$\bullet$};
    \node [on grid, Diag1] at (-0.9,3)(x31) {$\bullet$};
    \node [on grid] at (-0.1,3) (x32) {$\bullet$};
    \node [on grid, Diag1] at (-0.9,4)(x41) {$\bullet$};
    \begin{scope}[on background layer]
      \draw (x01.center) to (x11.center);
      \draw (x11.center) to (x21.center);
      \draw (x11.center) to (x22.center);
      \draw[Diag1,very thick] (x21.center) to (x31.center);
      \draw (x21.center) to (x32.center);
      \draw[Diag1,very thick] (x31.center) to (x41.center);
    \end{scope}
  \end{tikzpicture}
  \qquad
  Q =
  \begin{tikzpicture}[yscale=0.7,every node/.append
    style={scale=0.6},baseline=(x21.base)]
    \node [on grid] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (0,1) (x11) {$\bullet$};
    \node [on grid] at (-0.5,2)(x21){$\bullet$};
    \node [on grid] at (0.5,2) (x22){$\bullet$};
    \node [on grid] at (-0.9,3)(x31) {$\bullet$};
    \node [on grid] at (-0.1,3) (x32) {$\bullet$};
    \node [on grid, Diag1] at (-0.9,4)(x41) {$\bullet$};
    \begin{scope}[on background layer]
      \draw (x01.center) to (x11.center);
      \draw (x11.center) to (x21.center);
      \draw (x11.center) to (x22.center);
      \draw (x21.center) to (x31.center);
      \draw (x21.center) to (x32.center);
      \draw[Diag1,very thick] (x31.center) to (x41.center);
    \end{scope}
  \end{tikzpicture}
\]
While \(P\) and \(Q\) represent the same path, they have different
branch heights: the branch height of \(P\) is \(2\) while the branch
height of \(Q\) is \(3\). This will cause insertions along these two
branches to proceed differently (though we will see later in
  \cref{lem:insertion-irrel} that if both insertions are valid then the
results are equivalent). The leaf height and branch height of the
branch \(P\) is demonstrated in \cref{fig:leafheight}, where we also
depict the trunk height of \(T\), which was defined in \cref{sec:trees}.

\begin{figure}
  \[
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base)]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (0,1) (x11) {$\bullet$};
      \node [on grid] at (-0.5,2)(x21){$\bullet$};
      \node [on grid] at (0.5,2) (x22){$\bullet$};
      \node [on grid, Diag1] at (-0.9,3)(x31) {$\bullet$};
      \node [on grid] at (-0.1,3) (x32) {$\bullet$};
      \node [on grid, Diag1] at (-0.9,4)(x41) {$\bullet$};
      \node [left=0 of x31.center ,on grid] {$T^P$};
      \node [right=0 of x41.center ,on grid] {$\olsi P$};
      \begin{scope}[on background layer]
        \draw (x01.center) to (x11.center);
        \draw (x11.center) to (x21.center);
        \draw (x11.center) to (x22.center);
        \draw[Diag1,very thick] (x21.center) to (x31.center);
        \draw (x21.center) to (x32.center);
        \draw[Diag1,very thick] (x31.center) to (x41.center);
      \end{scope}
      \node [on grid] at (-0.7,1)(th) {};
      \draw [|->] (-0.7,0) to node [left] {$\th(T)$} (th.center);
      \draw [dotted,very thick] (th) to (x11);
      \node [on grid] at (-1.5,2) (bh) {};
      \draw [|->] (-1.5,0) to node [left] {$\bh(P)$} (bh.center);
      \draw [dotted,very thick] (bh) to (x21);
      \node [on grid] at (-2.3,4) (lh) {};
      \draw [|->] (-2.3,0) to node [left] {$\lh(P)$} (lh.center);
      \draw [dotted,very thick] (lh) to (x41);
    \end{tikzpicture}
  \]
  \caption{\label{fig:leafheight} Leaf height, branch height and trunk height.}
\end{figure}

Let us again consider the tree \(S = [[\emp,\emp],\emp]\) from
\cref{fig:tree-example}. This tree has three branches, corresponding
to the maximal paths \([0,0,0]\), \([0,1,0]\), and \([1,0]\). We
consider the action of insertion of three trees \(T_1,T_2,T_3\),
given below, into branch \(P = [0,0]\), which corresponds to the
first of these maximal paths.
\[T =
  \begin{tikzpicture}[yscale=0.7, every
    node/.style={scale=0.6},baseline=(x11.base)]
    \node [on grid] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (0,1)(x11) {$\bullet$};
    \node [on grid] at (-0.5, 2) (x21) {$\bullet$};
    \node [on grid] at (0.5,2)(x22) {$\bullet$};
    \draw (x01.center) to (x11.center);
    \draw (x11.center) to (x21.center);
    \draw (x11.center) to (x22.center);
  \end{tikzpicture}
  \qquad
  T' =
  \begin{tikzpicture}[yscale=0.7, every
    node/.style={scale=0.6},baseline=(x11.base)]
    \node [on grid] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (0,1)(x11) {$\bullet$};
    \draw (x01.center) to (x11.center);
    \path [draw=none] (-0.5,0) to (0.5,0);
  \end{tikzpicture}
  \qquad
  T'' =
  \begin{tikzpicture}[yscale=0.7, every
    node/.style={scale=0.6},baseline=(x21.base)]
    \node [on grid] at (0,1)(x11) {$\bullet$};
    \node [on grid] at (-0.5, 2) (x21) {$\bullet$};
    \node [on grid] at (0.5,2)(x22) {$\bullet$};
    \draw (x11.center) to (x21.center);
    \draw (x11.center) to (x22.center);
  \end{tikzpicture}
\]
We first consider the insertion of \(T\) into \(S\), which returns
the inserted tree \(\insertion S P {T}\), where \(P\) is drawn in
blue on the diagram.
\[ S =
  \begin{tikzpicture}[yscale=0.7, every
    node/.style={scale=0.6},baseline=(x11.base)]
    \node [on grid] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (-0.5,1) (x11) {$\bullet$};
    \node [on grid, Diag1] at (-0.9, 2)(x21) {$\bullet$};
    \node [on grid] at (-0.1, 2)(x22) {$\bullet$};
    \node [on grid] at (0.5,1)(x12) {$\bullet$};
    \begin{scope}[on background layer]
      \draw (x01.center) to (x11.center);
      \draw (x01.center) to (x12.center);
      \draw[Diag1, very thick] (x11.center) to (x21.center);
      \draw (x11.center) to (x22.center);
    \end{scope}
  \end{tikzpicture}
  \qquad
  T =
  \begin{tikzpicture}[yscale=0.7, every
    node/.style={scale=0.6},baseline=(x11.base), Diag2]
    \node [on grid] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (0,1) (x11) {$\bullet$};
    \node [on grid] at (-0.5, 2) (x21) {$\bullet$};
    \node [on grid] at (0.5,2)(x22) {$\bullet$};
    \draw (x01.center) to (x11.center);
    \draw (x11.center) to (x21.center);
    \draw (x11.center) to (x22.center);
  \end{tikzpicture}
  \qquad
  \insertion S P {T} =
  \begin{tikzpicture}[yscale=0.7, every
    node/.style={scale=0.6},baseline=(x11.base)]
    \node [on grid, Diag2] at (0,0) (x01) {$\bullet$};
    \node [on grid, Diag2] at (-0.5,1) (x11) {$\bullet$};
    \node [on grid, Diag2] at (-0.9, 2)(x21) {$\bullet$};
    \node [on grid, Diag2] at (-0.5, 2)(x22) {$\bullet$};
    \node [on grid] at (-0.1, 2)(x23) {$\bullet$};
    \node [on grid] at (0.5,1)(x12) {$\bullet$};
    \begin{scope}[on background layer]
      \draw [Diag2] (x01.center) to (x11.center);
      \draw (x01.center) to (x12.center);
      \draw [Diag2] (x11.center) to (x21.center);
      \draw [Diag2] (x11.center) to (x22.center);
      \draw (x11.center) to (x23.center);
    \end{scope}
  \end{tikzpicture}
\]

In this case the structure of \(T\) is compatible with the point of
insertion \(P\) and \(T\) can be inserted into \(S\), replacing the
branch \(P\) with the appropriate part of \(T\), where this
appropriate part is obtained by removing the trunk of \(T\).

We now consider the insertion of \(T'\) into \(S\). Despite \(T'\)
having a lower depth than \(S\), it is still insertable, forming the
following tree \(\insertion S P {T'}\).
\[ S =
  \begin{tikzpicture}[yscale=0.7, every
    node/.style={scale=0.6},baseline=(x11.base)]
    \node [on grid] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (-0.5,1) (x11) {$\bullet$};
    \node [on grid, Diag1] at (-0.9, 2)(x21) {$\bullet$};
    \node [on grid] at (-0.1, 2)(x22) {$\bullet$};
    \node [on grid] at (0.5,1)(x12) {$\bullet$};
    \begin{scope}[on background layer]
      \draw (x01.center) to (x11.center);
      \draw (x01.center) to (x12.center);
      \draw[Diag1, very thick] (x11.center) to (x21.center);
      \draw (x11.center) to (x22.center);
    \end{scope}
  \end{tikzpicture}
  \qquad
  T' =
  \begin{tikzpicture}[yscale=0.7, every
    node/.style={scale=0.6},baseline=(x11.base), Diag2]
    \node [on grid] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (0,1)(x11) {$\bullet$};
    \draw (x01.center) to (x11.center);
    \path [draw=none] (-0.5,0) to (0.5,0);
  \end{tikzpicture}
  \qquad
  \insertion S P {T'} =
  \begin{tikzpicture}[yscale=0.7, every
    node/.style={scale=0.6},baseline=(x11.base)]
    \node [on grid,Diag2] at (0,0) (x01) {$\bullet$};
    \node [on grid,Diag2] at (-0.5,1) (x11) {$\bullet$};
    \node [on grid] at (-0.5, 2)(x21) {$\bullet$};
    \node [on grid] at (0.5,1)(x12) {$\bullet$};
    \begin{scope}[on background layer]
      \draw [Diag2] (x01.center) to (x11.center);
      \draw (x01.center) to (x12.center);
      \draw (x11.center) to (x21.center);
    \end{scope}
  \end{tikzpicture}
\]
Here, the branch \(P\) is replaced by a singleton tree, which is the
remaining part of \(T'\) after removing its trunk. We note that this
operation is the same as pruning the locally maximal variable
\(\lfloor \olsi P \rfloor\) from \(\lfloor T \rfloor\). We will see
in \cref{sec:univ-prop-insert} that all instances of pruning can be
represented as an instance of insertion.

When we consider the insertion of \(T''\) into \(S\), it is not clear
how to proceed, as there is no ``corresponding part'' of \(T''\) to
replace the branch \(P\) with. In the other two cases this is
obtained by removing the trunk of the tree, but \(T''\) has no trunk
to remove. In this case we say that the insertion is not possible to
perform as \(\bh(P) > \th(T'')\), a condition necessary for insertion.

More generally we consider a (structured) coherence term \(\SCoh S A
L : \STerm_\U\). To apply insertion to this term, we must first
identify a branch \(P\) of \(S\) such that \(\olsi P \sub L \equiv
\stdcoh T {\lh(P)} \sub M\), that is there is a locally maximal
argument of \(L\) which is a standard coherence. We then must
construct the following data as part of the insertion operation:

\begin{itemize}
  \item The \emph{inserted tree} \(\insertion S P T\), obtained by
    inserting \(T\) into \(S\) along the branch \(P\). We have
    already given some examples of this operation.
  \item The \emph{interior labelling} \(\iota : T \to \insertion S P
    T\), the inclusion of \(T\) into a copy of \(T\) living in the
    inserted tree.
  \item The \emph{exterior labelling} \(\kappa : S \to \insertion S P
    T\), which maps \(\olsi P\) to standard coherence over the copy
    of \(T\), or more specifically \(\mathcal{C}_\Theta^{\lh(P)} \sub
    \iota\), and other maximal paths to their copy in the inserted tree.
  \item The \emph{inserted labelling} \(\insertion L P M : \insertion
    S P T \to \U\), which collects the appropriate parts of \(L\) and \(M\).
\end{itemize}
Using this notation, insertion yields the following equality:
\[\Coh S A L = \Coh {\insertion S P T} {A \sub \kappa} {\insertion L P M}\]
These constructions can be assembled into the following diagram,
where \(n = \lh(P)\):
% https://q.uiver.app/?q=WzAsNSxbMCwwLCJEX24iXSxbMSwwLCJcXERlbHRhIl0sWzAsMSwiXFxUaGV0YSJdLFsxLDEsIlxcaW5zZXJ0aW9uIFxcRGVsdGEgeCBcXFRoZXRhIl0sWzIsMiwiXFxHYW1tYSJdLFsxLDMsIlxca2FwcGEiXSxbMiwzLCJcXGlvdGEiLDJdLFswLDEsIlxce0EseFxcfSJdLFswLDIsIlxce1xcbWF0aGNhbHtVfV9cXFRoZXRhXm4sIFxcbWF0aGNhbHtDfV9cXFRoZXRhXm5cXH0iLDJdLFsxLDQsIlxcc2lnbWEiLDAseyJjdXJ2ZSI6LTN9XSxbMiw0LCJcXHRhdSIsMix7ImN1cnZlIjoyfV0sWzMsNCwiXFxpbnNlcnRpb24gXFxzaWdtYSB4IFxcdGF1IiwxXSxbMywwLCIiLDEseyJzdHlsZSI6eyJuYW1lIjoiY29ybmVyIn19XV0= % tex-fmt: skip
\[
  \begin{tikzcd}
    {D^n} & S \\
    T & {\insertion S P T} \\
    && \U
    \arrow["\kappa", from=1-2, to=2-2]
    \arrow["\iota"', from=2-1, to=2-2]
    \arrow["{\{\ty(\olsi P), \olsi P\}}", from=1-1, to=1-2]
    \arrow["{\{\stdty T n, \stdcoh T n\}}"', from=1-1, to=2-1]
    \arrow["L", curve={height=-18pt}, from=1-2, to=3-3]
    \arrow["M"', curve={height=12pt}, from=2-1, to=3-3]
    \arrow["{\insertion L P M}"{description}, from=2-2, to=3-3]
    % \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    % scale=1.5}, draw=none, from=2-2, to=1-1]
  \end{tikzcd}
\]
It will be proven in \cref{sec:univ-prop-insert} that the square
above is cocartesian, and so \(\insertion S P T\) is the pushout of
\(S\) and \(T\).

We now begin to define each of these constructions in turn. As we
need a lot of data to perform an insertion, we will package it up to
avoid repetition.

\begin{definition}
  An \emph{insertion point} is a triple \((S,P,T)\) such that \(S\)
  and \(T\) are trees and \(P\) is a branch of \(S\) with \(\bh(P)
  \leq \th(T)\) and \(\lh(S) \geq \dim(T)\).

  An \emph{insertion redex} is a sextuple \((S,P,T,\U,L,M)\) such
  that \((S,P,T)\) is an insertion point, \(L : S \to \U\) and \(M :
  T \to \U\) are labellings with \(\ty(L) \equiv \ty(M) \equiv
  \star\), and \(L(\olsi P) \equiv \mathcal{C}_T^{\lh(P)}\sub M\).
\end{definition}

We can now define the insertion operation on trees.

\begin{definition}[Inserted tree]
  Let \((S,P,T)\) be an insertion point. Define the \emph{inserted
  tree} \(\insertion S P T\) by induction on the branch \(P\), noting
  that \(P\) is always non-empty.
  \begin{itemize}
    \item Suppose \(P = [k]\) and \(S = [S_0,\dots,S_k,\dots,S_n]\). Then:
      \[\insertion S P T = [S_0,\dots,S_{k-1}] \doubleplus T
      \doubleplus [S_{k+1},\dots,S_n]\]
    \item Suppose \(P = k :: Q\) and again \(S =
      [S_0,\dots,S_k,\dots,S_n]\). We note that \(Q\) is a branch of
      \(S_k\) and by the condition on trunk height of \(T\) we have
      \(T = \Sigma(T_0)\). Then:
      \[\insertion S P T = [S_0,\dots,S_{k-1},(\insertion {S_k} {Q}
      {T_0}),S_{k+1},\dots,S_n ] \]
  \end{itemize}
  We draw attention to the condition of the trunk height of \(T\)
  being at least the branch height of \(P\), which is necessary for
  the induction to proceed. We recall that a tree is identified with
  a list of trees, and that in the first case of insertion \(T\) is
  treated as a list, and in the second case \(\insertion {S_k} {Q}
  {T_0}\) is treated as a single tree which forms one of the subtrees
  of \(\insertion S P T\).
\end{definition}

We now proceed to define the interior and exterior labellings, which
will be done using the diagrammatic notation introduced in
\cref{sec:wedge-sums}.

\begin{definition}[Interior labelling]
  Given an insertion point \((S, P, T)\) we define the interior
  labelling \(\iota_{S,P,T} : T \to \insertion S P T\) by induction on~\(P\).

  \begin{itemize}
    \item When \(P = [k]\) and \(S = [S_0,\dots,S_k,\dots,S_n]\) we
      define \(\iota\) by \(\ty(\iota) = \star\) and:
    % https://q.uiver.app/?q=WzAsNixbMCwwLCJTXzBcXHZlZVxcZG90c1xcdmVlIFNfe2stMX0iXSxbNCwwLCJTX3trKzF9IFxcdmVlIFxcZG90cyBcXHZlZSBTX24iXSxbMiwwLCJUIl0sWzMsMCwiXFx2ZWUiXSxbMSwwLCJcXHZlZSJdLFsyLDIsIlQiXSxbNSwyLCJcXGlkIl1d % tex-fmt: skip
      \[
        \begin{tikzcd}[column sep=smaller,row sep=10pt]
          {[S_0,\dots,S_{k-1}]} & \doubleplus & T & \doubleplus &
          {[S_{k+1},\dots,S_n]} \\
          \\
          && T
          \arrow["\id"{font = \normalsize}, from=3-3, to=1-3]
        \end{tikzcd}
      \]

    \item When \(P = k :: Q\), \(S = [S_0,\dots,S_k,\dots,S_n]\), and
      \(T = [T_0]\) (by the trunk height condition) we define
      \(\iota\) by \(\ty(\iota) = \star\) and:
    % https://q.uiver.app/?q=WzAsNixbMCwwLCJcXGxmbG9vciBbU18xLFxcZG90cyxTX3trLTF9XSBcXHJmbG9vciJdLFs0LDAsIlxcbGZsb29yIFtTX3trKzF9LFxcZG90cyxTX25dIFxccmZsb29yIl0sWzIsMCwiXFxTaWdtYSBcXGxmbG9vciBcXGluc2VydGlvbiB7U19rfSB7UCd9IHtUXzF9IFxccmZsb29yIl0sWzMsMCwiXFx2ZWUiXSxbMSwwLCJcXHZlZSJdLFsyLDIsIlxcU2lnbWEgXFxsZmxvb3IgVF8xIFxccmZsb29yIl0sWzUsMiwiXFxTaWdtYSBcXGlvdGFfe1NfayxQJyxUXzF9Il1d % tex-fmt: skip
      \[
        \begin{tikzcd}[column sep=smaller, row sep=10pt]
          {[S_0,\dots,S_{k-1}]} & \vee & {\Sigma \insertion {S_k} {Q}
          {T_0}} & \vee & {[S_{k+1},\dots,S_n]} \\
          \\
          && {\Sigma T_0}
          \arrow["{\Sigma \iota_{S_k,Q,T_0}}"{font = \normalsize},
          from=3-3, to=1-3]
        \end{tikzcd}
      \]
  \end{itemize}
  We may drop the subscripts on \(\iota\) when they are easily inferred.
\end{definition}

\begin{definition}[Exterior labelling]
  Given an insertion point \((S, P, T)\), we define the exterior
  labelling \(\kappa_{S,P,T} : S \to \insertion S P T\) by induction on \(P\).

  \begin{itemize}
    \item When \(P = [k]\) and \(S = [S_0,\dots,S_k,\dots,S_n]\) we
      define \(\kappa\) by \(\ty(\kappa) = \star\) and:
    % https://q.uiver.app/?q=WzAsMTAsWzAsMCwiXFxsZmxvb3IgW1NfMSxcXGRvdHMsU197ay0xfV0gXFxyZmxvb3IiXSxbNCwwLCJcXGxmbG9vciBbU197aysxfSxcXGRvdHMsU19uXSBcXHJmbG9vciJdLFsyLDAsIlxcbGZsb29yIFQgXFxyZmxvb3IiXSxbMywwLCJcXHZlZSJdLFsxLDAsIlxcdmVlIl0sWzIsMiwiXFxTaWdtYSBcXGxmbG9vciBTX2sgXFxyZmxvb3IiXSxbMCwyLCJcXGxmbG9vciBbU18xLFxcZG90cyxTX3trLTF9XFxyZmxvb3IiXSxbMSwyLCJcXHZlZSJdLFszLDIsIlxcdmVlIl0sWzQsMiwiXFxsZmxvb3IgW1Nfe2srMX0sXFxkb3RzLFNfbl0gXFxyZmxvb3IiXSxbNSwyLCJcXHtcXG1hdGhjYWx7VX1fVF5uLCBcXG1hdGhjYWx7Q31fVF5uXFx9Il0sWzYsMCwiXFxpZCJdLFs5LDEsIlxcaWQiXV0= % tex-fmt: skip
      \[
        \begin{tikzcd}[column sep=smaller,row sep = 10pt]
          {[S_0,\dots,S_{k-1}]} & \doubleplus & {T} & \doubleplus &
          {[S_{k+1},\dots,S_n]} \\
          \\
          {[S_0,\dots,S_{k-1}]} & \vee & {\Sigma S_k} & \vee &
          {[S_{k+1},\dots,S_n]}
          \arrow["{\{\stdty T m, \stdcoh T m\}}"{font = \normalsize,
          pos=.4}, from=3-3, to=1-3]
          \arrow["\id"{font = \normalsize}, from=3-1, to=1-1]
          \arrow["\id"{font = \normalsize}, from=3-5, to=1-5]
      \end{tikzcd}\]
      Where we note that by the condition of \(P\) being a branch we
      have that \(S_k\) is linear and so \(\Sigma \lfloor S_k
      \rfloor\) is a some disc \(D^m\) where \(m = \dep(S_k) + 1\).

    \item When \(P = k :: Q\), \(S = [S_0,\dots,S_k,\dots,S_n]\), and
      \(T = [T_0]\) (by the trunk height condition) we define
      \(\kappa\) by \(\ty(\kappa) = \star\) and:
    % https://q.uiver.app/?q=WzAsMTAsWzAsMCwiXFxsZmxvb3IgW1NfMSxcXGRvdHMsU197ay0xfV0gXFxyZmxvb3IiXSxbNCwwLCJcXGxmbG9vciBbU197aysxfSxcXGRvdHMsU19uXSBcXHJmbG9vciJdLFsyLDAsIlxcU2lnbWEgXFxsZmxvb3IgXFxpbnNlcnRpb24ge1Nfa30ge1AnfSB7VF8xfSBcXHJmbG9vciJdLFszLDAsIlxcdmVlIl0sWzEsMCwiXFx2ZWUiXSxbMiwyLCJcXFNpZ21hIFxcbGZsb29yIFNfayBcXHJmbG9vciJdLFswLDIsIlxcbGZsb29yIFtTXzEsXFxkb3RzLFNfe2stMX1cXHJmbG9vciJdLFsxLDIsIlxcdmVlIl0sWzMsMiwiXFx2ZWUiXSxbNCwyLCJcXGxmbG9vciBbU197aysxfSxcXGRvdHMsU19uXSBcXHJmbG9vciJdLFs1LDIsIlxcU2lnbWEgXFxrYXBwYV97U19rLFAnLFRfMX0iXSxbNiwwLCJcXGlkIl0sWzksMSwiXFxpZCJdXQ== % tex-fmt: skip
      \[
        \begin{tikzcd}[column sep=smaller, row sep = 10pt]
          {[S_0,\dots,S_{k-1}]} & \vee & {\Sigma \insertion {S_k} {Q}
          {T_0}} & \vee & {[S_{k+1},\dots,S_n]} \\
          \\
          {[S_0,\dots,S_{k-1}]} & \vee & {\Sigma S_k} & \vee &
          {[S_{k+1},\dots,S_n]}
          \arrow["{\Sigma \kappa_{S_k,Q,T_0}}"{font=\normalsize},
          from=3-3, to=1-3]
          \arrow["\id"{font=\normalsize}, from=3-1, to=1-1]
          \arrow["\id"{font=\normalsize}, from=3-5, to=1-5]
      \end{tikzcd}\]
  \end{itemize}
  Again the subscripts on \(\kappa\) may be dropped where they can be inferred.
\end{definition}

Lastly we define the inserted labelling, the labelling out of the inserted tree.

\begin{definition}[Inserted labelling]
  Given an insertion point \((S, P, T)\) with \(L : S \to \U\) and
  \(M : T \to \U\), we define the \emph{inserted labelling}
  \(\insertion L P M : {\insertion S P T} \to \U\). Let
  \[ S = [S_0,\dots,S_n] \qquad L = s_0 \{L_0\}s_1 \cdots \{L_n\}s_{n+1} : A\]
  and then proceed by induction on \(P\).

  \begin{itemize}
    \item Let \(P = [k]\), and
      \[ T = [T_0,\dots,T_m] \qquad M = t_0\{M_0\}t_1 \cdots
      \{M_m\}t_{m+1} : B\]
      Then define \(\insertion L {[k]} M\) to be:
      \[s_0\{L_0\}s_1 \cdots \{L_{k-1}\}t_0\{M_0\}t_1\cdots
      \{M_m\}t_{m+1}\{L_{k+1}\}s_{k+2}\cdots \{L_n\}s_{n+1} : A\]

    \item Suppose \(P = k :: Q\) so that
      \[T = [T_0] \qquad M = t_0\{M_0\}t_1 : B\]
      Define \(\insertion L P M\) as:
      \[s_0\{L_0\}s_1\cdots \{L_{k-1}\}t_0\{\insertion {L_k} {Q}
      {M_0}\}t_1\{L_{k+1}\}s_{k+2} \cdots \{L_n\}s_{n+1} : A\]
  \end{itemize}
\end{definition}

We now proceed to prove that each of these constructions used to
generate insertion is well-formed. We begin with the following small lemma.

\begin{lemma}
  \label{lem:inserted-label-lem}
  Let \((S,P,T,\U,L,M)\) be an insertion redex. If we further suppose
  that \(\U \vdash L : S\) and \(\U \vdash M : T\), then:
  \[ \U \vdash \arr {L[k]} {\ty(L)} {L[k+1]} = \arr {M[0]} {\ty(M)} {M[m+1]}\]
  where \(k\) is the first element of \(P\) (as \(P\) is non-empty)
  and \(T\) has length \(m\).
\end{lemma}
\begin{proof}
  From the insertion redex, we have \(L(\olsi P) \equiv \stdcoh T
  {\lh(P)} \sub M\). By assumption, \(P\) is of the form \(k :: p\),
  where \(p\) is a path and \(S = [S_0,\dots,S_n]\) and so
  \[\SPath(\olsi P) \equiv  \Inc_k(\SPath(p)) \]
  and so supposing that \(S_k \vdash \SPath(p) : A\) (as every path
  is well-formed), we can obtain:
  \[\U \vdash \SPath(\olsi P) \sub L : \Sigma(A) \sub {\inc_k} \sub L\]
  By \cref{prop:standard-typing}, \(\U \vdash \stdcoh T {\lh(P)} \sub
  M : \stdty T {\lh(P)} \sub M\). Therefore, by uniqueness of types
  (using the syntactic equality from the insertion redex), we have:
  \[ \U \vdash \Sigma(A) \sub {\inc_k \bullet L} = \stdty T {\lh(P)} \sub M\]
  By truncating both sides of this equality \(\lh(P) - 1\) times we get:
  \[ \U \vdash \Sigma(\star) \sub {\inc_k \bullet L} = \stdty T 1 \sub M\]
  which after expanding definitions on both sides gives the required equality.
\end{proof}

The typing properties of each of the constructions involved in
insertion are given in the following proposition.
\begin{proposition}
  \label{prop:ins-typing}
  Let \((S,P,T)\) be an insertion point. Then:
  \[\insertion S P T \vdash \iota_{S,P,T} : T \qquad \insertion S P T
  \vdash \kappa_{S,P,T} : S\]
  If we further have \(\U \vdash L : S\) and \(\U \vdash M : S\) with
  \(L(\olsi P) \equiv \stdcoh T {\lh(P)} \sub M\) then:
  \[ \U \vdash \insertion L P M : \insertion S P T\]
\end{proposition}
\begin{proof}
  The labellings \(\iota\) and \(\kappa\) are formed using
  constructions that have already been shown to be well-formed. We
  therefore focus on the typing judgement for the inserted labelling.
  As in the definition of the inserted labelling, we let
  \[ S = [S_0,\dots,S_n] \qquad L = s_0 \{L_0\}s_1 \cdots \{L_n\}s_{n+1} : A\]
  By inspection of the typing derivation \(\U \vdash L : S\) we have
  that \(\U \vdash s_i : A\) and \(\U \vdash L_i : S_i\) for each \(i\).

  We then proceed by induction on \(P\).
  \begin{itemize}
    \item Let \(P = [k]\) and
      \[ T = [T_0,\dots,T_m] \qquad M = t_0\{M_0\}t_1 \cdots
      \{M_m\}t_{m+1} : B\]
      By \(\U \vdash M : T\), we have that \(\U \vdash t_i : B\) and
      \(\U \vdash M_i : T_i\) for each \(i\). Applying
      \cref{lem:inserted-label-lem}, we have \(\U \vdash A = B\),
      \(\U \vdash s_k = t_0\), and \(\U \vdash s_{k+1} = t_{m+1}\).
      Therefore, by applying the conversion rule, \(\U \vdash t_i :
      A\). To complete this case, we must show that for each \(i\):
      \[ \U \vdash (\insertion L P M)_i : (\insertion S P T)_i\]
      For most \(i\) this is trivial, however there is a subtlety for
      \(i = k-1\) that \((\insertion L P M)_{k-1} \not\equiv L_{k-1}\), as:
      \[\ty((\insertion L P M)_{k-1}) \equiv \arr {s_{k-1}} A {t_0}
      \not\equiv \arr {s_{k-1}} A {s_k} \equiv \ty(L_{k-1})\]
      However, the equality \(\U \vdash s_k = t_0\) means that these
      two types are definitionally equal, and so the required typing
      derivation follows from \(\U \vdash L_{k-1} : S_k\). A similar
      argument is needed to prove that \(\U \vdash L_{k+1} :
      S_{k+1}\), completing this case.
    \item Suppose \(P = k :: Q\) so that
      \[T = [T_0] \qquad M = t_0\{M_0\}t_1 : B\]
      with \(\U \vdash M_0 : T_0\) and \(\U \vdash t_i : B\) for \(i
      \in \{0,1\}\). Then:
      \begin{align*}
        L_k(\olsi{Q}) &\equiv L(\olsi P)\\
        &\equiv \stdcoh T {\lh(P)} \sub M\\
        &\equiv \Sigma \left(\stdcoh {T_0} {\lh(Q)}\right) \sub M\\
        &\equiv \stdcoh {T_0} {\lh(Q)} \sub {M_0}
      \end{align*}
      and so by inductive hypothesis, we have \(\U \vdash\insertion
      {L_k} {Q} {M_0} : \insertion {S_k} {Q} {T_0}\). Then by a
      similar argument to above it can be shown that \(\insertion L P
      M\) is well-formed.
  \end{itemize}
  Hence, \(\U \vdash \insertion L P M : \insertion S P T\) for all
  branches \(P\).
\end{proof}

We now end this section by formally giving the equality rule set for insertion.

\begin{definition}
  \label{def:insertion-rule}
  The \emph{insertion rule set}, \insert, is the set consisting of the triples:
  \[ (\Gamma, \lfloor \SCoh S A L \rfloor, \lfloor \SCoh {\insertion
  S P T} {A \sub {\kappa_{S,P,T}}} {\insertion L P M} \rfloor)\]
  for each insertion redex \((S,P,T,\Gamma,L,M)\), and structured type \(A\).

  A set of rules \(\mathcal{R}\) \emph{contains insertion} if
  \(\insert \subseteq \mathcal{R}\). Insertion makes the following
  rule admissible:
  \begin{equation*}
    \inferrule{(S,P,T,\Gamma,L,M)\text{ is an insertion redex}\\ S
    \vdash A \\ \Gamma \vdash L : S}{\Gamma \vdash \SCoh S A L =
    \SCoh {\insertion S P T} {A \sub {\kappa_{S,P,T}}} {\insertion L P M}}
  \end{equation*}
  The set \(\mathcal{R}\) \emph{has insertion} if the rule
  \textsc{insert} holds in the generated theory.
\end{definition}

\subsection{Universal property of insertion}
\label{sec:univ-prop-insert}

As stated in the previous section, the constructions involved in
insertion arise as a pushout square. In this section, we prove this
result, which we state below. Throughout this section we assume that
we are working in a tame theory for which the support and
preservation conditions hold. Further, we only give the maximal
arguments of substitutions from a disc, as we only work with
well-formed syntax up to definitional equality and so the type will
always be inferable.

\begin{theorem}
  \label{thm:univ-prop}
  Let \((S,P,T)\) be an insertion point. Then the following
  commutative square of \(\mathsf{Catt}_{\mathcal{R}}\) is cocartesian:
  \[
    \begin{tikzcd}[column sep = large, row sep = large]
      {D^{\lh(P)}} & \lfloor S \rfloor \\
      \lfloor T \rfloor & {\lfloor \insertion S P T \rfloor}
      \arrow["\lfloor \kappa \rfloor", from=1-2, to=2-2]
      \arrow["\lfloor \iota \rfloor"', from=2-1, to=2-2]
      \arrow["{\{\lfloor \olsi P \rfloor\}}", from=1-1, to=1-2]
      \arrow["{\{\lfloor \stdcoh T {\lh(P)} \rfloor\}}"', from=1-1, to=2-1]
      \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
      scale=1.5}, draw=none, from=2-2, to=1-1]
    \end{tikzcd}
  \]
  The context \(\lfloor \insertion S P T \rfloor\) is the pushout of
  \(\lfloor S \rfloor\) and \(\lfloor T \rfloor\) along the maps that
  send the maximal variable of \(D^n\) to the locally maximal
  variable corresponding to the branch \(P\) and the standard
  coherence of over \(T\) of dimension equal to the leaf height of \(P\).
\end{theorem}

This theorem allows an intuitive understanding of the insertion
operation; the inserted tree \(\insertion S P T\) is the result of
taking the disjoint union of \(S\) and \(T\) and gluing the locally
maximal variable of \(S\) corresponding to the branch \(P\) to the
composite of \(T\). The original motivation for insertion was to take
a term where one of the locally maximal arguments was a standard
composition and flatten the structure, which aligns with the
intuition given by the universal property.

\begin{remark}
  As contexts have an interpretation as freely generated
  \(\infty\)-categories, and the category of \(\infty\)-categories is
  cocomplete, there is an \(\infty\)-category pushout of this square.
  It however may be surprising that this pushout is freely generated
  and happens to be freely generated by a pasting diagram.
\end{remark}

We work towards \cref{thm:univ-prop} by introducing a couple of
lemmas. These lemmas will mostly be proven by deferring to the
formalisation, using the machinery of structured terms introduced in
\cref{sec:structured-terms} to simplify the computations involved. We
first show that the square is commutative, while also justifying the
description of the exterior labelling given at the start of the section.

\begin{lemma}
  \label{lem:iota-kappa-comm}
  Let \((S,P,T)\) be an insertion point. Then \(\kappa(\olsi P)
  \equiv \stdcoh T {\lh(P)} \sub \iota\).
\end{lemma}
\begin{proof}
  See \func{Catt.Tree.Insertion.Properties}{κ-branch-path} in
  \module{Catt.Tree.Insertion.Properties}.
\end{proof}

We next state two factorisation properties for the interior and
exterior labellings.

\begin{lemma}
  \label{lem:ins-comm-max}
  For insertion redex \((S,P,T,\U,L,M)\), the following hold:
  \[ \iota_{S,P,T} \circ (\insertion L P M) \equiv M \qquad
  \kappa_{S,P,T} \circ (\insertion L PM) \equiv^{\max} L \]
  Hence, the maps \(L\) and \(M\) factor through the labellings
  \(\kappa\) and \(\iota\) respectively.
\end{lemma}
\begin{proof}
  See \funcn{Catt.Tree.Insertion.Properties}{4201}{ι-comm} and
  \funcn{Catt.Tree.Insertion.Properties}{4738}{κ-comm} in
  \module{Catt.Tree.Insertion.Properties}.
\end{proof}

We can now proceed with the proof of \cref{thm:univ-prop}.

\begin{proof}[Proof of \cref{thm:univ-prop}]
  Let \((S,P,T)\) be an insertion point. We must first show that the
  candidate pushout square is in fact commutative, for which it is
  sufficient to show:
  \[ \{\ty(\olsi P), \olsi P\} \bullet \kappa \equiv^{\max} \{\stdty
  T {\lh(P)}, \stdcoh T {\lh(P)}\} \bullet \iota \]
  which follows from \cref{lem:iota-kappa-comm}. To prove that this
  square is cocartesian, we take two substitutions \(\sigma : \lfloor
  S \rfloor \to \Gamma\) and \(\tau : \lfloor T \rfloor \to \Gamma\)
  such that the following diagram is commutative:
  \[
    \begin{tikzcd}[column sep = large, row sep = large]
      {D^{\lh(P)}} & \lfloor S \rfloor \\
      \lfloor T \rfloor & {\lfloor \insertion S P T \rfloor}\\
      && \Gamma
      \arrow["\lfloor \kappa \rfloor", from=1-2, to=2-2]
      \arrow["\lfloor \iota \rfloor"', from=2-1, to=2-2]
      \arrow["{\{\lfloor \olsi P \rfloor\}}", from=1-1, to=1-2]
      \arrow["{\{\lfloor \stdcoh T n \rfloor\}}"', from=1-1, to=2-1]
      \arrow["\sigma", curve={height=-18pt}, from=1-2, to=3-3]
      \arrow["\tau"', curve={height=12pt}, from=2-1, to=3-3]
    \end{tikzcd}
  \]
  We therefore have that \(\lceil \sigma \rceil\) is a labelling \(S
  \to \Gamma\) and \(\lceil \tau \rceil\) is a labelling \(T \to \Gamma\) with
  \[\Gamma \vdash \lceil \sigma \rceil(\olsi P) = \stdcoh T {\lh(P)}
  \sub{\lceil \tau \rceil}\]
  To apply \cref{lem:ins-comm-max}, we need this to be a syntactic
  equality. We therefore define \(M  =\lceil \tau \rceil\) and \(L\)
  to be given by:
  \[ L(p) =
    \begin{cases*}
      \stdcoh T {\lh(P)} \sub M &if \(p = \olsi P\)\\
      \lceil \sigma \rceil(p)&otherwise
    \end{cases*}
  \]
  by the equality above, \(L\) is well-formed and \(\lfloor L \rfloor
  = \sigma\). We then get a well-formed map \(\lfloor \insertion L P
  M \rfloor\) from \(\lfloor \insertion S P T \rfloor\) to \(\Gamma\)
  such that the following diagram is commutative by \cref{lem:ins-comm-max}:
  \[
    \begin{tikzcd}[column sep = large, row sep = large]
      {D^{\lh(P)}} & \lfloor S \rfloor \\
      \lfloor T \rfloor & {\lfloor \insertion S P T \rfloor}\\
      && \Gamma
      \arrow["\lfloor \kappa \rfloor", from=1-2, to=2-2]
      \arrow["\lfloor \iota \rfloor"', from=2-1, to=2-2]
      \arrow["{\{\lfloor \olsi P \rfloor\}}", from=1-1, to=1-2]
      \arrow["{\{\lfloor \stdcoh T n \rfloor\}}"', from=1-1, to=2-1]
      \arrow["\lfloor \sigma \rfloor", curve={height=-18pt}, from=1-2, to=3-3]
      \arrow["\lfloor M \rfloor"', curve={height=12pt}, from=2-1, to=3-3]
      \arrow["\lfloor \insertion L P M \rfloor"{description}, from=2-2, to=3-3]
    \end{tikzcd}
  \]
  The uniqueness of this morphism follows from the observation that
  every path of \(\insertion S P T\) is either of the form
  \(\iota(p)\) for some \(p : \Path_T\) or \(\kappa(q)\) for some
  \(q: \Path_S\).
\end{proof}

From this result we will be able to show that having insertion in a
theory implies the existence of pruning. The plan will be to show
that pruning satisfies a similar universal property.

\begin{proposition}
  Let \(\mathcal{D} : \Dyck_0\) be a Dyck word, and let \(p\) be a
  peak of \(\mathcal{D}\). Then the following square is a pushout square:
  \[
    \begin{tikzcd}[column sep = large, row sep = large]
      {D^{n+1}} & \lfloor \mathcal{D} \rfloor \\
      D^{n} & {\lfloor \mathcal{D} \sslash p \rfloor}
      \arrow["\pi_p", from=1-2, to=2-2]
      \arrow["{\{\src(\lfloor p \rfloor)\}}"', from=2-1, to=2-2]
      \arrow["{\{\lfloor p \rfloor\}}", from=1-1, to=1-2]
      \arrow["{\{\id(d_n)\}}"', from=1-1, to=2-1]
      \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
      scale=1.5}, draw=none, from=2-2, to=1-1]
    \end{tikzcd}
  \]
  where \(\dim(A) = n\), and each substitution from a disc is given
  only by its maximal element.
\end{proposition}
\begin{proof}
  As discussed in \cref{sec:prune-construction}, the substitution
  \(\pi_p\) sends \(\lfloor p \rfloor\) to the identity on the source
  of \(\lfloor p \rfloor\), which makes the square commute, as it
  suffices to consider the action of each substitution on
  \(d_{n+1}\), the maximal variable of \(D^{n+1}\). We now assume
  that we have substitutions \(\sigma : \lfloor \mathcal{D} \rfloor
  \to \Gamma\) and \(\{t\} : D^n \to \Gamma\) such that the following
  diagram commutes:
  \[
    \begin{tikzcd}[column sep = large, row sep = large]
      {D^{n+1}} & \lfloor \mathcal{D} \rfloor \\
      D^{n} & {\lfloor \mathcal{D} \sslash p \rfloor}\\
      && \Gamma
      \arrow["\pi_p", from=1-2, to=2-2]
      \arrow["{\{\src(\lfloor p \rfloor)\}}"', from=2-1, to=2-2]
      \arrow["{\{\lfloor p \rfloor\}}", from=1-1, to=1-2]
      \arrow["{\{\id(d_n)\}}"', from=1-1, to=2-1]
      \arrow["\sigma", curve={height=-18pt}, from=1-2, to=3-3]
      \arrow["{\{t\}}", curve={height=18pt}, from=2-1, to=3-3]
      \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
      scale=1.5}, draw=none, from=2-2, to=1-1]
    \end{tikzcd}
  \]
  We immediately have that \(\lfloor p \rfloor \sub \sigma =
  \id(\{t\})\). We can therefore let \(\sigma'\) the same
  substitution as \(\sigma\) but with \(\lfloor p \rfloor \sub
  \sigma\) replaced by \(\id(\{t\})\), and then can form the substitution:
  \[ \sigma \sslash p \equiv \sigma' \sslash p : \lfloor
  \mathcal{D}\sslash p \rfloor \to \Gamma\]
  By \cref{prop:prune-ty}, we immediately have \(\sigma = \sigma' =
  \pi_p \bullet \sigma \sslash p\). The other equality follows from a
  diagram chase, noting that \(d_n^-\) in \(D^{n+1}\) is sent to the
  variable \(d^n\) in \(D^n\) by the map \(\{\id(d_n)\}\).

  It remains to show that the chosen universal map \(\sigma \sslash
  p\) is unique, but this is trivial as every variable of \(\lfloor
  \mathcal{D} \sslash p \rfloor\) is also a variable of \(\lfloor
  \mathcal{D} \rfloor\), and so the universal map is fully determined
  by the substitution \(\sigma\).
\end{proof}

\begin{corollary}
  \label{cor:insertion-pruning}
  Let \(\mathcal{R}\) have insertion. Then \(\mathcal{R}\) has pruning.
\end{corollary}
\begin{proof}
  Assume \(\mathcal{R}\) has insertion. Then take a term \(\Coh
  {\lfloor \mathcal{D} \rfloor} A \sigma : \Term_\Gamma\) with a peak
  \(p : \Peak_{\mathcal{D}}\) such that:
  \[ \lfloor p \rfloor \sub \sigma \equiv \id(A,t)\]
  for some term \(t\) and type \(A\) of \(\Gamma\). We then need to show that:
  \[ \Gamma \vdash \Coh {\lfloor \mathcal{D} \rfloor} A \sigma = \Coh
  {\lfloor \mathcal{D} \sslash p\rfloor} {A \sub {\pi_p}} {\sigma \sslash p}\]
  From \(\lfloor \mathcal{D} \rfloor\) we can obtain a tree \(S\)
  with \(\lfloor S \rfloor \equiv \lfloor \mathcal{D} \rfloor\).
  Further, \(\lfloor p \rfloor\) is a locally maximal variable of
  \(\lfloor \mathcal{D}  \rfloor\), and so there exists a branch
  \(P\) such that \(\lfloor \olsi P \rfloor\) is this locally maximal
  variable, and \(\bh(P) = \lh(P) - 1\). Then the diagram:
  \[
    % https://q.uiver.app/#q=WzAsMyxbMSwwLCJEXntuKzF9Il0sWzAsMSwiRF5uIl0sWzIsMSwiXFxsZmxvb3IgUyBcXHJmbG9vciJdLFswLDEsIlxceyBcXGlkKHQpIFxcfSIsMl0sWzAsMiwiXFx7IFxcbGZsb29yIHAgXFxyZmxvb3IgXFx9Il1d % tex-fmt: skip
    \begin{tikzcd}
      & {D^{n+1}} \\
      {D^n} && {\lfloor S \rfloor}
      \arrow["{\{ \id(t) \}}"', from=1-2, to=2-1]
      \arrow["{\{ \lfloor p \rfloor \}}", from=1-2, to=2-3]
    \end{tikzcd}
  \]
  has two pushouts, the one given by insertion, and the one given by
  pruning. Therefore, we obtain an isomorphism \(\lfloor \insertion S
  P {D^n} \rfloor \cong \lfloor \mathcal{D} \sslash p \rfloor\). By
  \cref{prop:ps-context-iso}, this isomorphism must be the identity
  (as both pushouts exist in \textsf{Catt}), and so we can deduce
  that \(\pi_p = \kappa_{S,P,D^n}\) and \(\sigma \sslash p = \lfloor
    \insertion {\lceil \sigma \rceil} P {\{\lceil t \rceil\}}
  \rfloor\). Therefore, the above equality is given by an insertion along \(P\).
\end{proof}

\subsection{The insertion rule}
\label{sec:insertion-rule}

We now prove that the insertion rule set given in
\cref{sec:insertion-rule} satisfies the various conditions presented
in \cref{sec:ruleset}. We begin with the following lemma.

\begin{lemma}
  \label{lem:insertion-map}
  Let \((S,P,T)\) be an insertion point and let \(L : S \to \U\) and
  \(M : T \to \U\) be labellings. Let \(f : \STerm_\U \to
  \STerm_{\U'}\) be any function from structured terms of \(\U\) to
  structured terms of \(\U'\). Then for any path \(p\) of
  \(\insertion S P T\) we have:
  \[ f((\insertion S P T)(p)) \equiv (\insertion {(f \circ L)} P {(f
  \circ M)})(p)\]
  where \(f \circ L\) is the result of composing \(f\) to the
  function component of \(L\).
\end{lemma}
\begin{proof}
  The proof of this follows by a simple induction on \(P\) and is
  given in the formalisation module
  \module{Catt.Tree.Insertion.Properties} by function
  \func{Catt.Tree.Insertion.Properties}{label-from-insertion-map}.
\end{proof}

\begin{proposition}
  \label{prop:insert-tame}
  The insertion rule set, \insert, satisfies the suspension
  condition. It further satisfies the \(\mathcal{R}\)-substitution
  condition for any rule set \(\mathcal{R}\), and so also satisfies
  the weakening condition.
\end{proposition}
\begin{proof}
  Let \((S,P,T,\Gamma,L,M)\) be an insertion redex and let \(A\) be a
  structured type of \(S\), such that:
  \[ s \equiv \SCoh S A L \qquad t \equiv \SCoh {\insertion S P T} {A
    \sub {\kappa_{S,P,T}}} {\insertion L P T} \qquad (\Gamma, \lfloor s
  \rfloor, \lfloor t \rfloor) \in \insert\]
  To prove the suspension condition, we observe that \(0 :: P\) is a
  branch of \(\Sigma(S)\) such that \(\insertion {\Sigma(S)} {0::P}
  {\Sigma(T)} \equiv \Sigma(\insertion S P T)\) and
  \(\kappa_{\Sigma(S),0::P,\Sigma(T)} \equiv \Sigma(\kappa_{S,P,T})\)
  by definition. By applying \cref{lem:insertion-map} with \(f =
  \Sigma\), we get:
  \[ \insertion {\Sigma'(L)} {P} {\Sigma'(M)} \equiv \Sigma'(\insertion L P M)\]
  and so by unwrapping definitions we obtain \(\insertion {\Sigma(L)}
  {0 :: P} {\Sigma(M)} \equiv \Sigma(\insertion L P M)\). Therefore, we have:
  \begin{align*}
    \Sigma(s) &\equiv \SCoh {\Sigma(S)} {\Sigma (A)} {\Sigma(L)} \\
    \Sigma(t) &\equiv \SCoh {\insertion {\Sigma(S)} {0::P}
    {\Sigma(T)}} {\Sigma(A) \sub {\kappa_{\Sigma(S),0::P,\Sigma(T)}}}
    {\insertion {\Sigma(L)} {0::P} {\Sigma(M)}}
  \end{align*}
  and so as
  \[\Sigma(L)(0 :: \olsi P) \equiv \Sigma'(L)(\olsi P) \equiv
    \Sigma(\stdcoh T {\lh(P)} \sub M) \equiv \stdcoh {\Sigma(T)}
  {\lh(0::P)} \sub {\Sigma(M)}\]
  we get \((\Sigma(\Gamma), \Sigma(\lfloor s \rfloor), \Sigma
  (\lfloor t \rfloor) ) \in \insert\) as required.

  For the substitution condition we let \(\sigma : \arr \Gamma \star
  \Delta\) be any substitution. Then:
  \[\lfloor s \rfloor \sub \sigma \equiv \lfloor \SCoh S A {L \bullet
    \lceil \sigma \rceil}  \rfloor \qquad \lfloor t \rfloor \sub \sigma
    \equiv \lfloor \SCoh {\insertion S P T} {A \sub {\kappa_{S,P,T}}}
  {(\insertion L P T) \bullet \lceil \sigma \rceil} \rfloor\]
  Again using \cref{lem:insertion-map}, this time with \(f = u
  \mapsto u\sub{\lceil \sigma \rceil}\), we have:
  \[(\insertion L P M) \bullet \lceil \sigma \rceil \equiv \insertion
  {L \bullet \lceil \sigma \rceil} P {M \bullet \lceil \sigma \rceil}\]

  Further, we have the equality:
  \[(L \bullet \lceil \sigma \rceil)(\olsi P) \equiv L(p) \sub
    {\lceil \sigma \rceil} \equiv \stdcoh T {\lh(P)} \sub {M \bullet
  \lceil \sigma \rceil}\]
  and so \((\Gamma, \lfloor s \rfloor \sub \sigma, \lfloor t \rfloor
  \sub \sigma) \in \insert\) and so \insert satisfies the
  \(\mathcal{R}\)-substitution condition for any \(\mathcal{R}\), as
  we made no assumption on \(\sigma\) being well-formed.
\end{proof}

We next prove the support condition for the insertion rule set. We
start with the following support lemma for the exterior labelling.

\begin{lemma}
  \label{lem:kappa-full}
  Let \((\insertion S P T)\) be an insertion point. Then:
  \[ \Supp(\kappa_{S,P,T}) = \Var(\insertion S P T)\]
  The exterior labelling is full.
\end{lemma}
\begin{proof}
  Proof proceeds by induction on \(P\), the only non-trivial case is
  \(P = [k]\), where we rely on \(\Supp(\{\stdty T {\lh{P}}, \stdcoh
  T {\lh{P}}\})\) being \(\Var(T)\). A full proof is given in the
  formalisation module \module{Catt.Tree.Insertion.Support}.
\end{proof}

Similar to the other rule sets introduced so far, to prove the
support condition for the insertion rule set, we will take an
arbitrary rule set \(\mathcal{R}\) that is tame and satisfies the
support condition, and prove instead that the insertion set satisfies
the \(\mathcal{R}\)-support condition. This result can be then used
as part of the strategy for proving the support condition outlined in
\cref{lem:proof-strat-supp}.

\begin{proposition}
  \label{prop:insert-supp}
  Let \(\mathcal{R}\) be a tame equality rule set that satisfies the
  support condition. Then \insert satisfies the
  \(\mathcal{R}\)-support condition.
\end{proposition}
\begin{proof}
  As in the previous proposition, let \((S,P,T,\Gamma,L,M)\) be an
  insertion redex and \(A\) a structured type of \(S\), such that:
  \[ s \equiv \SCoh S A L \qquad t \equiv \SCoh {\insertion S P T} {A
    \sub {\kappa_{S,P,T}}} {\insertion L P T} \qquad (\Gamma, \lfloor s
  \rfloor, \lfloor t \rfloor) \in \insert\]
  We now assume that \(\Gamma \vdash_{\mathcal{R}} \lfloor s \rfloor
  : B\) for some \(B\) and must prove that \(\Supp(s) = \Supp(t)\).
  By inspecting the typing judgement, we can obtain proofs of the
  following typing judgements:
  \[ \Gamma \vdash L : S \qquad S \vdash A \qquad \Gamma \vdash M : T\]
  where the typing of \(M\) is obtained by transporting the typing of
  \(L(\olsi P)\) along the syntactic equality \(L(\olsi P) \equiv
  \stdcoh T {\lh(P)} \sub M\). By \cref{lem:ins-comm-max}, we have:
  \[ \kappa_{S,P,T} \bullet \insertion L P M \equiv^{\max} L \]
  By \cref{prop:ins-typing}, both sides of this equation are
  well-formed and so by \cref{thm:label-max-equality}, we obtain the equality:
  \[ \Gamma \vdash_{\mathcal{R}} \kappa_{S,P,T} \bullet \insertion L P M = L\]
  As \(\mathcal{R}\) satisfies the support property, we get:
  \begin{align*}
    \Supp(s) &= \Supp(L)\\
    &= \Supp(\kappa_{S,P,T} \bullet \insertion L P M)\\
    &= \Supp(\kappa_{S,P,T}) \sub {\insertion L P M}\\
    &= \Var(\insertion S P T) \sub {\insertion L P M}&\text{by
    \cref{lem:kappa-full}}\\
    &= \Supp(\insertion L P M) \\
    &= \Supp(t)
  \end{align*}
  and so \(\Supp(\lfloor s \rfloor) =\Supp(\lfloor t \rfloor)\) as required.
\end{proof}

Similarly to the situation in pruning, we are not able to show that
the type \(A \sub {\kappa}\) is a valid operation without knowing
more about the set of operations \(\mathcal{O}\). We therefore
introduce the following additional condition on the set of operations.

\begin{definition}
  An operation \(\mathcal{O}\) \emph{supports insertion} if for all
  insertion points \((S,P,T)\) and variable sets \(U,V \subseteq
  \Var(S)\) we have:
  \[ (\lfloor \insertion S P T \rfloor, \lfloor U \sub
    {\kappa_{S,P,T}} \rfloor, \lfloor V \sub {\kappa_{S,P,T}} \rfloor)
  \in \mathcal{O} \]
  whenever \((\lfloor S \rfloor, U, V) \in \mathcal{O}\)
\end{definition}

Using this property, we can give the preservation condition for the
insertion rule set.

\begin{proposition}
  \label{prop:insert-preserve}
  Let \(\mathcal{R}\) be a tame equality rule set and suppose the
  operation set \(\mathcal{O}\) supports insertion. Then the set
  \insert satisfies the \(\mathcal{R}\)-preservation condition.
\end{proposition}
\begin{proof}
  Let \((S,P,T,\Gamma,L,M)\) be an insertion redex and let \(\arr a A
  b\) be a structured type such that:
  \[ s \equiv \SCoh S {\arr a A b} L \qquad t \equiv \SCoh
    {\insertion S P T} {(\arr a A b) \sub {\kappa_{S,P,T}}} {\insertion
  L P T} \qquad (\Gamma, \lfloor s \rfloor, \lfloor t \rfloor) \in \insert\]
  We now suppose that \(\Gamma \vdash \lfloor s \rfloor : B\) and aim
  to prove that \(\Gamma \vdash \lfloor t \rfloor : B\). By
  inspecting the typing derivation we get:
  \begin{mathpar}
    S \vdash \arr a A b \and \Gamma \vdash L : S \and \Gamma \vdash M
    : T \and (\lfloor S \rfloor, \Supp(a), \Supp(b)) \in \mathcal{O}
    \and \Gamma \vdash (\arr a A b) \sub L = A
  \end{mathpar}
  and so by \cref{prop:ins-typing} we have:
  \[ \insertion S P T \vdash \kappa_{S,P,T} : S \qquad \Gamma \vdash
  \insertion L P M : \insertion S P T \]
  As the operation set supports insertion with \(\Supp(a \sub
  {\kappa}) = \Supp(a) \sub \kappa\) and \(\Supp(b \sub {\kappa}) =
  \Supp(b) \sub \kappa\) we get:
  \[ (\lfloor \insertion S P T \rfloor, \Supp(a \sub \kappa), \Supp(b
  \sub \kappa))\]
  and so we obtain:
  \[ \Gamma \vdash \SCoh {\insertion S P T} {(\arr a A b) \sub
    \kappa} {\insertion L P M} : {(\arr a A b) \sub \kappa \sub
  {\insertion L P M}}\]
  By \cref{lem:ins-comm-max,thm:label-max-equality}, \(\Gamma \vdash
  \kappa \bullet \insertion L P M = L\), and so:
  \begin{align*}
    (\arr a A b) \sub \kappa \sub {\insertion L P M} &\equiv (\arr a
    A b) \sub {\kappa \bullet (\insertion L P M)}\\
    &= (\arr a A b) \sub {L}\\
    &= B
  \end{align*}
  and so by applying the conversion rule we obtain \(\Gamma \vdash
  \lfloor t \rfloor : B\) as required.
\end{proof}

\subsection{Further properties}
\label{sec:further-properties}

It has now been proved that insertion can form part of a reasonable
type theory. We now proceed to prove further properties of the
insertion construction that will be critical for proving the
confluence of \Cattsua in \cref{sec:cattsua}. The majority of these
properties will therefore concern the interaction of insertion with
other constructions and itself. We will justify each property with up
to three of the following methods:
\begin{itemize}
  \item For each property, we will give a graphical depiction of the
    constructions involved, similar to the diagram for
    \cref{prop:prune-conf}, which should help build intuition for the
    constructions at play.
  \item Where applicable, each combination of constructions will be
    described using the universal property from
    \cref{sec:univ-prop-insert}. This can be used to classify these
    constructions up to definitional equality.
  \item As these properties are used in a confluence proof, we will
    need a more syntactic form than can be offered by the universal
    property approach. To do this we fall back to the formalisation,
    using the computation power of structured terms to brute force
    each property.
\end{itemize}

The first two properties we consider concern the interaction of
insertion with disc contexts, and will be crucial for proving
confluence cases involving insertion and disc removal. Disc contexts
often admit insertions, and the disc acts as a left and right unit
for the insertion operation.

\paragraph{Insertion into a disc}

We begin by considering insertions into a disc. A disc context has a
branch of height \(0\), and so if the locally maximal variable is
sent to a standard coherence, then insertion can always be preformed.
Inserting into a disc effectively performs disc removal, replacing
the entire disc with the entirety of the inner context. We illustrate
this by the following diagram, where we take the branch \([0,0]\) of
\(D^4\) (which we note is not the minimal branch).

\[
  \insertion {
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base)]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (0,1) (x11) {$\bullet$};
      \node [on grid, Diag1] at (0,2)(x21){$\bullet$};
      \node [on grid, Diag1] at (0,3)(x31) {$\bullet$};
      \node [on grid, Diag1] at (0,4)(x41) {$\bullet$};
      \begin{scope}[on background layer]
        \draw (x01.center) to (x11.center);
        \draw[Diag1,very thick] (x11.center) to (x21.center);
        \draw[Diag1,very thick] (x21.center) to (x31.center);
        \draw[Diag1,very thick] (x31.center) to (x41.center);
      \end{scope}
  \end{tikzpicture}\quad} {[0,0]} {\quad
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base), Diag2]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (0,1) (x11) {$\bullet$};
      \node [on grid] at (-0.5,2)(x21){$\bullet$};
      \node [on grid] at (0.5,2) (x22){$\bullet$};
      \node [on grid] at (-0.9,3)(x31) {$\bullet$};
      \node [on grid] at (-0.1,3) (x32) {$\bullet$};
      \node [on grid] at (-0.9,4)(x41) {$\bullet$};
      \draw (x01.center) to (x11.center);
      \draw (x11.center) to (x21.center);
      \draw (x11.center) to (x22.center);
      \draw (x21.center) to (x31.center);
      \draw (x21.center) to (x32.center);
      \draw (x31.center) to (x41.center);
  \end{tikzpicture}\qquad}
  =
  \qquad
  \begin{tikzpicture}[xscale=1.4,every node/.append
    style={scale=0.85},baseline=(x21.base),Diag2]
    \node [on grid] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (0,1) (x11) {$\bullet$};
    \node [on grid] at (-0.5,2)(x21){$\bullet$};
    \node [on grid] at (0.5,2) (x22){$\bullet$};
    \node [on grid] at (-0.9,3)(x31) {$\bullet$};
    \node [on grid] at (-0.1,3) (x32) {$\bullet$};
    \node [on grid] at (-0.9,4)(x41) {$\bullet$};
    \draw (x01.center) to (x11.center);
    \draw (x11.center) to (x21.center);
    \draw (x11.center) to (x22.center);
    \draw (x21.center) to (x31.center);
    \draw (x21.center) to (x32.center);
    \draw (x31.center) to (x41.center);
  \end{tikzpicture}
\]

This property of insertion also has a simple proof by universal
property. Suppose we have disc \(D^n\) with a branch \(P\) and we
insert tree \(T\). Then the inserted tree is given by the following pushout.
\[
  \begin{tikzcd}
    {D^n} & {D^n} \\
    T & {\insertion {D^n} P T}
    \arrow["{\{\stdcoh T n\}}"', from=1-1, to=2-1]
    \arrow["\id", from=1-1, to=1-2]
    \arrow["\iota"', from=2-1, to=2-2]
    \arrow["\kappa", from=1-2, to=2-2]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=2-2, to=1-1]
  \end{tikzcd}
\]
By standard properties of pushouts, we have that \(\insertion {D^n} P
T\) is isomorphic to \(T\). As this pushout holds in \Catt, we have a
\Catt isomorphism between pasting contexts and so by
\cref{prop:ps-context-iso}, \(T = \insertion {D^n} P T\), \(\iota =
\id\). The following lemma gives syntactic versions of these properties.

\begin{lemma}
  \label{lem:disc-insertion-1}
  Let \(T\) be a tree, \(n \geq \dim (T)\), and \(P\) a branch of
  \(D^n\) with \(\bh(P) \leq \th(T)\). Then \(\insertion {D^n} P T =
  T\) and \(\iota_{D^n,P,T} \equiv \id\). Suppose further that
  \((D^n,P,T,\Gamma,L,M)\) is an insertion redex. Then \(\insertion L
  P M \equiv M\).
\end{lemma}
\begin{proof}
  See the functions
  \func{Catt.Tree.Insertion.Properties}{disc-insertion},
  \func{Catt.Tree.Insertion.Properties}{disc-ι}, and
  \func{Catt.Tree.Insertion.Properties}{disc-label-from} in
  formalisation module \module{Catt.Tree.Insertion.Properties}.
\end{proof}

\paragraph{Insertion of a disc}

We now consider the opposite situation, where a disc context is
inserted into an arbitrary tree. For a tree \(T\), with a branch
\(P\), we can always insert the disc context \(D^{\lh(P)}\), as the
trunk height condition will be satisfied by the linearity of the disc
context. Inserting such a disc context makes no change to the tree
\(T\), as the operation effectively replaces a branch of \(T\) (which
is linear by construction) by a disc. The diagram below depicts this
construction.

\[
  \insertion {
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base)]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (0,1) (x11) {$\bullet$};
      \node [on grid] at (-0.5,2)(x21){$\bullet$};
      \node [on grid] at (0.5,2) (x22){$\bullet$};
      \node [on grid] at (0,3)(x31) {$\bullet$};
      \node [on grid] at (1,3) (x32) {$\bullet$};
      \node [on grid, Diag1] at (0,4)(x41) {$\bullet$};
      \node [on grid] at (1,4)(x42) {$\bullet$};
      \begin{scope}[on background layer]
        \draw (x01.center) to (x11.center);
        \draw (x11.center) to (x21.center);
        \draw (x11.center) to (x22.center);
        \draw (x22.center) to (x31.center);
        \draw (x22.center) to (x32.center);
        \draw[very thick,Diag1] (x31.center) to (x41.center);
        \draw (x32.center) to (x42.center);
      \end{scope}
  \end{tikzpicture}\quad} {[0,1,0,0]} {\quad
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base),Diag2]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (0,1) (x11) {$\bullet$};
      \node [on grid] at (0,2)(x21){$\bullet$};
      \node [on grid] at (0,3)(x31) {$\bullet$};
      \node [on grid] at (0,4)(x41) {$\bullet$};
      \draw (x01.center) to (x11.center);
      \draw (x11.center) to (x21.center);
      \draw (x21.center) to (x31.center);
      \draw (x31.center) to (x41.center);
  \end{tikzpicture}\qquad}
  =
  \qquad
  \begin{tikzpicture}[xscale=1.4,every node/.append
    style={scale=0.85},baseline=(x21.base)]
    \node [on grid, Diag2] at (0,0) (x01) {$\bullet$};
    \node [on grid, Diag2] at (0,1) (x11) {$\bullet$};
    \node [on grid] at (-0.5,2)(x21){$\bullet$};
    \node [on grid, Diag2] at (0.5,2) (x22){$\bullet$};
    \node [on grid, Diag2] at (0,3)(x31) {$\bullet$};
    \node [on grid] at (1,3) (x32) {$\bullet$};
    \node [on grid, Diag2] at (0,4)(x41) {$\bullet$};
    \node [on grid] at (1,4)(x42) {$\bullet$};
    \begin{scope}[on background layer]
      \draw[Diag2] (x01.center) to (x11.center);
      \draw (x11.center) to (x21.center);
      \draw[Diag2] (x11.center) to (x22.center);
      \draw[Diag2] (x22.center) to (x31.center);
      \draw (x22.center) to (x32.center);
      \draw[Diag2] (x31.center) to (x41.center);
      \draw (x32.center) to (x42.center);
    \end{scope}
  \end{tikzpicture}
\]

Similar to the insertion into the disc, the insertion of a disc can
be characterised by universal property. Take any tree \(T\) with a
branch \(P\). Then the tree \(\insertion T P {D^{\lh(P)}}\) is the
following pushout:
\[
  \begin{tikzcd}
    {D^n} & T \\
    {D^n} & {\insertion T P {D^n}}
    \arrow["{\{\olsi P\}}", from=1-1, to=1-2]
    \arrow["{\{\stdcoh {D^n} n\}}"', from=1-1, to=2-1]
    \arrow["\iota"', from=2-1, to=2-2]
    \arrow["\kappa", from=1-2, to=2-2]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=2-2, to=1-1]
  \end{tikzcd}
\]
The situation here is less clear than before, as the map \(D^n \to
D^n\) is not the identity. However, in the presence of disc removal
this map becomes equal to the identity, and in this case a similar
argument can be made to determine that \(\kappa\) should be the
identity and \(\insertion T P {D^{\lh(P)}}\) should be equal to the
tree \(T\). The results are given in the lemma below:

\begin{lemma}
  \label{lem:disc-insertion-2}
  Let \((T,P,D^{\lh(P)},\Gamma,L,M)\) be an insertion redex. Then:
  \[\insertion T P {D^{\lh(P)}} \equiv S \qquad \insertion L P M
  \equiv^{\mathsf{max}} L\]
  We further have:
  \[S \vdash_{\mathcal{R}} \kappa_{S,P,D^{\lh(P)}} =^{\mathsf{max}} \id_{S}\]
  if \(\mathcal{R}\) is a (tame) equality rule set which has disc removal.
\end{lemma}
\begin{proof}
  See the functions
  \func{Catt.Tree.Insertion.Properties}{insertion-disc} and
  \func{Catt.Tree.Insertion.Properties}{disc-label-from-2} in the
  formalisation module \module{Catt.Tree.Insertion.Properties} and
  \funcn{Catt.Typing.Insertion.Equality}{10459}{κ-disc} in
  \module{Catt.Typing.Insertion.Equality}.
\end{proof}

\paragraph{Insertion of an endo-coherence} We now turn our attention
to the interaction between insertion and endo-coherence removal.
Unlike in \Cattsu, the locally maximal argument in an insertion redex
need not be in normal form. In particular, since the only condition
on the locally maximal argument is that it is a standard coherence,
it may be an endo-coherence. In such a situation there are two
distinct ways of applying equalities:
\begin{itemize}
  \item The endo-coherence could be directly inserted into the head term.
  \item The endo-coherence could be transformed into an identity on a
    standard coherence (see \cref{thm:std-ecr}) after which the head
    term could undergo two insertions, the first of which ``prunes''
    the identity, and the second of which inserts the locally maximal
    argument of the pruned identity.
\end{itemize}
As the insertion of an identity acts in a similar way to pruning (see
\cref{cor:insertion-pruning}), we re-use the notation.

\begin{definition}
  Let \(S\) be a tree, and \(P\) be a branch of \(S\). Then define:
  \[ S \sslash P = \insertion S P {D^{\lh(P) - 1}} \qquad \pi_P =
  \kappa_{S,P,D^{\lh(P) - 1}}\]
  where we note that \((S,P,D^{\lh(P)-1})\) is always an insertion point.
\end{definition}

To perform the second equality path of pruning an identity followed
by inserting the maximal argument of that identity, we must obtain a
branch of the pruned context \(S \sslash P\). This can be done when
\(\lh(P) - \bh(P) \geq 2\) by taking the same list as \(P\), as
depicted in \cref{fig:pruned-branch}. We name such a branch the
\emph{pruned branch}.

\begin{definition}
  Let \(S\) be a tree, and \(P\) be a branch of \(S\) with \(\lh(P) -
  \bh(P) \geq 2\). We then define the \emph{pruned branch} \(P'\) of
  \(S \sslash P\) to be given by the same list as \(P\).
\end{definition}

If \(\lh(P) - \bh(P) = 1\) (noting that \(\lh(P) - \bh(P)\) cannot be
zero) then pruning the branch \(P\) removes the branch entirely, and
so the condition \(\lh(P) - \bh(P) \geq 2\) is necessary to form the
pruned branch. It is clear that \(\bh(P') = \bh(P)\) and \(\lh(P') =
\lh(P) - 1\).

\begin{figure}[ht]
  \centering
  \begin{subfigure}{0.45\linewidth}
    \centering
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base)]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (-0.5,1) (x11) {$\bullet$};
      \node [on grid] at (0.5,1)(x12){$\bullet$};
      \node [on grid] at (0.5,2) (x21){$\bullet$};
      \node [on grid, Diag1] at (0,3)(x31) {$\bullet$};
      \node [on grid] at (1,3) (x32) {$\bullet$};
      \node [on grid, Diag1] at (0,4)(x41) {$\bullet$};
      \node [on grid] at (1,4)(x42) {$\bullet$};
      \node [on grid] at (-0.8,2)(bh) {};
      \node [left=0 of bh ,on grid] {$\bh(P)$};
      \node [on grid] at (-0.8,4)(lh) {};
      \node [left=0 of lh ,on grid] {$\lh(P)$};
      \begin{scope}[on background layer]
        \draw (x01.center) to (x11.center);
        \draw (x01.center) to (x12.center);
        \draw (x12.center) to (x22.center);
        \draw[very thick,Diag1] (x22.center) to (x31.center);
        \draw (x22.center) to (x32.center);
        \draw[very thick,Diag1] (x31.center) to (x41.center);
        \draw (x32.center) to (x42.center);
      \end{scope}
      \draw [<->] (bh.center) to (lh.center);
      \draw [dotted, very thick] (bh) to (x21);
      \draw [dotted, very thick] (lh) to (x41);
    \end{tikzpicture}
    \caption{Tree \(S\) and branch \(P = [1,0,0]\).}
  \end{subfigure}
  \begin{subfigure}{0.45\linewidth}
    \centering
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base)]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (-0.5,1) (x11) {$\bullet$};
      \node [on grid] at (0.5,1)(x12){$\bullet$};
      \node [on grid] at (0.5,2) (x21){$\bullet$};
      \node [on grid, Diag1] at (0,3)(x31) {$\bullet$};
      \node [on grid] at (1,3) (x32) {$\bullet$};
      \node [on grid] at (1,4)(x42) {$\bullet$};
      \begin{scope}[on background layer]
        \draw (x01.center) to (x11.center);
        \draw (x01.center) to (x12.center);
        \draw (x12.center) to (x22.center);
        \draw[very thick,Diag1] (x22.center) to (x31.center);
        \draw (x22.center) to (x32.center);
        \draw (x32.center) to (x42.center);
      \end{scope}
    \end{tikzpicture}
    \caption{Tree \(S \sslash P\) and branch \(P' = [1,0,0]\).}
  \end{subfigure}
  \caption{The pruned branch.}
  \label{fig:pruned-branch}
\end{figure}

We also note that the path \(\olsi{P'}\) is the maximal argument of
the labelling \(\iota_{S,P,D^{\lh{P}- 1}}\), the inclusion of
\(D^{\lh(P)- 1}\) into \(S \sslash P\). Insertion along the pruned
branch is then characterised by the following pushout.
\[
  \begin{tikzcd}
    {D^n} & S \\
    {D^{n-1}} & {S \sslash P} \\
    T & {\insertion {(S \sslash P)} {P'} T} \\
    &&& \U\\
    &&& \U
    \arrow["{\{\olsi P\}}", from=1-1, to=1-2]
    \arrow["{\pi_P}", from=1-2, to=2-2]
    \arrow["{\{\olsi {P'}\}}"', from=2-1, to=2-2]
    \arrow["{\{\stdcoh {D^{n-1}} n\}}"', from=1-1, to=2-1]
    \arrow["{\{ \stdcoh T {n-1} \}}"', from=2-1, to=3-1]
    \arrow["\kappa", from=2-2, to=3-2]
    \arrow["\iota"', from=3-1, to=3-2]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=2-2, to=1-1]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=3-2, to=2-1]
    \arrow["L", curve={height=-36pt}, from=1-2, to=4-4]
    \arrow["M"', curve={height=30pt}, from=3-1, to=5-4]
    \arrow["{\insertion L P {(\{\stdcoh T {n-1}\} \bullet
    M)}}"{sloped}, from=2-2, to=4-4, dashed]
    \arrow["{\insertion {(\insertion L P {(\{\stdcoh T {n-1}\}
    \bullet M)})} {P'} {M}}"'{sloped, pos=0.45}, from=3-2, to=5-4, dashed]
    \arrow[equal, nfold, from=4-4, to=5-4]
  \end{tikzcd}
\]
The top pushout is from the construction of \(S \sslash P\), noting
that \(\iota_{S,P,D^{\lh(P) - 1}} = \{\olsi {P'}\}\). The bottom
pushout is from the construction of the insertion along the pruned
branch. By the pasting lemma for pushouts, the whole outer rectangle
is also a pushout along the maps \(\{\hat P\}\) and \(\{\stdcoh
{D^{n-1}} n\} \bullet \{\stdcoh T {n-1}\}\). In the presence of
endo-coherence removal we have:
\[ \{\stdcoh {D^{n-1}} n\} \bullet \{\stdcoh T {n-1}\} = \{\stdcoh T n\}\]
by \cref{thm:std-ecr} and so the outer pushout rectangle is the
pushout generated by directly inserting the endo-coherence. There are
two ways to form the unique map \(\insertion {(S \sslash P)} {P'} {T}
\to \U\), one by the outer pushout rectangle that gives the map
\(\insertion L P M\), and one by first using the top pushout square
with the maps \(L\) and \(\{\stdcoh T {n-1}\} \bullet M\) to get a
map \(S \sslash P \to \U\), and then using this map with the bottom
pushout square and \(M\) to get the morphisms depicted in the
commutative diagram.

These results appear in the next lemma.

\begin{lemma}
  \label{lem:pruned-bp}
  Suppose \(S\) has branch \(P\) with \(\lh(P) - \bh(P) \geq 2\).
  Then \(\iota_{S,P,D^{\lh(P) - 1}} \equiv \{ \olsi {P'} \} \).
  Further suppose that \((S,P,T)\) is an insertion point. Then if the
  (tame) rule set \(\mathcal{R}\) has disc removal and endo-coherence
  removal we get:
  \[\insertion {(S \sslash P)} {P'} T = \insertion S P T \qquad \U
    \vdash_{\mathcal{R}} \pi_P \bullet \kappa_{S \sslash P,P',T}
  =^{\max} \kappa_{S,P,T} \]
  If we further have that \((S,P,T,\U,L,M)\) is an insertion redex then:
  \[\insertion {(\insertion {L} P {(\{\stdcoh T {\lh(P) - 1}\}
  \bullet M)})} {P'} {M} \equiv^{\max} \insertion L P M\]
\end{lemma}
\begin{proof}
  See the functions
  \func{Catt.Tree.Insertion.Properties}{insertion-tree-pruned-branch},
  \func{Catt.Tree.Insertion.Properties}{pruned-branch-prop}, and
  \func{Catt.Tree.Insertion.Properties}{label-from-pruned-branch} in
  formalisation module \module{Catt.Tree.Insertion.Properties}, and
  \funcn{Catt.Typing.Insertion.Equality}{3281}{pruned-branch-κ} in
  \module{Catt.Typing.Insertion.Equality}.
\end{proof}

\paragraph{Branch irrelevance}
As has already been noted, a tree \(S\) may admit multiple branches
\(P\) and \(Q\) which represent the same locally maximal variable,
that is \(\olsi P \equiv \olsi Q\). If there is an insertion that can
be applied along either branch \(P\) or \(Q\) then it does not matter
which branch we choose. This can be immediately seen by the universal
property: The pushout square for an insertion point \((S,P,T)\) only
mentions the path \(\olsi P\) and never uses the actual branch \(P\).

\begin{lemma}
  \label{lem:insertion-irrel}
  Suppose \((S,P,T)\) and \((S,Q,T)\) are insertion points with
  \(\olsi P \equiv \olsi Q\). Then \(\insertion S P T \equiv
  \insertion S Q T\) and \(\kappa_{S,P,T} \equiv^{\mathsf{max}}
  \kappa_{S,Q,T}\). If we further have \(L : S \to \Gamma\) and \(M :
  T \to \Gamma\), then \(\insertion L P M \equiv^{\mathsf{max}}
  \insertion L Q M\).
\end{lemma}
\begin{proof}
  See the functions
  \func{Catt.Tree.Insertion.Properties}{insertion-irrel},
  \func{Catt.Tree.Insertion.Properties}{κ-irrel}, and
  \func{Catt.Tree.Insertion.Properties}{irrel-label-from} in
  formalisation module \module{Catt.Tree.Insertion.Properties}.
\end{proof}

It is natural to ask why we define branches at all, and don't
identify points where insertion can be performed by a maximal path,
implicitly taking the branch of minimal branching height. While this
could be done, it would make other confluence cases more difficult,
as the branch associated to a maximal path could significantly change
if a different branch is pruned from the tree.

\paragraph{Parallel insertion}
We now begin to consider the interaction between insertion and
itself. In contrast to the previous case, we now consider two
branches \(P\) and \(Q\) such that \(\olsi P\) and \(\olsi Q\) are
not the same maximal path, in which case we say the branches \(P\)
and \(Q\) are \emph{parallel}. Assume we have a tree \(S\) such that
\((S, P, T)\) and \((S,Q, U)\) are insertion points. We then aim to
perform both insertions, and prove that the order they occur in is
irrelevant. To do this we must form a branch of the inserted tree
\(\insertion S P T\), which is intuitively given by the branch \(Q\),
but such a branch must be adapted to the new inserted tree.

\begin{definition}
  Let \((S, P, T)\) be an insertion point and let \(Q\) be a branch
  of \(S\) such that \(\olsi P \neq \olsi Q\). Then we define the
  branch \(\insertion Q P T\) of \(\insertion S P T\) by induction on
  \(P\) and \(Q\).
  \begin{itemize}
    \item Suppose \(P = [k]\) and \(Q = j :: x\). Then if \(j < k\)
      we let \(\insertion Q P T = Q\). Otherwise, we let:
      \[\insertion Q P T = (j + \len(T) - 1) :: x\]
    \item Suppose \(P = k :: P_2\) and \(Q = j :: x\). If \(j \neq
      k\) then let \(\insertion Q P T = Q\). Otherwise, both \(P_2\)
      and \(x\) are branches of \(S_k\) and so we let \[\insertion Q
      P T = k :: \insertion x {P_2} T\]
  \end{itemize}
  It is clear that \(\insertion Q P T\) satisfies the condition for
  being a branch.
\end{definition}

The maximal path associated to the branch \(\insertion Q P T\) is
obtained by applying the labelling \(\kappa\) to the maximal path
associated to \(Q\). That is:
\[ \olsi {\insertion Q P T} \equiv \olsi Q \sub {\kappa_{S,P,T}}\]
A graphical example of such a situation is given in
\cref{fig:ins-parallel} where we note how the right branch changes
after the left-hand insertion is performed. We also note that the
final trees at the bottom of the diagram are coloured slightly
differently, which corresponds to the inserted labellings from these
trees being different. To remedy this, we introduce a variant of the
inserted labelling, which takes arguments from the head labelling
instead of the argument labelling wherever possible.

\begin{figure}[ht]
  \centering
  \newsavebox\redbase
  \sbox\redbase{\(
      \begin{tikzpicture}[xscale=1.4,every node/.append
        style={scale=0.85},baseline=(x11.base),Diag1]
        \node [on grid] at (0,0) (x01) {$\bullet$};
        \node [on grid] at (0,1) (x11) {$\bullet$};
        \node [on grid] at (-0.5,2)(x21){$\bullet$};
        \node [on grid] at (0,2) (x22){$\bullet$};
        \node [on grid] at (0.5,2) (x23){$\bullet$};
        \draw (x01.center) to (x11.center);
        \draw (x11.center) to (x21.center);
        \draw (x11.center) to (x22.center);
        \draw (x11.center) to (x23.center);
      \end{tikzpicture}
      \quad\mathop{{}_{[1,0]}\mathord{\gg}}\quad
      \insertion{
        \begin{tikzpicture}[xscale=1.4,every node/.append
          style={scale=0.85},baseline=(x11.base)]
          \node [on grid] at (0,0) (x01) {$\bullet$};
          \node [on grid] at (-0.5,1) (x11) {$\bullet$};
          \node [on grid] at (0,1)(x12){$\bullet$};
          \node [on grid] at (0.5,1)(x13){$\bullet$};
          \node [on grid, Diag1] at (-0.5,2) (x21){$\bullet$};
          \node [on grid, Diag2] at (0.5,2) (x22){$\bullet$};
          \begin{scope}[on background layer]
            \draw (x01.center) to (x11.center);
            \draw (x01.center) to (x12.center);
            \draw (x01.center) to (x13.center);
            \draw[Diag1, very thick] (x12.center) to (x21.center);
            \draw[Diag2, very thick] (x12.center) to (x22.center);
          \end{scope}
        \end{tikzpicture}\quad
      }{[1,1]}{\quad
        \begin{tikzpicture}[xscale=1.4,every node/.append
          style={scale=0.85},baseline=(x11.base),Diag2]
          \node [on grid] at (0,0) (x01) {$\bullet$};
          \node [on grid] at (0,1) (x11) {$\bullet$};
          \node [on grid] at (-0.5,2)(x21){$\bullet$};
          \node [on grid] at (0.5,2) (x22){$\bullet$};
          \draw (x01.center) to (x11.center);
          \draw (x11.center) to (x21.center);
          \draw (x11.center) to (x22.center);
  \end{tikzpicture}}\)}
  \newsavebox\redleft
  \sbox{\redleft}{\(\insertion{
        \begin{tikzpicture}[xscale=1.4,every node/.append
          style={scale=0.85},baseline=(x11.base)]
          \node [on grid, Diag1] at (0,0) (x01) {$\bullet$};
          \node [on grid] at (-0.5,1) (x11) {$\bullet$};
          \node [on grid, Diag1] at (0,1)(x12){$\bullet$};
          \node [on grid] at (0.5,1)(x13){$\bullet$};
          \node [on grid, Diag1] at (-0.6,2) (x21){$\bullet$};
          \node [on grid, Diag1] at (-0.2,2) (x22){$\bullet$};
          \node [on grid, Diag1] at (0.2,2) (x23){$\bullet$};
          \node [on grid, Diag2] at (0.6,2) (x24){$\bullet$};
          \begin{scope}[on background layer]
            \draw (x01.center) to (x11.center);
            \draw[Diag1] (x01.center) to (x12.center);
            \draw (x01.center) to (x13.center);
            \draw[Diag1] (x12.center) to (x21.center);
            \draw[Diag1] (x12.center) to (x22.center);
            \draw[Diag1] (x12.center) to (x23.center);
            \draw[Diag2, very thick] (x12.center) to (x24.center);
          \end{scope}
        \end{tikzpicture}\quad
      }{[1,3]}{\quad
        \begin{tikzpicture}[xscale=1.4,every node/.append
          style={scale=0.85},baseline=(x11.base),Diag2]
          \node [on grid] at (0,0) (x01) {$\bullet$};
          \node [on grid] at (0,1) (x11) {$\bullet$};
          \node [on grid] at (-0.5,2)(x21){$\bullet$};
          \node [on grid] at (0.5,2) (x22){$\bullet$};
          \draw (x01.center) to (x11.center);
          \draw (x11.center) to (x21.center);
          \draw (x11.center) to (x22.center);
  \end{tikzpicture}}\)}

  \newsavebox\redright
  \sbox{\redright}{\(
      \begin{tikzpicture}[xscale=1.4,every node/.append
        style={scale=0.85},baseline=(x11.base),Diag1]
        \node [on grid] at (0,0) (x01) {$\bullet$};
        \node [on grid] at (0,1) (x11) {$\bullet$};
        \node [on grid] at (-0.5,2)(x21){$\bullet$};
        \node [on grid] at (0,2) (x22){$\bullet$};
        \node [on grid] at (0.5,2) (x23){$\bullet$};
        \draw (x01.center) to (x11.center);
        \draw (x11.center) to (x21.center);
        \draw (x11.center) to (x22.center);
        \draw (x11.center) to (x23.center);
      \end{tikzpicture}
      \quad\mathop{{}_{[1,0]}\mathord{\gg}}\quad
      \begin{tikzpicture}[xscale=1.4,every node/.append
        style={scale=0.85},baseline=(x11.base)]
        \node [on grid,Diag2] at (0,0) (x01) {$\bullet$};
        \node [on grid] at (-0.5,1) (x11) {$\bullet$};
        \node [on grid,Diag2] at (0,1)(x12){$\bullet$};
        \node [on grid] at (0.5,1)(x13){$\bullet$};
        \node [on grid, Diag1] at (-0.5,2) (x21){$\bullet$};
        \node [on grid, Diag2] at (0,2) (x22){$\bullet$};
        \node [on grid, Diag2] at (0.5,2) (x23){$\bullet$};
        \begin{scope}[on background layer]
          \draw (x01.center) to (x11.center);
          \draw[Diag2] (x01.center) to (x12.center);
          \draw (x01.center) to (x13.center);
          \draw[Diag1, very thick] (x12.center) to (x21.center);
          \draw[Diag2] (x12.center) to (x22.center);
          \draw[Diag2] (x12.center) to (x23.center);
        \end{scope}
  \end{tikzpicture}\)}

  \newsavebox\redleftbot
  \sbox{\redleftbot}{
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x11.base)]
      \node [on grid,Diag2] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (-0.5,1) (x11) {$\bullet$};
      \node [on grid,Diag2] at (0,1)(x12){$\bullet$};
      \node [on grid] at (0.5,1)(x13){$\bullet$};
      \node [on grid, Diag1] at (-0.8,2) (x21){$\bullet$};
      \node [on grid, Diag1] at (-0.4,2) (x22){$\bullet$};
      \node [on grid, Diag1] at (0,2) (x23){$\bullet$};
      \node [on grid, Diag2] at (0.4,2) (x24){$\bullet$};
      \node [on grid, Diag2] at (0.8,2) (x25){$\bullet$};
      \begin{scope}[on background layer]
        \draw (x01.center) to (x11.center);
        \draw[Diag2] (x01.center) to (x12.center);
        \draw (x01.center) to (x13.center);
        \draw[Diag1] (x12.center) to (x21.center);
        \draw[Diag1] (x12.center) to (x22.center);
        \draw[Diag1] (x12.center) to (x23.center);
        \draw[Diag2] (x12.center) to (x24.center);
        \draw[Diag2] (x12.center) to (x25.center);
      \end{scope}
  \end{tikzpicture}}

  \newsavebox\redrightbot
  \sbox{\redrightbot}{
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x11.base)]
      \node [on grid,Diag1] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (-0.5,1) (x11) {$\bullet$};
      \node [on grid,Diag1] at (0,1)(x12){$\bullet$};
      \node [on grid] at (0.5,1)(x13){$\bullet$};
      \node [on grid, Diag1] at (-0.8,2) (x21){$\bullet$};
      \node [on grid, Diag1] at (-0.4,2) (x22){$\bullet$};
      \node [on grid, Diag1] at (0,2) (x23){$\bullet$};
      \node [on grid, Diag2] at (0.4,2) (x24){$\bullet$};
      \node [on grid, Diag2] at (0.8,2) (x25){$\bullet$};
      \begin{scope}[on background layer]
        \draw (x01.center) to (x11.center);
        \draw[Diag1] (x01.center) to (x12.center);
        \draw (x01.center) to (x13.center);
        \draw[Diag1] (x12.center) to (x21.center);
        \draw[Diag1] (x12.center) to (x22.center);
        \draw[Diag1] (x12.center) to (x23.center);
        \draw[Diag2] (x12.center) to (x24.center);
        \draw[Diag2] (x12.center) to (x25.center);
      \end{scope}
  \end{tikzpicture}}

  \begin{tikzpicture}
    \node(redbase) at (0,0) {\fcolorbox{gray}{white}{\usebox\redbase}};
    \node(redleft) at (-4,-5){\fcolorbox{gray}{white}{\usebox\redleft}};
    \node(redright) at (4,-5){\fcolorbox{gray}{white}{\usebox\redright}};
    \node(redleftbot) at (-4,-9){\fcolorbox{gray}{white}{\usebox\redleftbot}};
    \node(redrightbot) at (4,-9){\fcolorbox{gray}{white}{\usebox\redrightbot}};
    \draw[arrows={->[scale=1.5]},
      line join=round,
      decorate, decoration={
        zigzag,
        segment length=4,
        amplitude=1.2,post=lineto,
        post length=2pt
    }] (redbase) to (redleft);
    \draw[arrows={->[scale=1.5]},
      line join=round,
      decorate, decoration={
        zigzag,
        segment length=4,
        amplitude=1.2,post=lineto,
        post length=2pt
    }] (redbase) to (redright);
    \draw[arrows={->[scale=1.5]},
      line join=round,
      decorate, decoration={
        zigzag,
        segment length=4,
        amplitude=1.2,post=lineto,
        post length=2pt
    }] (redleft) to (redleftbot);
    \draw[arrows={->[scale=1.5]},
      line join=round,
      decorate, decoration={
        zigzag,
        segment length=4,
        amplitude=1.2,post=lineto,
        post length=2pt
    }] (redright) to (redrightbot);
  \end{tikzpicture}
  \caption{Parallel insertions.}
  \label{fig:ins-parallel}
\end{figure}

\begin{definition}
  We define an alternative to the inserted labelling as follows:
  Given an insertion point \((S, P, T)\) with \(L : S \to \U\) and
  \(M : T \to \U\) we define this \emph{alternative inserted
  labelling} \(\insertionprime L P M : {\insertion S P T} \to \U\). Let
  \[ S = [S_0,\dots,S_n] \qquad L = s_0 \{L_0\}s_1 \cdots \{L_n\}s_{n+1} : A\]
  and then proceed by induction on \(P\).

  \begin{itemize}
    \item Let \(P = [k]\), and
      \[ T = [T_0,\dots,T_m] \qquad M = t_0\{M_0\}t_1 \cdots
      \{M_m\}t_{m+1} : B\]
      Then define \(\insertion L {[k]} M\) to be:
      \[s_0\{L_0\}s_1 \cdots \{L_{k-1}\}\mathbf{s_k}\{M_0\}t_1\cdots
      \{M_m\}\mathbf{s_{k+1}}\{L_{k+1}\}s_{k+2}\cdots \{L_n\}s_{n+1} : A\]

    \item Suppose \(P = k :: Q\) so that
      \[T = [T_0] \qquad M = t_0\{M_0\}t_1 : B\]
      Define \(\insertion L P M\) as:
      \[s_0\{L_0\}s_1\cdots \{L_{k-1}\}\mathbf{s_k}\{\insertion {L_k}
      {Q} {M_0}\}\mathbf{s_{k+1}}\{L_{k+1}\}s_{k+2} \cdots \{L_n\}s_{n+1} : A\]
  \end{itemize}
  The terms that differ from the regular inserted labelling are
  written in bold. In the edge case where \(M = \emp\), we
  arbitrarily use \(s_k\) instead of \(s_{k+1}\) for the definition
  of \(\insertionprime L {[k]} M\).
\end{definition}

It is immediate that the alternative inserted labelling only differs
up to definitional equality.

\begin{proposition}
  \label{prop:insertion-prime-eq}
  Let \((S,P,T,\U,L,M)\) be an insertion redex. Then:
  \[\insertionprime L P M = \insertion L P M\]
\end{proposition}
\begin{proof}
  See function
  \func{Catt.Tree.Insertion.Typing}{label-from-insertion-eq} in the
  module \module{Catt.Tree.Insertion.Typing}.
\end{proof}

We now examine the universal property of parallel insertion. This is
given by the following diagram, where we insert along \(P\) first,
followed by \(Q\), letting \(n = \lh(P)\) and \(m = \lh(Q)\).
\[
  \begin{tikzcd}[row sep = large]
    & {D^n} & T \\
    {D^m} & S & {\insertion S P T} \\
    U && {\insertion {(\insertion S P T)} {\insertion Q P T} U}
    \arrow["{\{\olsi P\}}", from=1-2, to=2-2]
    \arrow["{\{\olsi Q\}}"', from=2-1, to=2-2]
    \arrow["{\{\stdcoh U m\}}"', from=2-1, to=3-1]
    \arrow["{\{\stdcoh T n\}}", from=1-2, to=1-3]
    \arrow["{\iota_{S,P,T}}", from=1-3, to=2-3]
    \arrow["{\kappa_{S,P,T}}"', from=2-2, to=2-3]
    \arrow["{\kappa_{\insertion S P T,\insertion Q P T, U}}", from=2-3, to=3-3]
    \arrow["{\iota_{\insertion S P T, \insertion Q P T, U}}"', from=3-1, to=3-3]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=2-3, to=1-2]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=3-3, to=2-1]
  \end{tikzcd}
\]
Here, the top pushout square is given by the insertion along \(P\),
and the bottom square is given by the insertion along \(\insertion Q
P T\), noting that:
\[ \{ \olsi Q \} \bullet \kappa_{S,P,T} \equiv \{ \olsi {\insertion Q P T}\}\]
The construction is therefore given by the colimit of the top-left
border of the diagram. By a symmetric argument, it can be seen that
performing the insertions in the opposite order also leads to a
colimit of the same diagram. We state the lemma that formally states
these ideas.

\begin{lemma}
  \label{lem:insertion-different}
  Let \((S,P,T)\) and \((S,Q,U)\) be insertion points such that
  \(\olsi P \not\equiv \olsi Q\). Then we have:
  \begin{align*}
    \insertion {(\insertion S P T)} {\insertion Q P T} U &\equiv
    \insertion {(\insertion S Q U)} {\insertion P Q U} T\\
    \kappa_{S,P,T} \circ \kappa_{\insertion S P T, \insertion Q P T,
    U} &\equiv^{\max} \kappa_{S,Q,U} \circ \kappa_{\insertion S Q U,
    \insertion P Q U, T}
    \intertext{Further:}
    \insertionprime {(\insertion L P M)} {\insertion Q P T} N
    &\equiv^{\max} \insertionprime {(\insertion L Q N)} {\insertion P Q U} M
  \end{align*}
  for any insertion redexes \((S,P,T,\U,L,M)\) and \((S,P,T,\U,L,N)\).
\end{lemma}
\begin{proof}
  See functions
  \func{Catt.Tree.Insertion.Properties}{insertion-parallel},
  \funcn{Catt.Tree.Insertion.Properties}{33917}{κ-parallel}, and
  \funcn{Catt.Tree.Insertion.Properties}{40709}{label-from-parallel}
  in formalisation module \module{Catt.Tree.Insertion.Properties}.
\end{proof}

\paragraph{Boundaries of inserted trees}

We now work towards the most complex property of insertion, the
action of insertion on an insertable argument. To do this, we must
first understand the action of insertion on standard coherences,
which itself requires an understanding of how insertion interacts
with the boundary inclusion maps of trees.

There are two fundamental cases for the boundary of an inserted tree:
\begin{itemize}
  \item The boundary has low enough dimension such that it is
    unaffected by the insertion. In this case applying the boundary
    to the inserted tree is the same as applying the boundary to the
    original tree.
  \item The boundary has sufficient dimension such that the boundary
    of the original tree still contains the insertion branch. In this
    case applying the boundary to the inserted tree is the same as
    inserting into the boundary of the original tree along this branch.
\end{itemize}

We begin with the first case. Suppose we have an insertion point
\((S, P, T)\) and a dimension \(n \in \mathbb{N}\). The main
criterion for the boundary having no interaction with the insertion is that:
\[ n \leq \th(T) \]
When this condition holds, taking the \(n\)-boundary of \(T\) returns
a linear tree, and we have already seen that inserting linear trees
has no effect on the head tree. We illustrate this case in the
diagram below, where the tree \(T\) has trunk height \(3\) and we set
\(n = 2\). The dashed line represents taking the boundary operation,
and it is easy to see that the two boundary of \(S\) and the
insertion tree \(\insertion S P T\) are the same.

\[\insertion{
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base)]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (-0.5,1) (x11) {$\bullet$};
      \node [on grid] at (0.5,1)(x12){$\bullet$};
      \node [on grid] at (-0.5,2) (x21){$\bullet$};
      \node [on grid] at (0.5,2) (x22){$\bullet$};
      \node [on grid] at (-0.5,3)(x31) {$\bullet$};
      \node [on grid, Diag1] at (0.5,3)(x32) {$\bullet$};
      \node [on grid, Diag1] at (0.5,4)(x41) {$\bullet$};
      \draw[dashed, thick] (-1,2.5) to (1,2.5);
      \begin{scope}[on background layer]
        \draw (x01.center) to (x11.center);
        \draw (x01.center) to (x12.center);
        \draw (x11.center) to (x21.center);
        \draw (x12.center) to (x22.center);
        \draw (x21.center) to (x31.center);
        \draw[very thick,Diag1] (x22.center) to (x32.center);
        \draw[very thick,Diag1] (x32.center) to (x41.center);
      \end{scope}
  \end{tikzpicture}\quad}
  {[1,0,0]}
  {\quad
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base),Diag2]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (0,1) (x11) {$\bullet$};
      \node [on grid] at (0,2) (x21){$\bullet$};
      \node [on grid] at (0,3)(x31) {$\bullet$};
      \node [on grid] at (-0.5,4)(x41) {$\bullet$};
      \node [on grid] at (0.5,4)(x42) {$\bullet$};
      \draw (x01.center) to (x11.center);
      \draw (x11.center) to (x21.center);
      \draw (x21.center) to (x31.center);
      \draw (x31.center) to (x41.center);
      \draw (x31.center) to (x42.center);
  \end{tikzpicture}}
  \qquad = \qquad
  \begin{tikzpicture}[xscale=1.4,every node/.append
    style={scale=0.85},baseline=(x21.base)]
    \node [on grid, Diag2] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (-0.5,1) (x11) {$\bullet$};
    \node [on grid, Diag2] at (0.5,1)(x12){$\bullet$};
    \node [on grid] at (-0.5,2) (x21){$\bullet$};
    \node [on grid, Diag2] at (0.5,2) (x22){$\bullet$};
    \node [on grid] at (-0.5,3)(x31) {$\bullet$};
    \node [on grid, Diag2] at (0.5,3)(x32) {$\bullet$};
    \node [on grid, Diag2] at (0,4)(x41) {$\bullet$};
    \node [on grid, Diag2] at (1,4)(x42) {$\bullet$};
    \draw[dashed, thick] (-1,2.5) to (1,2.5);
    \begin{scope}[on background layer]
      \draw (x01.center) to (x11.center);
      \draw[Diag2] (x01.center) to (x12.center);
      \draw (x11.center) to (x21.center);
      \draw[Diag2] (x12.center) to (x22.center);
      \draw (x21.center) to (x31.center);
      \draw[Diag2] (x22.center) to (x32.center);
      \draw[Diag2] (x32.center) to (x41.center);
      \draw[Diag2] (x32.center) to (x42.center);
    \end{scope}
  \end{tikzpicture}
\]

As well as knowing about the interaction of the boundary with the
inserted tree, we also need to investigate the interaction of the
inclusion maps with the exterior labelling. In this first case, we
would hope to prove that:
\[ \incbd d - S \bullet \kappa_{S,P,T} \equiv \incbd d - {\insertion S P T}\]
Now that \(\bound n {\insertion S P T} \equiv \bound n S\), there are
two ways to encode the source inclusion \(\bound d S\) into
\(\insertion S P T\). The right-hand side of the above equation
directly includes \(\bound d {\insertion S P T}\) into the source of
\(\insertion S P T\), and the left-hand side first includes \(\bound
d S\) into the source of \(S\) and then projects \(S\) onto
\(\insertion S P T\) via the exterior labelling.

There is a catch with proving this equality; The exterior labelling
sends \(\olsi P\) to the standard coherence, and so if \(\incbd d -
S\) has \(\olsi P\) in its image, the equality cannot hold
syntactically. We therefore further require that \(d < \lh(P)\),
which ensures this cannot happen. We now state these results in the
following lemma.

\begin{lemma}
  \label{lem:insertion-bd-1}
  Let \(n \in \mathbb{N}\) and suppose \((S,P,T)\) is an insertion
  point such that \(n \leq \th(T)\). Then:
  \[ \bound n S \equiv \bound n {\insertion S P T}\]
  If we further have \(n < \lh(P)\) then:
  \[ \incbd n \epsilon S \circ \kappa_{S,P,T} \equiv^{\mathsf{max}}
  \incbd n \epsilon {\insertion S P T}\]
  for \(\epsilon \in \{-,+\}\).
\end{lemma}

\begin{proof}
  See the functions
  \func{Catt.Tree.Insertion.Properties}{insertion-bd-1} and
  \func{Catt.Tree.Insertion.Properties}{bd-κ-comm-1} in the
  formalisation module \module{Catt.Tree.Insertion.Properties}.
\end{proof}

We now move to the second case. We again suppose we have an insertion
point \((S,P,T)\) and dimension \(n \in \mathbb{N}\). To perform an
insertion into the boundary \(\bound n S\), the dimension \(n\) must
be high enough not to remove the branch \(P\) from \(S\). More
specifically, we must have the inequality:
\[ n > \bh(P)\]
which ensures that the list \(P\) is still a branch of \(\bound n S\).

\begin{definition}
  Let \(S\) be a tree with a branch \(P\), and let \(n > \bh(P)\).
  Then there is a branch \(\bound n P\) of \(\bound n S\) given by
  the same list as \(P\) with \(\bh(\bound n P) = \bh(P)\).
\end{definition}

As \(\th(\bound n T) \geq \bh(P)\) when \(\th(T) \geq \bh(P)\) and
\(n > \bh(P)\), we are able to insert the tree \(\bound n T\) into
\(\bound n S\) along the branch \(\bound n P\). This is depicted in
the following diagram, where \(\bh(P) = 2\) and \(n = 3\). In this
diagram, the insertion \(\insertion S P T\) is drawn, and dashed
lines are drawn across each tree where they would be truncated by the
boundary operation. Crucially, the branch is still well-formed under
this line, and preforming the insertion on the truncated trees yields
the truncation of the inserted tree.

\[\insertion{
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base)]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (-0.5,1) (x11) {$\bullet$};
      \node [on grid] at (0.5,1)(x12){$\bullet$};
      \node [on grid] at (0.5,2) (x21){$\bullet$};
      \node [on grid, Diag1] at (0,3)(x31) {$\bullet$};
      \node [on grid] at (1,3) (x32) {$\bullet$};
      \node [on grid, Diag1] at (0,4)(x41) {$\bullet$};
      \node [on grid] at (1,4)(x42) {$\bullet$};
      \draw [dashed, thick] (-0.5,3.5) to (1.5,3.5);
      \begin{scope}[on background layer]
        \draw (x01.center) to (x11.center);
        \draw (x01.center) to (x12.center);
        \draw (x12.center) to (x22.center);
        \draw[very thick,Diag1] (x22.center) to (x31.center);
        \draw (x22.center) to (x32.center);
        \draw[very thick,Diag1] (x31.center) to (x41.center);
        \draw (x32.center) to (x42.center);
      \end{scope}
  \end{tikzpicture}\quad}
  {[1,0,0]}
  {\quad
    \begin{tikzpicture}[xscale=1.4,every node/.append
      style={scale=0.85},baseline=(x21.base), Diag2]
      \node [on grid] at (0,0) (x01) {$\bullet$};
      \node [on grid] at (0,1) (x11) {$\bullet$};
      \node [on grid] at (0,2) (x21){$\bullet$};
      \node [on grid] at (-0.5,3)(x31) {$\bullet$};
      \node [on grid] at (0.5,3) (x32) {$\bullet$};
      \node [on grid] at (-0.5,4)(x41) {$\bullet$};
      \draw [dashed, black, thick] (-1,3.5) to (1,3.5);
      \draw (x01.center) to (x11.center);
      \draw (x11.center) to (x21.center);
      \draw (x21.center) to (x31.center);
      \draw (x21.center) to (x32.center);
      \draw (x31.center) to (x41.center);
  \end{tikzpicture}}
  \qquad = \qquad
  \begin{tikzpicture}[xscale=1.4,every node/.append
    style={scale=0.85},baseline=(x21.base)]
    \node [on grid, Diag2] at (0,0) (x01) {$\bullet$};
    \node [on grid] at (-0.5,1) (x11) {$\bullet$};
    \node [on grid, Diag2] at (0.5,1)(x12){$\bullet$};
    \node [on grid, Diag2] at (0.5,2) (x21){$\bullet$};
    \node [on grid, Diag2] at (0,3)(x31) {$\bullet$};
    \node [on grid, Diag2] at (0.5,3) (x32) {$\bullet$};
    \node [on grid] at (1,3) (x33) {$\bullet$};
    \node [on grid, Diag2] at (0,4)(x41) {$\bullet$};
    \node [on grid] at (1,4)(x42) {$\bullet$};
    \draw [dashed,thick] (-0.5,3.5) to (1.5,3.5);
    \begin{scope}[on background layer]
      \draw (x01.center) to (x11.center);
      \draw[Diag2] (x01.center) to (x12.center);
      \draw[Diag2] (x12.center) to (x22.center);
      \draw[Diag2] (x22.center) to (x31.center);
      \draw[Diag2] (x22.center) to (x32.center);
      \draw (x22.center) to (x33.center);
      \draw[Diag2] (x31.center) to (x41.center);
      \draw (x33.center) to (x42.center);
    \end{scope}
  \end{tikzpicture}
\]

As with the previous case, we explore the interaction of the boundary
inclusion labellings and the exterior labelling. We aim to give
conditions under which:
\[ \incbd n - S \bullet \kappa_{S,P,T} \equiv \kappa_{\bound n
S,\bound n P,\bound n T} \bullet \incbd n - {\insertion S P T}\]
We examine the action of each side of the equation on the path
\(\olsi {\bound n P}\). On the right-hand side, this path is sent by
\(\kappa\) to a standard coherence, and so on the left-hand side,
\((\incbd n - S)(\olsi {\bound n P}) \) must also be sent to a
standard coherence by \(\kappa\). If \((\incbd n - S)( \olsi {\bound
n P})\) is a maximal path, which will always be the case when \(n
\geq \lh(P)\), then it will be sent to a standard coherence.
Alternatively, if \(n \leq \lh(P)\) then \(\lh(\bound n P) = n\) and
if \(n > \th(T)\) then the standard term returned by
\(\kappa_{S,P,T}\) will be a standard coherence. These conditions
lead to the following lemma.

\begin{lemma}
  \label{lem:insertion-bd-2}
  Let \(n \in \mathbb{N}\) and suppose \((S,P,T)\) is an insertion
  point with \(n > \bh(P)\). Then:
  \[ \insertion {\bound n S} {\bound n P} {\bound n T} \equiv \bound
  n {\insertion S P T} \]
  Suppose further that one of the following holds:
  \begin{enumerate}
    \item \(n > \th(T)\) and \(n \leq \lh(P)\)
    \item \(n \geq \lh(P)\)
  \end{enumerate}
  Then:
  \[ \incbd n \epsilon S \bullet \kappa_{S,P,T} \equiv^{\mathsf{max}}
    \kappa_{\bound n S,\bound n P,\bound n T} \bullet \incbd n \epsilon
  {\insertion S P T} \]
  for \(\epsilon \in \{-,+\}\).
\end{lemma}
\begin{proof}
  See the functions
  \func{Catt.Tree.Insertion.Properties}{insertion-bd-2} and
  \funcn{Catt.Tree.Insertion.Properties}{50068}{bd-κ-comm-2} in the
  formalisation module \module{Catt.Tree.Insertion.Properties}.
\end{proof}

Both of the further conditions in \cref{lem:insertion-bd-2} imply
that \(n > \bh(P)\). We have therefore seen 3 conditions that can be
put on \(n\), \(P\), and \(T\):
\begin{itemize}
  \item \(n \leq \th(T)\) and \(n < \lh(P)\),
  \item \(n > \th(T)\) and \(n \leq \lh(P)\),
  \item \(n \geq \lh(P)\).
\end{itemize}
One of these conditions must always hold for any \(n\) and insertion
point \((S,P,T)\), and hence one of
\cref{lem:insertion-bd-1,lem:insertion-bd-2} can always be applied.

\begin{remark}
  The further conditions in each of
  \cref{lem:insertion-bd-1,lem:insertion-bd-2} could be dropped in
  favour of weakening the syntactic equalities to definitional
  equalities in a theory with disc removal, as this would remove the
  distinction between standard terms and standard coherences. It was
  however more convenient to take this approach in the formalisation,
  and although the extra side conditions may seem arbitrary, the key
  result is that one of the above lemmas always holds.
\end{remark}

\paragraph{Insertion into standard constructions}

Equipped with \cref{lem:insertion-bd-1,lem:insertion-bd-2}, we can
now prove that the standard constructions are preserved by applying
an exterior labelling up to a definitional equality containing
insertion and disc removal. We begin with the following lemma, whose
intuition is clear from the universal property of insertion.

\begin{lemma}
  \label{lem:kappa-iota-insert}
  Suppose \((S,P,T)\) is an insertion point. Then \(\insertion
  {\kappa_{S,P,T}} P {\iota_{S,P,T}} \equiv \id_{\insertion S P T}\).
\end{lemma}
\begin{proof}
  See \func{Catt.Tree.Insertion.Properties}{κ-ι-prop} in
  \module{Catt.Tree.Insertion.Properties}.
\end{proof}
We can then proceed to the main theorem of this section.
\begin{theorem}
  \label{thm:std-insert-props}
  Let \(\mathcal{R}\) be a tame equality rule set that has disc
  removal and insertion.  Then for any insertion point \((S,P,T)\)
  and \(n \in \mathbb{N}\), we have:
  \[\insertion S P T \vdash_{\mathcal{R}} \stdtm {\bound n S} n \sub
    {\incbd n \epsilon S \bullet \kappa_{S,P,T}} = \stdtm {\bound n
  {\insertion S P T}} n \sub {\incbd n \epsilon {\insertion S P T}}\]
  \[\insertion S P T \vdash_{\mathcal{R}} \stdty S n \sub
  {\kappa_{S,P,T}} = \stdty {\insertion S P T} n\]

  for \(\epsilon \in \{-,+\}\) and if \(n \geq \dep(S)\) then:
  \[\insertion S P T \vdash_{\mathcal{R}} \stdcoh S n \sub
    {\kappa_{S,P,T}} = \stdcoh {\insertion S P T} n \qquad \insertion S
    P T \vdash_{\mathcal{R}}\stdtm S n \sub {\kappa_{S,P,T}} = \stdtm
  {\insertion S P T} n \]
\end{theorem}

\begin{proof}
  We prove all three properties by mutual induction: We begin with the equality:
  \[ \stdtm {\bound n S} n \sub {\incbd n \epsilon S \bullet
    \kappa_{S,P,T}} = \stdtm {\bound n {\insertion S P T}} n \sub
  {\incbd n \epsilon {\insertion S P T}}\]
  The conditions for either
  \cref{lem:insertion-bd-1,lem:insertion-bd-2} must hold, and so we
  treat in case separately. If the conditions for
  \cref{lem:insertion-bd-1} hold then the required equality is
  immediately implied by \(\bound n {\insertion S P T} \equiv \bound
  n S\) and \(\incbd n \epsilon S \bullet \kappa_{S,P,T} \equiv
  \incbd n \epsilon {\insertion S P T}\). If instead the conditions
  for \cref{lem:insertion-bd-2} hold then:
  \begin{align*}
    \stdtm {\bound n S} n \sub {\incbd n \epsilon S \bullet
    \kappa_{S,P,T}} &\equiv \stdtm {\bound n S} n \sub
    {\kappa_{\bound n S, \bound n P, \bound n T} \bullet \incbd n
    \epsilon {\insertion S P T}}\\
    &\equiv \stdtm {\bound n S} n \sub {\kappa_{\bound n S, \bound n
    P, \bound n T}} \sub {\incbd n \epsilon {\insertion S P T}}\\
    &= \stdtm {\insertion {\bound n S} {\bound n P} {\bound n T}} n
    \sub{\incbd n \epsilon {\insertion S P T}}\\
    &\equiv \stdtm {\bound n {\insertion S P T}} n \sub {\incbd n
    \epsilon {\insertion S P T}}
  \end{align*}
  where the definitional equality is due to the inductive hypothesis on terms.

  We continue to the case for types. If \(n = 0\), then both sides of
  the equality are \(\star\). Instead, consider the \(n + 1\) case,
  where we have:
  \begin{alignat*}{3}
    \stdty S {n+1} \sub {\kappa_{S,P,T}} \equiv{} &\stdtm {\bound n
    S} n \sub {\incbd n - S} \sub {\kappa_{S,P,T}} &\qquad& \stdty
    {\insertion S P T} {n+1} \equiv{}&&\stdtm {\bound n {\insertion S
    P T}} n \sub {\incbd n - {\insertion S P T}} \\
    &\to_{\stdty S n \sub {\kappa_{S,P,T}}} &&&&\to_{\stdty
    {\insertion S P T} n}\\
    &\stdtm {\bound n S} n \sub {\incbd n + S} \sub
    {\kappa_{S,P,T}}&&&&\stdtm {\bound n {\insertion S P T}} n \sub
    {\incbd n + {\insertion S P T}}
  \end{alignat*}
  By the inductive hypothesis on \(n\), we have \(\stdty S n \sub
  {\kappa_{S,P,T}} = \stdty {\insertion S P T} n\), and other
  necessary equalities follow from the first case we considered.

  We now consider the case for standard coherences, where we must prove that:
  \[ \SCoh S {\stdty S n} {\kappa_{S,P,T}} = \SCoh {\insertion S P T}
  {\stdty {\insertion S P T} n} {\id} \]
  By \cref{lem:iota-kappa-comm}, \(\olsi P \sub \kappa_{S,P,T}\) is
  the standard coherence \(\stdcoh T {\lh(P)} \sub {\iota_{S,P,T}}\),
  and so the left-hand side of the above equation admits an
  insertion. Therefore:
  \begin{align*}
    \SCoh S {\stdty S n} {\kappa_{S,P,T}} &= \SCoh {\insertion S P T}
    {\stdty S n \sub {\kappa_{S,P,T}}} {\insertion {\kappa_{S,P,T}} P
    {\iota_{S,P,T}}}&\text{by insertion}\\
    &\equiv \SCoh {\insertion S P T} {\stdty S n \sub
    {\kappa_{S,P,T}}} {\id}&\text{by \cref{lem:kappa-iota-insert}}\\
    &= \SCoh {\insertion S P T} {\stdty {\insertion S P T} n}
    {\id}&\text{by inductive hypothesis}\\
    &\equiv \stdcoh {\insertion S P T} n
  \end{align*}
  The equality for standard terms follows from the equality for
  standard coherences, using \cref{thm:std-dr}.
\end{proof}

\begin{corollary}
  \label{cor:standard-coh-insert}
  If \(\mathcal{R}\) has disc removal and insertion, then an
  insertion into a standard coherence is equal to the standard
  coherence over the inserted tree.
\end{corollary}
\begin{proof}
  Let \(s \equiv \stdcoh S n \sub L\) be a standard coherence, and
  suppose \((S,P,T,\U,L,M)\) is an insertion redex with \(\U \vdash s
  : A\) for some \(A\). Then:
  \begin{align*}
    \stdcoh S n \sub L &= \SCoh {\insertion S P T} {\stdty S n \sub
    {\kappa_{S,P,T}}} {\insertion L P M}\\
    &= \SCoh {\insertion S P T} {\stdty {\insertion S P T} n}
    {\insertion L P M}\\
    &= \stdcoh {\insertion S P T} n \sub {\insertion L P M}
  \end{align*}
  and so \(s\) is equal to a standard coherence over the tree
  \(\insertion S P T\).
\end{proof}

\paragraph{Chained insertion}

We explore the situation where a term \(s\) has a locally maximal
argument \(t\) which can be inserted, and this term \(t\) admits an
insertion itself. For the argument \(t\) to be insertable, it must be
a standard coherence, and by \cref{cor:standard-coh-insert}, if \(t =
t'\) by insertion, then \(t'\) will be equal to a standard coherence
over some tree \(T\). For the term \(t'\) to be insertable, \(T\)
must have sufficient trunk height. Conditions for this are given in
the following lemma.

\begin{lemma}
  \label{lem:insert-lin-height}
  Let \((S,P,T)\) be an insertion point. Further, assume \(S\) is not
  linear. Then \(\th(\insertion S P T) \geq \th(S)\).
\end{lemma}
\begin{proof}
  See \func{Catt.Tree.Insertion.Properties}{insertion-trunk-height}
  in \module{Catt.Tree.Insertion.Properties}.
\end{proof}

If a tree \(S\) is not linear, then any branch of \(S\) has branch
height greater than the trunk height of \(S\), and hence any
insertion into \(S\) only modifies the tree above its trunk height,
and so can only increase the trunk height. Therefore, if \((S,P,T)\)
and \(T, Q , U\) are insertion points, and \(T\) is not linear, then
\((S, P, \insertion T Q U)\) is also an insertion point.

Conversely, it is possible to insert the argument directly into the
head term, before performing the inner insertion, looking to perform
the inner insertion afterwards. For this to be possible, a branch of
the inserted tree must be given. This can again be done under a
non-linearity condition.

\begin{definition}
  Let \((S, P, T)\) be an insertion point where \(T\) is not linear.
  Then from a branch \(Q\) of \(T\) we can obtain a branch
  \(\insertion S P Q\) of \(\insertion S P T\). We first observe that
  \(\bh(Q) \geq \th(T) \geq \bh(P)\). We define this branch by
  induction on \(P\) and \(Q\):
  \begin{itemize}
    \item Suppose \(P = [k]\) and \(Q = q :: x\). Then define:
      \[\insertion S P Q = (k - 1 + q) :: x\]
    \item Suppose \(P = k :: P_2\) with \(S = [S_0,\dots,S_n]\) and
      \(T = \Sigma(T_0)\). In this case we must have \(Q = 0 :: Q_2\)
      where \(Q_2\) is a branch of \(T_0\). Then define:
      \[ \insertion S P Q = k :: \insertion {S_k} {P_2} {Q_2}\]
  \end{itemize}
  It is clear that \(\insertion S P Q\) has the same branching and
  leaf height as \(Q\).
\end{definition}

A simple inductive proof shows that:
\[\olsi {\insertion S P Q} \equiv \olsi Q \sub{\iota_{S,P,T}}\]
Now given insertion points \((S,P,T)\) and \((T, Q ,U)\) with \(T\)
non-linear we have that the triple \((\insertion S P T, \insertion S
P Q, U)\) is another insertion point. Therefore, two ways of
performing both insertions, which are depicted in \cref{fig:chained-insertion}.

\begin{figure}[ht]
  \centering
  \newsavebox\chaintop
  \sbox\chaintop{\(\insertion{\insertion{
          \begin{tikzpicture}[xscale=1.4,every node/.append
            style={scale=0.85},baseline=(x21.base)]
            \node [on grid] at (0,0) (x01) {$\bullet$};
            \node [on grid] at (-0.5,1) (x11) {$\bullet$};
            \node [on grid] at (0.5,1)(x12){$\bullet$};
            \node [on grid, Diag1] at (0,2) (x21){$\bullet$};
            \node [on grid] at (1,2) (x22){$\bullet$};
            \node [on grid, Diag1] at (0,3)(x31) {$\bullet$};
            \node [on grid] at (1,3) (x32) {$\bullet$};
            \node [on grid, Diag1] at (0,4)(x41) {$\bullet$};
            \node [on grid] at (1,4)(x42) {$\bullet$};
            \begin{scope}[on background layer]
              \draw (x01.center) to (x11.center);
              \draw (x01.center) to (x12.center);
              \draw[very thick,Diag1] (x12.center) to (x21.center);
              \draw (x12.center) to (x22.center);
              \draw[very thick,Diag1] (x21.center) to (x31.center);
              \draw (x22.center) to (x32.center);
              \draw[very thick,Diag1] (x31.center) to (x41.center);
              \draw (x32.center) to (x42.center);
            \end{scope}
        \end{tikzpicture}\quad}
        {[1,0]}
        {\quad
          \begin{tikzpicture}[xscale=1.4,every node/.append
            style={scale=0.85},baseline=(x21.base), Diag1]
            \node [on grid] at (0,0) (x01) {$\bullet$};
            \node [on grid] at (0,1) (x11) {$\bullet$};
            \node [on grid] at (0,2) (x21){$\bullet$};
            \node [on grid] at (-0.5,3)(x31) {$\bullet$};
            \node [on grid] at (0.5,3) (x32) {$\bullet$};
            \node [on grid, Diag2] at (-0.5,4)(x41) {$\bullet$};
            \node [on grid] at (0.5,4)(x42) {$\bullet$};
            \draw (x01.center) to (x11.center);
            \draw (x11.center) to (x21.center);
            \draw (x21.center) to (x31.center);
            \draw (x21.center) to (x32.center);
            \begin{scope}[on background layer]
              \draw[very thick, Diag2] (x31.center) to (x41.center);
            \end{scope}
            \draw (x32.center) to (x42.center);
      \end{tikzpicture}}\quad}
      {[0,0,0,0]}
      {
        \begin{tikzpicture}[xscale=1.4,every node/.append
          style={scale=0.85},baseline=(x21.base), Diag2]
          \node [on grid] at (0,0) (x01) {$\bullet$};
          \node [on grid] at (0,1) (x11) {$\bullet$};
          \node [on grid] at (0,2) (x21){$\bullet$};
          \node [on grid] at (0,3)(x31) {$\bullet$};
          \node [on grid] at (-0.5,4)(x41) {$\bullet$};
          \node [on grid] at (0.5,4)(x42) {$\bullet$};
          \draw (x01.center) to (x11.center);
          \draw (x11.center) to (x21.center);
          \draw (x21.center) to (x31.center);
          \draw (x31.center) to (x41.center);
          \draw (x31.center) to (x42.center);
  \end{tikzpicture}}\)}
  \newsavebox\chainleft
  \sbox\chainleft{\(\insertion{
        \begin{tikzpicture}[xscale=1.4,every node/.append
          style={scale=0.85},baseline=(x21.base)]
          \node [on grid, Diag1] at (0,0) (x01) {$\bullet$};
          \node [on grid] at (-0.5,1) (x11) {$\bullet$};
          \node [on grid, Diag1] at (0.5,1)(x12){$\bullet$};
          \node [on grid, Diag1] at (0,2) (x21){$\bullet$};
          \node [on grid] at (1,2) (x22){$\bullet$};
          \node [on grid, Diag1] at (-0.33,3)(x31) {$\bullet$};
          \node [on grid, Diag1] at (0.33,3) (x32) {$\bullet$};
          \node [on grid] at (1,3) (x33) {$\bullet$};
          \node [on grid, Diag2] at (-0.33,4)(x41) {$\bullet$};
          \node [on grid, Diag1] at (0.33,4)(x42) {$\bullet$};
          \node [on grid] at (1,4)(x43) {$\bullet$};
          \begin{scope}[on background layer]
            \draw (x01.center) to (x11.center);
            \draw[Diag1] (x01.center) to (x12.center);
            \draw[Diag1] (x12.center) to (x21.center);
            \draw (x12.center) to (x22.center);
            \draw[Diag1] (x21.center) to (x31.center);
            \draw[Diag1] (x21.center) to (x32.center);
            \draw (x22.center) to (x33.center);
            \draw[very thick,Diag2] (x31.center) to (x41.center);
            \draw[Diag1] (x32.center) to (x42.center);
            \draw (x33.center) to (x43.center);
          \end{scope}
      \end{tikzpicture}\quad}
      {[1,0,0,0]}
      {
        \begin{tikzpicture}[xscale=1.4,every node/.append
          style={scale=0.85},baseline=(x21.base), Diag2]
          \node [on grid] at (0,0) (x01) {$\bullet$};
          \node [on grid] at (0,1) (x11) {$\bullet$};
          \node [on grid] at (0,2) (x21){$\bullet$};
          \node [on grid] at (0,3)(x31) {$\bullet$};
          \node [on grid] at (-0.5,4)(x41) {$\bullet$};
          \node [on grid] at (0.5,4)(x42) {$\bullet$};
          \draw (x01.center) to (x11.center);
          \draw (x11.center) to (x21.center);
          \draw (x21.center) to (x31.center);
          \draw (x31.center) to (x41.center);
          \draw (x31.center) to (x42.center);
    \end{tikzpicture}}\)
  }
  \newsavebox\chainright
  \sbox\chainright{\(\insertion{
        \begin{tikzpicture}[xscale=1.4,every node/.append
          style={scale=0.85},baseline=(x21.base)]
          \node [on grid] at (0,0) (x01) {$\bullet$};
          \node [on grid] at (-0.5,1) (x11) {$\bullet$};
          \node [on grid] at (0.5,1)(x12){$\bullet$};
          \node [on grid, Diag1] at (0,2) (x21){$\bullet$};
          \node [on grid] at (1,2) (x22){$\bullet$};
          \node [on grid, Diag1] at (0,3)(x31) {$\bullet$};
          \node [on grid] at (1,3) (x32) {$\bullet$};
          \node [on grid, Diag1] at (0,4)(x41) {$\bullet$};
          \node [on grid] at (1,4)(x42) {$\bullet$};
          \begin{scope}[on background layer]
            \draw (x01.center) to (x11.center);
            \draw (x01.center) to (x12.center);
            \draw[very thick,Diag1] (x12.center) to (x21.center);
            \draw (x12.center) to (x22.center);
            \draw[very thick,Diag1] (x21.center) to (x31.center);
            \draw (x22.center) to (x32.center);
            \draw[very thick,Diag1] (x31.center) to (x41.center);
            \draw (x32.center) to (x42.center);
          \end{scope}
      \end{tikzpicture}\quad}
      {[1,0]}
      {
        \begin{tikzpicture}[xscale=1.4,every node/.append
          style={scale=0.85},baseline=(x21.base), Diag2]
          \node [on grid] at (0,0) (x01) {$\bullet$};
          \node [on grid] at (0,1) (x11) {$\bullet$};
          \node [on grid] at (0,2) (x21){$\bullet$};
          \node [on grid] at (-0.5,3)(x31) {$\bullet$};
          \node [on grid, Diag1] at (0.5,3) (x32) {$\bullet$};
          \node [on grid] at (-0.83,4)(x41) {$\bullet$};
          \node [on grid] at (-0.16,4)(x42) {$\bullet$};
          \node [on grid, Diag1] at (0.5,4)(x43) {$\bullet$};
          \draw (x01.center) to (x11.center);
          \draw (x11.center) to (x21.center);
          \draw (x21.center) to (x31.center);
          \draw (x31.center) to (x41.center);
          \draw (x31.center) to (x42.center);
          \begin{scope}[on background layer]
            \draw[Diag1] (x21.center) to (x32.center);
            \draw[Diag1] (x32.center) to (x43.center);
          \end{scope}
    \end{tikzpicture}}\)
  }
  \newsavebox\chainbot
  \sbox\chainbot{\(
      \begin{tikzpicture}[xscale=1.4,every node/.append
        style={scale=0.85},baseline=(x21.base)]
        \node [on grid, Diag2] at (0.125,0) (x01) {$\bullet$};
        \node [on grid] at (-0.3125,1) (x11) {$\bullet$};
        \node [on grid, Diag2] at (0.5625,1)(x12){$\bullet$};
        \node [on grid, Diag2] at (0.125,2) (x21){$\bullet$};
        \node [on grid] at (1,2) (x22){$\bullet$};
        \node [on grid, Diag2] at (-0.25,3)(x31) {$\bullet$};
        \node [on grid, Diag1] at (0.5,3) (x32) {$\bullet$};
        \node [on grid] at (1,3) (x33) {$\bullet$};
        \node [on grid, Diag2] at (-0.5,4)(x41) {$\bullet$};
        \node [on grid, Diag2] at (0,4)(x42) {$\bullet$};
        \node [on grid, Diag1] at (0.5,4)(x43) {$\bullet$};
        \node [on grid] at (1,4)(x44) {$\bullet$};
        \begin{scope}[on background layer]
          \draw (x01.center) to (x11.center);
          \draw[Diag2] (x01.center) to (x12.center);
          \draw[Diag2] (x12.center) to (x21.center);
          \draw (x12.center) to (x22.center);
          \draw[Diag2] (x21.center) to (x31.center);
          \draw[Diag1] (x21.center) to (x32.center);
          \draw (x22.center) to (x33.center);
          \draw[Diag2] (x31.center) to (x41.center);
          \draw[Diag2] (x31.center) to (x42.center);
          \draw[Diag1] (x32.center) to (x43.center);
          \draw (x33.center) to (x44.center);
        \end{scope}
    \end{tikzpicture}\)
  }

  \begin{tikzpicture}
    \node(chaintop) at (0,0) {\fcolorbox{gray}{white}{\usebox\chaintop}};
    \node(chainleft) at (-4.5,-6){\fcolorbox{gray}{white}{\usebox\chainleft}};
    \node(chainright) at (4.5,-6){\fcolorbox{gray}{white}{\usebox\chainright}};
    \node(chainbot) at (0,-12){\fcolorbox{gray}{white}{\usebox\chainbot}};
    \draw[arrows={->[scale=1.5]},
      line join=round,
      decorate, decoration={
        zigzag,
        segment length=4,
        amplitude=1.2,post=lineto,
        post length=2pt
    }] (chaintop) to (chainleft);
    \draw[arrows={->[scale=1.5]},
      line join=round,
      decorate, decoration={
        zigzag,
        segment length=4,
        amplitude=1.2,post=lineto,
        post length=2pt
    }] (chaintop) to (chainright);
    \draw[arrows={->[scale=1.5]},
      line join=round,
      decorate, decoration={
        zigzag,
        segment length=4,
        amplitude=1.2,post=lineto,
        post length=2pt
    }] (chainleft) to (chainbot);
    \draw[arrows={->[scale=1.5]},
      line join=round,
      decorate, decoration={
        zigzag,
        segment length=4,
        amplitude=1.2,post=lineto,
        post length=2pt
    }] (chainright) to (chainbot);
  \end{tikzpicture}
  \caption{Chained insertion.}
  \label{fig:chained-insertion}
\end{figure}

We now explore the universal property of the insertion along the
branch \(\insertion S P Q\). We assume that \(n = \lh(P)\) and \(m =
\lh(Q)\) and form the following diagram:
\[
  \begin{tikzcd}
    & {D^n} & S \\
    {D^m} & T & {\insertion S P T} \\
    U && {\insertion {(\insertion S P T)} {\insertion S P Q} U}
    \arrow["{\{\olsi P \}}", from=1-2, to=1-3]
    \arrow["{\{ \stdcoh T n \}}"', from=1-2, to=2-2]
    \arrow["{\{ \olsi Q \}}"', from=2-1, to=2-2]
    \arrow["{\{\stdcoh U m\}}"', from=2-1, to=3-1]
    \arrow["{\iota_{S,P,T}}"', from=2-2, to=2-3]
    \arrow["{\kappa_{S,P,T}}", from=1-3, to=2-3]
    \arrow["{\kappa_{\insertion S P T, \insertion S P Q, U}}", from=2-3, to=3-3]
    \arrow["{\iota_{\insertion S P T, \insertion S P Q, U}}"', from=3-1, to=3-3]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=2-3, to=1-2]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=3-3, to=2-1]
  \end{tikzcd}
\]
The top pushout square is given by the insertion of \(T\) into \(S\)
along \(P\). The morphism \(\{\olsi Q\} \bullet \iota_{S,P,T}\)
through the middle of the diagram is then equal to  \(\{\olsi
{\insertion S P Q} \}\), allowing the bottom pushout rectangle to be
formed by the insertion of \(U\) into \(\insertion S P T\) along
\(\insertion S P Q\).

We can also consider the universal property of the tree generated by
first inserting \(U\) into \(T\), and then inserting the inserted
tree into \(S\), which is given by the diagram below:
\[
  \begin{tikzcd}
    & {D^n} && S \\
    {D^m} & T \\
    U & {\insertion T Q U} & & {\insertion S P {(\insertion T Q U)}}
    \arrow["{\{\olsi P \}}", from=1-2, to=1-4]
    \arrow["{\{ \stdcoh T n \}}"', from=1-2, to=2-2]
    \arrow["{\{ \olsi Q \}}"', from=2-1, to=2-2]
    \arrow["{\{\stdcoh U m\}}"', from=2-1, to=3-1]
    \arrow["{\kappa_{T,Q,U}}", from=2-2, to=3-2]
    \arrow["{\iota_{T,Q,U}}"', from=3-1, to=3-2]
    \arrow["{\kappa_{S,P,\insertion T Q U}}", from=1-4, to=3-4]
    \arrow["{\iota_{S,P,\insertion T Q U}}"', from=3-2, to=3-4]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=3-4, to=1-2]
    \arrow["\lrcorner"{anchor=center, pos=0.125, rotate=180,
    scale=1.5}, draw=none, from=3-2, to=2-1]
  \end{tikzcd}
\]
The left-hand pushout square is given by the insertion of \(U\) into
\(T\) along \(Q\). The morphism \(\{\stdcoh T n\} \bullet
\kappa_{T,Q,U}\) which runs vertically through the centre of the
diagram is then equal to \(\{\stdcoh {\insertion T Q U} n\}\) by
\cref{cor:standard-coh-insert}, allowing for the right-hand pushout
square to be formed as the insertion of \(\insertion T Q U\) into
\(S\) along \(P\). By common properties of colimits, both of these
constructions then arise as colimits of the same diagram, the shared
top left boundary of both constructions. The results of this section
are stated in the following lemma.

\begin{lemma}
  \label{lem:inserted-insertion}
  Let \((S,P,T)\) and \((T,Q,U)\) be insertion points. Further assume
  \(T\) is not linear. Then:
  \begin{alignat*}{3}
    &\insertion S P {(\insertion T Q U)} & &= & &\insertion
    {(\insertion S P T)} {\insertion S P Q} U\\
    &\kappa_{S,P,\insertion T Q U} &&=^{\mathsf{max}}{}
    &&\kappa_{S,P,T} \circ \kappa_{\insertion S P T, \insertion S P Q, U}\\
    &\insertion L P {(\insertion M Q N)} &&\equiv^{\mathsf{max}}{}
    &&\insertion {(\insertion L P M)} {\insertion S P Q} N
  \end{alignat*}
  for any \(L : S \to \U\), \(M : T \to \U\), and \(N : U \to \U\).
\end{lemma}
\begin{proof}
  See the functions
  \func{Catt.Tree.Insertion.Properties}{insertion-tree-inserted-branch}
  and
  \func{Catt.Tree.Insertion.Properties}{label-from-inserted-branch}
  in the formalisation module
  \module{Catt.Tree.Insertion.Properties}, and
  \funcn{Catt.Typing.Insertion.Equality}{22361}{κ-inserted-branch} in
  module \module{Catt.Typing.Insertion.Equality}.
\end{proof}

\chapter{Semistrict variants of \Catt}
\label{cha:cattstrict}

The type theories \Cattsu, a type theory for strictly unital
\(\infty\)-categories, and \Cattsua, a type theory for strictly
unital and associative \(\infty\)-categories, are introduced in this
chapter, where we will define both theories and explore some
metatheory and properties of each type theory in detail.

The results in this chapter will heavily depend on the theory
developed in the previous chapters. Both type theories will be
defined as instances of \Cattr, which was introduced in
\cref{sec:catt-with-equality}, and much of the initial metatheory can
be immediately derived by demonstrating that the equality rule sets
that generate \Cattsu and \Cattsua satisfy the various conditions
given in \cref{sec:ruleset}. The theory \Cattsu is primarily
generated by pruning, which was introduced in \cref{sec:pruning}, and
the theory \Cattsua depends on the insertion operation, which was
introduced in \cref{sec:insertion}.

\cref{sec:cattsu} will introduce and define the \Cattsu, and
\cref{sec:cattsua} will do the same for \Cattsua. The main
contribution of these sections is to give normalisation algorithms
for their respective theories, giving a notion of computation to each
theory. A normalisation algorithm is a function \(\N : \Term_\Gamma
\to \Term_\Gamma\) with the following properties:
\begin{itemize}
  \item For any term \(t : \Term_\Gamma\), \(\Gamma \vdash \N(t) = t\).
  \item For any \(s, t : \Term_\Gamma\) with \(\Gamma \vdash s = t\),
    \(\N(s) \equiv \N(t)\).
\end{itemize}
The term \(\N(t)\) is called the \emph{normal form} of \(t\). Such an
algorithm allows equality of two term \(s\) and \(t\) to be decided
by taking the normal form of each term and checking if they are
syntactically equal. Normalisation can be extended to types and
substitutions in a natural way.

In \cref{sec:cattsu,sec:cattsua}, the normalisation algorithm is
defined by giving a reduction system on the syntax of the type
theory, which we show to be terminating, meaning that there is no
infinite reduction sequence and confluent, meaning that any two
reduction paths converge to a common reduct. The normal form of a
term can then be obtained by reducing it until there are no further
reductions possible. In \cref{sec:reduction}, these notions are
recalled, and we demonstrate that the resulting normalisation
algorithm satisfies the two properties stated above. This section
also introduces a method for obtaining a reduction system from an
arbitrary equality rule set \(\mathcal{R}\).

Such a normalisation procedure allows a type checking algorithm to be
implemented, creating an interpreter for the language. This allows us
to write larger terms, and it can be automatically verified whether
they are well-formed. In \cref{sec:towards-nbe}, we introduce our
implementation of \Catt, \Cattsu, and \Cattsua, written in rust. This
implementation supports features such as implicit arguments to terms,
implicit suspension, and native support for trees and tree
labellings. We will explain how the tool can be used, and use it to
give larger examples of \Cattsua terms, including proofs of
Eckmann-Hilton (see \cref{fig:eh}) and its higher-dimensional
coherence condition, the syllepsis.

The implementation uses an approach closer to normalisation by
evaluation for typechecking terms in the theory.
\cref{sec:towards-nbe} explores this algorithm and presents some
perspectives on applying normalisation by evaluation to semistrict
versions of \Catt.

\cref{sec:models} provides a discussion of the models of the
semistrict type theories \Cattsu and \Cattsua, demonstrating how they
can be viewed as semistrict \(\infty\)-categories. The section proves
a partial conservativity result, which allows a proof that
semistrictness is a property of a weak \(\infty\)-category, and not
additional structure. A discussion is provided on some of the
challenges that must be overcome to extend this partial conservativity result.

The thesis ends with \cref{sec:future-work}, which provides a review
of avenues for future work in this area, including a discussion of
further variants of \Catt which could be defined.

\section{Reduction}
\label{sec:reduction}

Reduction is a method for defining computation for a type theory. For
each term, a number of reductions can be applied to it, representing
the various computations that could be applied to the term.
Computation can then be run on a term by repeatedly searching for
positions in the term that admit a reduction, known as
\emph{redexes}, and applying this reduction, until no more redexes
exist in the term. When a term admits no reductions, it is called a
\emph{normal form}.

\begin{definition}
  A \emph{reduction system} is given by a relation \(s \red t\) on
  terms. The relation \(\red^{*}\) is defined to be the reflexive
  transitive closure of \(\red\), and so \(s \red^* t\) exactly when
  there is some chain
  \[s \equiv u_0 \red \cdots \red u_k \equiv t\]
  for \(k \in \mathbb{N}\) (which could be \(0\) with \(s \equiv t\))
  and terms \(u_i\) for \(i \leq k\). Further define
  \(\leftrightsquigarrow\) to be the reflexive symmetric transitive
  closure of \(\red\).

  When a term \(s\) admits no reductions, that is there is no \(t\)
  such that \(s \red t\), we say it is in \emph{normal form}.
\end{definition}

If we have an equality rule set \(\mathcal{R}\) (see
\cref{sec:ruleset}) that generates \Cattr, a reduction system can be
defined on \(\mathcal{R}\) modifying the rules for equality to remove
the reflexivity, symmetry, and transitivity constructors and ensure
that reductions do not happen ``in parallel''.

\begin{definition}
  Let \(\mathcal{R}\) be an equality rule set. Define the reduction
  system \(\redr\) on well-formed terms, well-formed substitutions,
  and well-formed types to be generated by the rules in
  \cref{fig:reduction}. When it is clear which equality rule set is
  being used, we may simply write \(s \red t\) instead of \(s \redr t\).
\end{definition}

\begin{figure}[ht]
  \centering
  \begin{mathpar}
    \inferrule{(\Gamma, s, t) \in \mathcal{R}}{s \redr t}\textsc{rule} \and
    \inferrule{A \redr B}{\Coh \Delta A \sigma \redr \Coh \Delta B
    \sigma}\textsc{cell} \and
    \inferrule{\sigma \redr \tau}{\Coh \Delta A \sigma = \Coh \Delta
    A \tau}\textsc{arg} \\
    \inferrule{s \redr s'}{\arr s A t \redr \arr {s'} A t}\and
    \inferrule{t \redr t'}{\arr s A t \redr \arr s A {t'}}\and
    \inferrule{A \redr A'}{\arr s A t \redr \arr s {A'} t}\\
    \inferrule{\sigma \redr \tau}{\langle \sigma, s \rangle \redr
    \langle \tau, s \rangle}\and
    \inferrule{s \redr t}{\langle \sigma, s \rangle \redr \langle
    \sigma, t \rangle}
  \end{mathpar}
  \caption[Reduction rules]{Rules for \(\rightsquigarrow_{\mathcal{R}}\).}
  \label{fig:reduction}
\end{figure}

The rules for reduction are set up so that each reduction \(s \redr
t\) corresponds to the application of exactly one rule from
\(\mathcal{R}\) at a single point in the term. Given a coherence
\(\Coh \Delta A \sigma\), we call reductions generated by the
\textsc{cell} rule \emph{cell reductions} and reductions generated by
the \textsc{arg} rule \emph{argument reductions}. Reductions
generated by \textsc{rule} will be named by the rule in
\(\mathcal{R}\) that was used. For example a reduction generated by
\textsc{rule} applied with an instance of pruning will be called a
pruning reduction.

We highlight that our reduction system \(\redr\) is only defined
between well-formed pieces of syntax. As this reduction will be used
with rule sets \(\mathcal{R}\) which satisfy the preservation
condition, there will be no additional burden of checking that typing
is preserved while applying reductions. Therefore, we can prove that
the reflexive symmetric transitive closure of reduction, \(\redrts\),
is the same relation as equality on well-formed terms, given the
similarity between the rules for reduction and the rules for equality.

\begin{proposition}
  \label{prop:red-is-eq}
  Let \(\mathcal{R}\) be a rule set satisfying the preservation,
  support, and substitution conditions (such that the generated
  equality preserves typing). Letting \(\redrts\) be the reflexive
  symmetric transitive closure of \(\redr\), we get:
  \begin{align*}
    \Gamma \vdash s = t &\iff s \redrts t \\
    \intertext{for \(s,t : \Term_\Gamma\) such that \(\Gamma \vdash s
    : A\) and \(\Gamma \vdash t : A\) for some \(A : \Type_\Gamma\)}
    \Gamma \vdash A = B &\iff A \redrts B \\
    \intertext{for \(A,B : \Type_\Gamma\) such that \(\Gamma \vdash
    A\) and \(\Gamma \vdash B\)}
    \Gamma \vdash \sigma = \tau &\iff \sigma \redrts \tau
  \end{align*}
  for \(\sigma, \tau : \arr \Delta \star \Gamma\) such that \(\Gamma
  \vdash \sigma : \Delta\) and \(\Gamma \vdash \tau : \Delta\).
\end{proposition}
\begin{proof}
  Each direction can be proved separately by a mutual induction on
  the derivation in the premise. For the right to left direction, it
  suffices to show that the single step reduction (\(\redr\)) is
  contained in the equality, as equality is an equivalence relation
  by construction.
\end{proof}

Just as the preservation condition on a rule set \(\mathcal{R}\)
allows us to deduce that reduction preserves typing, the substitution
condition can be used to prove that reduction is preserved by
application of substitution.

\begin{proposition}
  \label{prop:red-sub}
  Suppose \(\mathcal{R}\) satisfies the substitution condition and
  let \(\sigma : \Delta \to \Gamma\) be a well-formed substitution. Then:
  \begin{align*}
    s \redr t &\implies s \sub \sigma \redr t \sub \sigma \\
    A \redr B &\implies A \sub \sigma \redr B \sub \sigma \\
    \tau \redr \mu &\implies \tau \bullet \sigma \redr \mu \bullet \sigma
  \end{align*}
  for well-formed terms \(s,t\), well-formed types \(A,B\), and
  well-formed substitutions \(\tau\) and \(\mu\). Furthermore, if
  \(\sigma \redr \tau\), then:
  \[ s \sub \sigma \redr^* s \sub \tau \qquad A \sub \sigma \redr^* A
  \sub \tau \qquad \mu \bullet \sigma \redr^* \mu \bullet \tau\]
  for term \(s\), type \(A\), and substitution \(\mu\).
\end{proposition}
\begin{proof}
  The first part by a simple induction on the reduction in the
  premise. The second holds by a mutual induction on the term \(s\),
  type \(A\), and substitution \(\mu\).
\end{proof}

\subsection{Termination}
\label{sec:termination}

In order to obtain a normal form of each term of the theory, we
perform reductions on a term until no more can be applied. This can
only be done if we know that this will eventually result in a normal
form, a property known as \emph{strong termination}.

\begin{definition}
  A reduction system \(\red\) is \emph{strongly terminating} if there
  is no infinite sequence of reductions:
  \[ s_0 \red s_1 \red s_2 \red \cdots \]
  For such a reduction, applying reductions to a term will eventually
  reach a normal form.
\end{definition}

Demonstrating the termination of the reduction systems defined in
\cref{sec:cattsu,sec:cattsua} will be non-trivial, as each reduction
adds new constructions to the term, which could themselves admit
reductions. Suppose we have the following reduction due to
endo-coherence removal (see \cref{sec:ecr}):
\[ \Coh \Delta {\arr s A s} \sigma \red \id(A \sub \sigma,s \sub \sigma) \]
The identity term was not present in the premise of the reduction,
and the term \(s \sub \sigma\) is newly created by the reduction, and
could itself admit any number of reductions.

To prove termination, we will exploit that although each reduction
creates new subterms, these subterms are all of a lower dimension
than the dimension of the term that is being reduced. In the example
above, the dimension of \(\Coh \Delta {\arr s A s} \sigma\) is
greater than the dimension of the term \(s\), and so the reduction
has still made progress towards a normal form by decreasing the
complexity of the term in dimension \(\dim(A)\), even though it may
introduce arbitrary complexity below \(\dim(A)\).

To this end we define the following notion of complexity for each
class of syntax, which assigns an ordinal number to each term, which
we call its \emph{syntactic complexity}. As the ordinal numbers are
well-founded, we aim to prove that our reduction is terminating by
proving that each single-step reduction reduces the complexity of the
term. To define syntactic complexity, we will need to use ordinal
numbers up to \(\omega^\omega\). We will also need a construction
known as the natural sum of ordinals, \(\alpha \+ \beta\), which is
associative, commutative, and strictly monotone in both of its
arguments~\cite{lipparini16}.

\begin{definition}
  For all terms \(t\), types \(A\), and substitutions \(\sigma\), the
  \emph{syntactic complexity} \(\sc(t)\), \(\sc(A)\), and
  \(\sc(\sigma)\) are mutually defined as follows:
  \begin{itemize}
    \item For types:
      \[ \sc(\star) = 0 \qquad \sc(\arr s A t) = \sc(s) \+ \sc(A) \+ \sc(t)\]
    \item For substitutions we have:
      \[\sc(\langle t_0, \dots, t_n \rangle) = \bighash_{i=0}^n t_i\]
    \item For terms, we have \(\sc(x) = 0\) for variables \(x\) and
      for coherences we have:
      \begin{equation*}
        \sc(\Coh \Delta A \sigma) =
        \begin{cases*}
          \omega^{\dim(A)} \+ \sc(\sigma)&if \(\Coh \Delta A \sigma\)
          is an identity\\
          2\omega^{\dim(A)} \+ \sc(\sigma)&otherwise
        \end{cases*}
      \end{equation*}
  \end{itemize}
\end{definition}

The syntactic complexity is given as an ordinal to leverage known
results, though it should be noted that ordinals below
\(\omega^\omega\) can be represented by a list of natural numbers
ordered reverse lexicographically. Under this interpretation the
syntactic complexity effectively computes the number of coherences at
each dimension. Therefore, removing a coherence of dimension \(n\)
reduces the complexity, even if arbitrary complexity is added at
lower dimensions. Syntactic complexity also treats identities in a
special way, as these play a special role in blocking reduction for
the theories presented in this chapter.

The syntactic complexity does not account for the type in a
coherence, as this is difficult to encode. Instead of showing that
all reductions reduce syntactic complexity, we instead show that all
reductions which are not cell reductions (reductions that have the
rule marked \textsc{cell} in their derivation) reduce syntactic
complexity and deduce that a hypothetical infinite reduction sequence
must only consist of cell reductions after a finite number of steps,
and then appeal to an induction on dimension.

\begin{lemma}
  \label{lem:termination-lem}
  Let \(\mathcal{R}\) be an equality set with \( \sc(s) > \sc(t) \)
  for all \((\Gamma,s,t) \in \mathcal{R}\). Then \(\redr\) is
  strongly terminating.
\end{lemma}
\begin{proof}
  By a simple induction on reductions, we immediately have that if
  \(s \redr t\) then \(\sc(s) \geq \sc(t)\), with the inequality
  strict when the reduction is not a cell reduction. We then proceed
  by induction on the dimension.
  Suppose there is an infinite reduction sequence, starting with a
  \(k\)-dimensional term:
  \[ s_0 \red s_1 \red s_2 \red \cdots\]
  Then by assumption, only finitely many of these reductions do not
  use the cell rule, as otherwise we would obtain an infinite chain of ordinals
  \[ \sc(s_0) \geq \sc(s_1) \geq \sc(s_2) \geq \cdots\]
  where infinitely many of these inequalities are strict. Therefore,
  there is an \(n\) such that:
  \[ s_n \red s_{n+1} \red \cdots\]
  are all cell reductions. Each of these reductions reduces one of
  finitely many subterms of \(s_n\), and each of these subterms has
  dimension less than \(k\), so by inductive hypothesis, none of
  these subterms can be reduced infinitely often, contradicting the
  existence of an infinite reduction sequence.
\end{proof}

We can immediately prove that disc removal reduces syntactic complexity.

\begin{proposition}
  \label{prop:disc-rem-sc}
  Let \(s \red t\) be an instance of disc removal. Then \(\sc(s) > \sc(t)\).
\end{proposition}
\begin{proof}
  We must have \(s \equiv \Coh {D^n} {\wk(U^n)} {\{A,t\}}\) for some
  \(n\) and \(A\). Then:
  \begin{align*}
    \sc(s) &= \sc(\Coh {D^n} {\wk(U^n)} {\{A,t\}})\\
    &= 2\omega^n \+ \sc(\{A,t\})\\
    &> \sc(\{A,t\})\\
    &\geq \sc(t)
  \end{align*}
  where the last inequality holds by a simple induction on the
  dimension of \(A\).
\end{proof}

We note that as stated so far the reduction:
\[ \id(A,s) \red \id(A,s)\]
is a valid instance of endo-coherence removal for type \(A\) and term
\(s\), which will break termination. We therefore let \(\ecr'\) be
the equality rule set obtained by removing all triples
\((\Gamma,s,t)\) from \(\ecr\) where \(s\) is already an identity. We
justify replacing \ecr by \ecr' with the following lemma.

\begin{lemma}
  \label{lem:always-ecr}
  The following reduction holds, even when the left-hand side is an identity:
  \[\Coh \Delta {\arr s A s} \sigma \red_{\ecr'}^* \id(A\sub
  \sigma,s\sub\sigma)\]
\end{lemma}
\begin{proof}
  If \(\Coh \Delta {\arr s A s} \sigma\) is not an identity then it
  can be reduced by endo-coherence removal. Otherwise, we have
  \(\Delta = D^n\) for some \(n\), \(s \equiv d_n\), \(A \equiv
  \wk(U^n)\), and \(\sigma \equiv \{B,t\}\) for some \(B\) and \(t\) and so:
  \[\id(A\sub \sigma,s \sub \sigma) \equiv \id(\wk(U^n)\sub{\{B,t\}},
  d_n \sub {\{B,t\}}) \equiv \id(B,t) \]
  It follows that the reduction is trivial.
\end{proof}

It can then be proven that the reductions in this set reduce
syntactic complexity.

\begin{proposition}
  \label{prop:ecr-sc}
  Let \(s \red t\) be an instance of endo-coherence removal. If \(s\)
  is not an identity then \(\sc(s) > \sc(t)\).
\end{proposition}
\begin{proof}
  As \(s \red t\) is an instance of endo-coherence removal, we must
  have \(s \equiv \Coh \Delta {\arr u A u} \sigma\) and \(t \equiv
  \id(A \sub \sigma, u \sub \sigma)\). Further, \(s\) is not an identity and so:
  \begin{align*}
    \sc(s) &= \sc(\Coh \Delta {\arr u A u} \sigma)\\
    &= 2\omega^{\dim(A) + 1} \+ \sc(\sigma)\\
    &\geq 2\omega^{\dim(A) + 1}\\
    &< \omega^{\dim(A) + 1} \+ \sc(A \sub \sigma) \+ \sc(u \sub \sigma)
    &= \sc(\id(A \sub \sigma, u \sub \sigma)) \\
    &= \sc(t)
  \end{align*}
  where the last inequality holds as \(\sc(A \sub \sigma) \+ \sc(u
  \sub \sigma) < \omega^{\dim(A) + 1}\) due to both \(A \sub \sigma\)
  and \(u \sub \sigma\) having the same dimension as \(\dim(A)\),
  meaning that their syntactic complexities are strictly bounded by
  \(\omega^{\dim(A) + 1}\).
\end{proof}

\subsection{Confluence}
\label{sec:confluence}

Another crucial property of reduction systems is \emph{confluence}. A
term \(s\) may have any number of redexes and could reduce to
distinct terms \(t\) and \(u\). Confluence states that both the terms
\(t\) and \(u\) must reduce to some common term, allowing us to apply
reductions to a term in any order.

\begin{definition}
  Let \(\red\) be a reduction system. It is \emph{(globally)
  confluent} if for all terms \(s\),\(t\), and \(u\) with \(s \red^*
  t\) and \(s \red^* u\), there is a term \(v\) such that \(t \red^*
  v\) and \(t \red^* v\). This can be assembled into the following diagram:
  \[
    \begin{tikzcd}
      & s \\
      t && u \\
      & v
      \arrow["{*}", squiggly, from=1-2, to=2-3]
      \arrow["{*}"', squiggly, from=1-2, to=2-1]
      \arrow["{*}"', squiggly, from=2-1, to=3-2]
      \arrow["{*}", squiggly, from=2-3, to=3-2]
    \end{tikzcd}
  \]
  and hence is sometimes called the diamond property for \(\red^*\).
\end{definition}

From global confluence, it is clear that if \(s \redrts t\), where
\(\redrts\) is the reflexive symmetric transitive closure of
\(\redr\), then there is \(u\) with \(s \redr^* u\) and \(t \redr^*
u\). It is sometimes simpler to show that the following weaker
confluence property holds:
\begin{definition}
  Let \(\red\) be a reduction system. It is \emph{locally confluent}
  if given \(s \red t\) and \(s \red u\) there exists a term \(v\) such that:
  \[
    \begin{tikzcd}
      & s \\
      t && u \\
      & v
      \arrow["", squiggly, from=1-2, to=2-3]
      \arrow[""', squiggly, from=1-2, to=2-1]
      \arrow["{*}"', squiggly, from=2-1, to=3-2]
      \arrow["{*}", squiggly, from=2-3, to=3-2]
    \end{tikzcd}
  \]
  that is \(t \red^* v\) and \(u \red^* v\).
\end{definition}

Global confluence trivially implies local confluence. If we further
know that the reduction system \(\red\) is strongly terminating then
local confluence is sufficient to show global confluence.

\begin{lemma}[Newman's lemma \cite{newman1942theories}]
  \label{lem:newman}
  Let \(\red\) be strongly terminating and locally confluent. Then
  \(\red\) is globally confluent.
\end{lemma}

Local confluence for the reduction systems of the type theories
\Cattsu and \Cattsua will be proved using \emph{critical pair
analysis}. A critical pair is a pair of distinct reductions which
apply to the same term. When analysing the critical pairs of our
semistrict type theories, we will encounter terms that are
structurally similar, but differ on lower-dimensional subterms up to
equality. We define this precisely.

\begin{definition}
  Let \(\mathcal{R}\) be an equality rule set. For \(n \in
  \mathbb{N}\), define the \emph{bounded equality set} \(\mathcal{R}_n\) as:
  \[ \mathcal{R}_n = \left\{ (\Gamma, s, t) \in \mathcal{R} \mid
  \dim(s) = \dim(t) < n \right\}\]
  Let the \emph{bounded equality relation} \(s =_n t\) be the
  equality generated by the set \(\mathcal{R}_n\).
\end{definition}

This is used to prove the following lemma, which implies that for a
critical pair \(t \leftsquigarrow s \rightsquigarrow u\) it is not
necessary to find a common reduct of \(t\) and \(u\), but simply find
reducts \(t'\) and \(u'\) of \(t\) and \(u\) such that \(t' =_{\dim(s)} u'\).

\begin{lemma}
  \label{lem:conf-strat}
  Let \(\mathcal{R}\) be a tame equality rule set which satisfies the
  preservation and support conditions, and further assume that
  \(\redr\) is strongly terminating. Suppose the following diagram
  can be formed:
  % https://q.uiver.app/#q=WzAsNixbMiwwLCJzIl0sWzAsMSwidCJdLFs0LDEsInUiXSxbMSwyLCJ0JyJdLFszLDIsInUnIl0sWzIsMiwiPV97XFxkaW0ocyl9Il0sWzAsMiwiIiwwLHsic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoic3F1aWdnbHkifX19XSxbMCwxLCIiLDIseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJzcXVpZ2dseSJ9fX1dLFsxLDMsIioiLDIseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJzcXVpZ2dseSJ9fX1dLFsyLDQsIioiLDAseyJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJzcXVpZ2dseSJ9fX1dXQ== % tex-fmt: skip
  \[
    \begin{tikzcd}[column sep=tiny]
      && s \\
      t &&&& u \\
      & {t'} & {\mathclap{=_{\dim(s)}}} & {u'}
      \arrow[squiggly, from=1-3, to=2-5]
      \arrow[squiggly, from=1-3, to=2-1]
      \arrow["{*}"', squiggly, from=2-1, to=3-2]
      \arrow["{*}", squiggly, from=2-5, to=3-4]
    \end{tikzcd}
  \]
  for all critical pairs \(t \leftsquigarrow_{\mathcal{R}} s \redr
  u\) such that \(s \redr t\) is derived using \textsc{rule}.

  Then \(\redr\) is confluent.
\end{lemma}
\begin{proof}
  By \cref{lem:newman}, it suffices to show local confluence. We
  proceed by strong induction on \(n\) and \(s\), proving that all
  critical pairs \(t \leftsquigarrow_{\mathcal{R}_n} s
  \red_{\mathcal{R}_n} u\) have a common reduct, assuming that all
  critical pairs \(t \leftsquigarrow_{\mathcal{R}_m} s'
  \red_{\mathcal{R}_m} u\) have a common reduct, where \(s'\) is a
  subterm of \(s\) or \(m < n\). We justify this induction principle
  by noting that for all subterms \(s'\) of \(s\) we have \(\dim(s')
  \leq \dim(s)\).

  We now consider critical pair \(t \leftsquigarrow_{\mathcal{R}_n} s
  \red_{\mathcal{R}_n} u\). We first suppose that \(s
  \red_{\mathcal{R}_n} t\) is derived from \textsc{rule}. Then, by
  definition of the set \(\mathcal{R}_n\), we must have that \(n >
  \dim(s)\). By the assumption of the lemma, there exist \(t'\) and
  \(u'\) with \(t' =_{\dim(s)} u'\) and \(t \redr^* t'\) and \(u
  \redr^* u'\). As \(n > \dim(s)\), we further have that \(t
  \red_{\mathcal{R}_n}^* t'\) and \(u \red_{\mathcal{R}_n}^* u'\).

  By \cref{prop:red-is-eq}, \(t'
  \leftrightsquigarrow_{\mathcal{R}_{\dim(s)}} u'\), and so as
  \(\red_{\mathcal{R}_{\dim(s)}}\) is confluent by inductive
  hypothesis on dimension we have \(v\) such that \(t'
    \red_{\mathcal{R}_{\dim(s)}}^* v
  \leftsquigarrow_{\mathcal{R}_{\dim(s)}}^* u'\). The following
  diagram can therefore be formed, where all the reductions are
  \(\mathcal{R}_n\) reductions (noting that \(\mathcal{R}_{\dim(s)}
  \subseteq \mathcal{R}_n\)):
  \[
    \begin{tikzcd}[column sep=tiny]
      && s \\
      t &&&& u \\
      & {t'} && {u'} \\
      && v
      \arrow[squiggly, from=1-3, to=2-5]
      \arrow[squiggly, from=1-3, to=2-1]
      \arrow["{*}"', squiggly, from=2-1, to=3-2]
      \arrow["{*}", squiggly, from=2-5, to=3-4]
      \arrow["{*}"', squiggly, from=3-2, to=4-3]
      \arrow["{*}", squiggly, from=3-4, to=4-3]
    \end{tikzcd}
  \]

  If \(s \red_{\mathcal{R}_n} u\) was derived from \textsc{rule},
  then finding a reduct can be found similarly to the first case by
  symmetry. We therefore consider the cases where neither \(s \red
  t\) nor \(s \red u\) are derived using \textsc{rule}. Both
  reductions must be either cell or argument reductions, and so each
  reduces some subterm of \(s\). If they reduce distinct subterms of
  \(s\), then a common reduct \(v\) can be formed by applying both
  reductions to \(s\). Otherwise, both reductions act on the same
  subterm of \(s\), and a common reduct can be found by applying the
  inductive hypothesis for subterms.
\end{proof}

Once termination and confluence have been proven, a normalisation
function can be defined, which repeatedly applies reductions until no
more can be applied.

\begin{lemma}
  Suppose that \(\red\) is strongly terminating and confluent. Then
  every term \(s\) reduces to a unique normal form \(\N(s)\).
  Furthermore, if \(s \redrts t\), then \(\N(s) \equiv \N(t)\).
\end{lemma}
\begin{proof}
  By termination, repeatedly reducing a term will reach a normal
  form. Suppose a term \(s\) has two normal forms \(t\) and \(u\)
  such that there are reduction sequences \(s \red^* t\) and \(s
  \red^* u\). Then by confluence there must be a term \(v\) with \(t
  \red^* v\) and \(u \red^* u\). However, \(t\) and \(u\) are normal
  forms and so admit no reductions, so \(t \equiv v \equiv u\) as required.

  Suppose \(s \redrts t\). Then there are terms \(s_i\) such that:
  \[ s \equiv s_0 \rightsquigarrow^* s_1 \leftsquigarrow^* s_2
  \rightsquigarrow^* \cdots \leftsquigarrow^* s_k \equiv t\]
  Now we must have \(\N(s_i) \equiv \N(s_{i+1})\) for each \(i\) as
  if \(s_i \rightsquigarrow^* s_{i+1}\) then both \(\N(s_i)\) and
  \(\N(s_{i+1})\) are normal forms of \(s_i\) and if \(s_i
  \leftsquigarrow^* s_{i+1}\) then both are normal forms of
  \(s_{i+1}\). Therefore, \(\N(s)\) and \(\N(t)\) are syntactically
  equal as required.
\end{proof}
\begin{corollary}
  Let \(\mathcal{R}\) be tame and satisfy the preservation and
  support properties. Further, suppose that \(\redr\) is strongly
  terminating and confluent, and it is decidable whether a term
  admits a reduction. Then the equality \(s = t\) is decidable.
\end{corollary}
\begin{proof}
  By \cref{prop:red-is-eq}, \(s = t\) if and only if \(s \redrts t\).
  By the above lemma, \(s \redrts t\) if and only if \(\N(s) \equiv
  \N(t)\). As syntactic equality is clearly decidable, and normal
  forms can be computed, equality is also decidable.
\end{proof}

We note that for an arbitrary rule set \(\mathcal{R}\), it may not be
decidable whether a specific term \(s\) admits a reduction, but for
the rule sets introduced in \cref{sec:cattsu,sec:cattsua}, it will be
easy to mechanically check whether any reduction applies to a term \(s\).

\section{\texorpdfstring{\Cattsu}{Cattsu}}
\label{sec:cattsu}

We are ready to define \Cattsu, the type theory for strictly unital
\(\infty\)-categories. \Cattsu is a variant of \Cattr for which the
equality is built from three classes of equalities:
\begin{itemize}
  \item Pruning: The pruning operation was introduced in
    \cref{sec:pruning}. Pruning is the key operation in \Cattsu and
    drives the strict unitality of the theory. The operation
    ``prunes'' identities that appear as locally maximal arguments to
    other terms, simplifying the overall structure of a term by
    removing unnecessary units.
  \item Endo-coherence removal: This operation was introduced in
    \cref{sec:ecr}, and converts ``fake identities'', terms which are
    morally identities yet have the wrong syntactic form, into true
    identities. These converted identities can then be further
    removed from terms by pruning.
  \item Disc removal: Disc removal was the running example from
    \cref{sec:catt-with-equality}, and removes unary composites from
    the theory. Commonly after pruning, a composite is reduced to a
    unary composite, for which disc removal is necessary to complete
    the simplification of the term.
\end{itemize}
In this section we will prove that \Cattsu is a type theory
satisfying many standard meta-theoretic properties by combining
results from previous chapters. We also give a reduction system for
\Cattsu and show that this is strongly terminating and confluent.

\begin{example}
  Suppose we have terms \(f : \arr x \star y\), \(g : \arr y \star
  z\), \(h : \arr x \star z\), and \(\alpha : f * g \to h\) in some
  context \(\Gamma\). We can then consider the term:
  \[ \Coh {(x : *), (y : *), (f : \arr x \star y), (z : \star), (g :
  \arr y \star z)} {f * g \to f * g} {\langle x,y,f,z, g \rangle} * \alpha\]
  which consists of an endo-coherence composed with the variable
  \(\alpha\). This then reduces as follows:
  \begin{align*}
    &\phantom{{}\red{}}\Coh {(x : *), (y : *), (f : \arr x \star y),
    (z : \star), (g : \arr y \star z)} {f * g \to f * g} {\langle f,
    g \rangle} * \alpha\\
    &\red \id(\arr x \star z, f * g) * \alpha&\text{by endo-coherence removal}\\
    &\red \Coh {D^2} {\wk(U^2)} {\langle x,z,f*g,h,\alpha
    \rangle}&\text{by pruning}\\
    &\red \alpha &\text{by disc removal}
  \end{align*}
  and so uses all three reductions to fully simplify to a variable.
\end{example}

We define \Cattsu by the following equality rule set.

\begin{definition}
  Define the equality rule set \su for \Cattsu by:
  \[ \su = \dr \cup \prune \cup \ecr\]
  \Cattsu is then the variant of \Cattr where \(\mathcal{R} = \su\).
\end{definition}

When it is not specified, we will assume that the operation set
\(\mathcal{O}\) is given by the regular operation set \(\Reg\).

\begin{theorem}
  The rule set \su is tame and satisfies the support and preservation
  conditions.
\end{theorem}
\begin{proof}
  By \cref{prop:dr-weak,prop:dr-susp,prop:dr-sub}, disc removal
  satisfies the weakening, suspension, and \(\su\)-substitution
  conditions. Endo-coherence removal and pruning satisfy the same
  conditions by \cref{prop:ecr-props,prop:prune-tame}. As these
  conditions are closed under unions, the set \su must also satisfy
  the weakening, suspension, and substitution conditions, and hence is tame.

  We now use the proof strategy introduced in
  \cref{sec:further-conditions} to prove that \su satisfies the
  support condition. Firstly, by \cref{lem:supp-sat-conds} we know
  that \(\su_{\mathsf{s}}\) is also tame. Disc removal then satisfies
  the \(\su_{\mathsf{s}}\)-support condition by \cref{prop:dr-supp}.
  The same condition is satisfied by endo-coherence removal
  (\cref{item:ecr-supp}) and pruning (\cref{prop:prune-supp}) and so
  \(\su\) satisfies the \(\su_{\mathsf{s}}\)-support condition. By
  \cref{lem:proof-strat-supp}, \su satisfies the support condition.

  Lastly, \su satisfies the \su-preservation condition as it is
  satisfied by disc removal (\cref{prop:dr-preserve}), endo-coherence
  removal (\cref{item:ecr-preserve}), and pruning
  (\cref{prop:prune-preserve}) and is closed under unions of rule sets.
\end{proof}

From this theorem it can be deduced that weakening, suspension, and
applications of substitution are well-formed. Furthermore, equality
in \Cattsu preserves the support of a term and preserves typing
judgements. Such results are found in \cref{sec:ruleset}.

Before giving normalisation results for \Cattsu, we recall the
Eckmann-Hilton argument (\cref{fig:eh}) and give the definition of
the term giving this equivalence. First let \(\Delta\) be the
ps-context given by:
\begin{alignat*}{2}
  \Delta = D^2 \wedge D^2 ={} &(x : *),\\
  &(y : *),&\ &(f : x \to y),\\
  &&&(g : x \to y),(a : f \to g),\\
  &(z : *),&&(h : x \to y),\\
  &&&(j : x \to y),(b : h \to j)
\end{alignat*}
which is given by the diagram:
\[
  \begin{tikzcd}
    \bullet & \bullet & \bullet
    \arrow[""{name=0, anchor=center, inner sep=0}, "g",
    curve={height=-18pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
    curve={height=18pt}, from=1-1, to=1-2]
    \arrow[""{name=2, anchor=center, inner sep=0}, "h"',
    curve={height=18pt}, from=1-2, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0}, "i",
    curve={height=-18pt}, from=1-2, to=1-3]
    \arrow["a"', shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow["b"', shorten <=5pt, shorten >=5pt, Rightarrow, from=2, to=3]
  \end{tikzcd}
\]
The following term can be formed, which is similar to an interchange
move, and changes the order in which two whiskered terms are composed:
\[ \mathsf{swap} = \Coh {\Delta} {(a *_0 j) *_1 (g *_0 b) \to (f *_0
b) *_1 (a *_0 h)} {\id_\Delta}\]
Then given a context \(\Gamma\) with terms \(x : *\) and
\(\alpha,\beta : \id(x) \to \id(x)\), the following term, the
Eckmann-Hilton term, can be formed:
\[ \mathsf{EH}_{\alpha,\beta} = \mathsf{swap} \sub{\langle
x,x,\id(x),\id(x),\alpha, x,\id(x),\id(x),\beta \rangle}\]
In \Cattsu, this term can be typed as follows:
\[ \Gamma \vdash \mathsf{EH}_{\alpha,\beta} : \alpha *_1 \beta \to
\beta *_1 \alpha\]
and so witnesses the Eckmann-Hilton argument.

We note that there is a clear inverse of the Eckmann Hilton term,
which immediately gives rise to two morphisms \(\alpha *_1 \beta \to
\beta *_1 \alpha\): the original term \(\mathsf{EH}_{\alpha,\beta}\)
and the term \(\mathsf{EH}_{\beta,\alpha}^{-1}\). These two terms
manoeuvre  \(\alpha\) and \(\beta\) round each other in opposite
directions, and are not in general equivalent.

However, we can instead apply Eckmann-Hilton to terms \(\phi\) and
\(\psi\) of type \(\id^2(x) \to \id^2(x)\), which is done by
suspending the Eckmann-Hilton term. By an abuse of notation we define
this term to be (only giving the locally maximal arguments of the substitution):
\[\mathsf{EH}_{\phi,\psi} = \Sigma(\mathsf{swap}) \sub{\langle \phi,
\psi \rangle}\]
In this case, the extra dimension gives enough freedom to give an
equivalence between the resulting two terms \(\phi *_2 \psi \to \psi
*_2 \phi\) which is called the \emph{syllepsis} and has the type:
\[ \mathsf{Syl}_{\phi,\psi} : \mathsf{EH}_{\phi,\psi} \to
\mathsf{EH}^{-1}_{\psi,\phi}\]
To define this term, a similar approach to the approach used for
Eckmann-Hilton of giving a single coherence containing a more complex
type and a substitution containing multiple identity terms, and
letting the \Cattsu reduction simplify the type to the required one.
We delay defining this term until \cref{sec:towards-nbe}, where the
implementation presented in this section can be used to check that
the resulting term is well-formed.

\subsection{Normalisation for \texorpdfstring{\Cattsu}{Cattsu}}
\label{sec:reduction-cattsu}

Following \cref{sec:reduction} we aim to give a normalisation
algorithm for \Cattsu by exhibiting a strongly terminating and
confluent reduction system.

The reduction system \(\red_{\su}\) cannot be used directly because
the reduction generated from \ecr is not terminating, as it allows
identities to reduce to identities. Even after replacing the equality
rule set \ecr by \ecr', the equality set obtained by removing these
trivial identity to identity reductions from \ecr, the generated
reduction is still non-terminating. Consider the term \(\id(\arr t A
t,\id(A,t))\) for some term \(t\) of type \(A\). Then the following
reduction sequence can be formed:
\[ \id(\arr t A t,\id(A,t)) \red \Coh {D^n} {\id(\wk(U^n), d_n) \to
\id(\wk(U^n), d_n)} {\{A,t\}} \red \id(\arr t A t, \id(A,t)) \]
where \(n = \dim(A)\), the first reduction is by pruning, and the
second reduction is by endo-coherence removal. We therefore choose to
also restrict the pruning equality rule set to not apply when the
head term is an identity, obtaining the set \prune'. We can now
define the reduction system for \Cattsu.

\begin{definition}
  Define the reduction \(\red_{\su'}\) to be the reduction generated
  by the equality rule set \(\su'\) where
  \[ \su' = \dr \cup \prune' \cup \ecr'\]
  where \ecr' is the endo-coherence removal set without identity to
  identity equalities and \prune' is the pruning set restricted to
  the triples where the left-hand side term is not an identity.
\end{definition}

The reduction \(\red_{\su'}\) applies equality rules from \Cattsu
when the redex is not an identity, effectively forcing identities to
be normal forms of the theory. As applying a substitution to or
suspending a non-identity term cannot result in an identity, it is
clear that \su' is tame. Strong termination for \(\red_{\su'}\) can
now be proven using \cref{lem:termination-lem}, by showing that all
rules reduce the syntactic complexity of terms.

\begin{proposition}
  Let \(s \red t\) be an instance of pruning. If \(s\) is not an
  identity then \(\sc(s) > \sc(t)\).
\end{proposition}
\begin{proof}
  The reduction \(s \red t\) is an instance of pruning, and so there
  must be Dyck word \(\mathcal{D} : \Dyck_0\), and peak \(p :
  \Peak_{\mathcal{D}}\) such that
  \[s \equiv \Coh {\lfloor \mathcal{D} \rfloor} {A} {\sigma} \qquad t
    \equiv \Coh {\lfloor \mathcal{D} \sslash p \rfloor} {A \sub
  {\pi_p}} {\sigma \sslash p}\]
  where \(s\) is not an identity and \(\lfloor p \rfloor \sub
  \sigma\) is. We then have \(\sc(s) = \sc(\sigma)\) and \(\sc(t) =
  \sc(\sigma \sslash p)\), but \(\sigma \sslash p\) is simply
  \(\sigma\) with two terms removed, one of which is known to be a
  coherence, and so \(\sc(s) > \sc(t)\).
\end{proof}
\begin{corollary}
  The reduction \(\red_{\su'}\) is strongly terminating.
\end{corollary}
\begin{proof}
  By \cref{lem:termination-lem}, it suffices to show that each rule
  of \(\su'\) reduces syntactic complexity, which follows from the
  preceding proposition and \cref{prop:ecr-sc,prop:disc-rem-sc}.
\end{proof}

By \cref{prop:red-is-eq}, we know that the reflexive symmetric
transitive closure of \(\red_{\su'}\) is the equality relation
generated by \su'. We therefore prove that this agrees with the
equality relation from \Cattsu.

\begin{proposition}
  \label{prop:suprime-equiv}
  The type theories generated from \su and \su' are equivalent. Terms
  are equal or well-formed in one theory exactly when they are equal
  or well-formed in the other, and similar properties hold for types
  and substitutions.
\end{proposition}
\begin{proof}
  We use \cref{lem:subset-lem} for both directions. Since \(\su'
  \subseteq \su\), we are only required to show that if \((\Gamma, s,
  t) \in \su\) with \(\Gamma \vdash_{\su'} s : A\) for some \(A :
  \Type_\Gamma\) then
  \[ \Gamma \vdash_{\su'} s = t\]
  If \((\Gamma,s,t) \in \su'\), then the equality follows from the
  \textsc{rule} constructor. Otherwise, \(s\) must be an identity and
  the rule is an instance of endo-coherence removal or pruning.
  Suppose \(s\) reduces to \(t\) by endo-coherence removal. Then \(s
  \equiv \id(A,u)\) and
  \[t \equiv \id(\wk(U^n) \sub {\{A,u\}}, d_n \sub {\{A,u\}}) \equiv
  \id(A,u) \equiv s\]
  and so the equality holds by reflexivity.

  Now assume \(s\) reduces by pruning to \(t\). Letting \(s \equiv
  \id(A,u)\) and \(n = \dim(A)\), we get:
  \begin{align*}
    t &\equiv \Coh {\lfloor \mathcal{D}^n \sslash p^n \rfloor} {\arr
    {d_n} {\wk(U^n)} {d_n} \sub {\pi_{p^n}}} {\{A,u\} \sslash p} \\
    &= \id(\wk(U^n) \sub {\pi_{p^n}} \sub {\{A,u\} \sslash p^n}, d_n
    \sub {\pi_{p^n}} \sub {\{A,u\} \sslash p^n})&\text{by
    endo-coherence removal}\\
    &\equiv \id(\wk(U^n),d_n) \sub {\pi_{p^n} \bullet \{A,u\} \sslash p^n}\\
    &= \id(\wk(U^n),d_n) \sub {\{A,u\}}&\text{by \cref{prop:prune-ty}}\\
    &\equiv \id(\wk(U^n) \sub {\{A,u\}}, d_n \sub {\{A,u\}})\\
    &\equiv \id(A,u)
  \end{align*}
  and so the equality holds as required.
\end{proof}

We therefore have that two terms \(s\) and \(t\) are equal in \Cattsu
if and only if \(s \leftrightsquigarrow_{\su'} t\). To demonstrate
normalisation, it therefore remains to show that the reduction system
is confluent, for which we employ the strategy introduced in
\cref{lem:conf-strat}.

\begin{theorem}
  \label{thm:su-conf}
  The reduction \(\red_{\su'}\) is confluent.
\end{theorem}
\begin{proof}
  By \cref{lem:conf-strat} it is sufficient to show that for all \(t
  \leftsquigarrow s \rightsquigarrow u\) with \(s \rightsquigarrow
  t\) being a reduction derived from \textsc{rule}, that the
  following diagram can be formed:
  \[
    \begin{tikzcd}[column sep=tiny]
      && s \\
      t &&&& u \\
      & {t'} & {\mathclap{=_{\dim(s)}}} & {u'}
      \arrow[squiggly, from=1-3, to=2-5]
      \arrow[squiggly, from=1-3, to=2-1]
      \arrow["{*}"', squiggly, from=2-1, to=3-2]
      \arrow["{*}", squiggly, from=2-5, to=3-4]
    \end{tikzcd}
  \]
  We therefore begin by case splitting on the reduction \(s \red t\),
  ignoring cases where both reductions are identical and ignoring
  cases which follow by symmetry of other cases.

  \textbf{Disc removal:} Suppose \(s \red t\) is a disc removal
  reduction. Then \(s \equiv \Coh {D^n} {\wk(U^n)} {\{A,t\}}\). We
  now split on the reduction \(s \red u\). We immediately know that
  \(s \red u\) cannot be an endo-coherence removal reduction, as
  \(s\) is not an endo-coherence. It also cannot be a cell reduction
  as \(\wk(U^n)\) only contains variables and so is in normal form.

  Let \(s \red u\) be an argument reduction. It must therefore be
  generated by a reduction on \(\{A,t\}\). If it is a reduction
  generated by \(A \red A'\) then \(u \red t\) by endo-coherence
  removal and so we are done. Otherwise, it is generated by \(t \red
  t'\) and so \(t\) and \(u\) both reduce by disc removal to \(t'\).

  The only remaining case is where \(s \red u\) is an instance of
  pruning, which forces \(t \equiv \id(B,a)\) for some \(B\) and
  \(a\). As \(s\) is well-formed, we must have \(n > 0\) and so \(A
  \equiv \arr b {A'} c\). Therefore:
  \begin{align*}
    u &\equiv \Coh {\lfloor \mathcal{D}^{n} \sslash p \rfloor}
    {\wk(U^n) \sub {\pi_{p}}} {\{A,\id(B,a)\} \sslash p}\\
    &\equiv \Coh {D^{n-1}} {\wk(U^n) \sub {\{\arr {d_{n-1}}
    {\wk(U^{n-1})} {d_{n-1}}, \id(\wk(U^{n-1}), d_{n-1})\} }}
    {\{A',b\}}&\text{by \cref{prop:prune-disc}}\\
    &\equiv \Coh {D^{n-1}} {\arr {d_{n-1}} {\wk(U^{n-1})} {d_{n-1}}}
    {\{A',b\}}\\
    &\equiv \id(A',b)
  \end{align*}
  Now as \(s\) is well-formed we have \(\Gamma \vdash \{A,\id(B,a)\}
  : D^n\) and so by \cref{lem:disc-typing}, we have \(\Gamma \vdash
  \id(B,a) : A\) and hence by \cref{cor:id-typing} and uniqueness of typing:
  \[ \arr a {B} a = A \equiv \arr b {A'} c\]
  and so \(a = b\) and \(B = A'\) and hence \(s \equiv \id(A', b) =
  \id(B,a) \equiv t\). Since \(\dim(a) = \dim(B) < \dim(s)\), we get
  \(t =_{\dim(s)} u\) as required.

  \textbf{Endo coherence removal:} Suppose \(s \red t\) is an
  endo-coherence removal reduction. Then:
  \[ s \equiv \Coh {\Delta} {\arr a A a} {\sigma} \red \id(A \sub
  \sigma, a \sub \sigma) \equiv t\]
  with \(s\) not being an identity.
  We now split on the reduction \(s \red u\). First consider when it
  is an argument reduction generated by \(\sigma \red \tau\). Then by
  \cref{prop:red-sub}, we have \(t \equiv \id(A \sub \sigma, a \sub
  \sigma) \red^* \id(A \sub \tau, a \sub \tau)\). By endo-coherence
  removal, \(u \red \id(A \sub \tau, a \sub \tau)\), completing this case.

  Now suppose the reduction \(s \red u\) is an instance of cell
  reduction. If it is generated from a reduction \(A \red B\) then by
  \cref{prop:red-sub}, \(t \red \id(B \sub \sigma, a \sub \sigma)\)
  and by endo-coherence removal:
  \[u \equiv \Coh \Delta {\arr a B a} \sigma \red \id(B \sub \sigma,
  a \sub \sigma)\]
  We now consider when the reduction is generated by \(\arr a A a
  \red \arr b A a\), with the case where it is generated by \(\arr a
  A a \red \arr a A b\) following symmetrically. We consider the
  reductions sequence  from \(u\):
  \begin{align*}
    u &\equiv \Coh \Delta {\arr b A a} {\sigma} \\
    &\red \Coh \Delta {\arr b A b} \sigma &\text{by cell reduction}\\
    &\red \id(A\sub \sigma,b\sub \sigma) &\text{by endo-coherence removal}
  \end{align*}
  Again by \cref{prop:red-sub}, \(t \equiv
  \id(A\sub\sigma,a\sub\sigma) \red \id(A\sub\sigma,b\sub\sigma)\),
  completing the case.

  Lastly, we consider when \(s \red u\) is a pruning reduction. We
  suppose \(\Delta = \lfloor \mathcal{D} \rfloor\) and that the
  pruning is generated from peak \(p : \mathcal{D}\). Then:
  \[ u \equiv \Coh {\lfloor \mathcal{D} \sslash p \rfloor} {(\arr a A
  a) \sub {\pi_p}} {\sigma \sslash p}\]
  Then:
  \begin{align*}
    u &\red \id(A \sub {\pi_p} \sub {\sigma\sslash p},a \sub {\pi_p}
    \sub {\sigma\sslash p})&\text{by \cref{lem:always-ecr}}\\
    &\equiv \id(A,a) \sub {\pi_p \bullet \sigma \sslash p} \\
    &=_{\dim(s)} \id(A,a) \sub \sigma
  \end{align*}
  where the last (bounded) equality is by \cref{prop:prune-ty} and by
  noting that \(\dim(A) = \dim(a) < \dim(s)\).

  \textbf{Pruning:} Let \(s \red t\) be a reduction by pruning with
  \[ s \equiv \Coh {\lfloor \mathcal{D} \rfloor} A \sigma\]
  for some \(\mathcal{D} : \Dyck_0\) with peak \(p :
  \Peak_{\mathcal{D}}\) such that \(\lfloor p \rfloor \sub \sigma\)
  is an identity. Then:
  \[ t \equiv \Coh {\lfloor \mathcal{D} \sslash p \rfloor} {A \sub
  {\pi_p}} {\sigma \sslash p}\]
  We now split on the reduction \(s \red u\). First suppose it is
  given by an argument reduction \(\sigma \red \tau\). Identities do
  not admit head reductions, meaning \(\lfloor p \rfloor \sub \tau\)
  is still an identity. Therefore, pruning can be applied to \(u\) to get:
  \[ u \red \Coh {\lfloor \mathcal{D} \sslash p \rfloor} {A \sub
  {\pi_p}} {\tau \sslash p}\]
  Now \(\sigma \sslash p\) is simply \(\sigma\) with two terms
  removed, and so \(\sigma \sslash p \red^* \tau \sslash p\), meaning
  \(t\) reduces to the same term as \(u\).

  If \(s \red u\) is a cell reduction \(A \red B\), then pruning can
  be applied to \(u\) immediately to get the term:
  \[\Coh {\lfloor \mathcal{D} \sslash p \rfloor} {B \sub {\pi_p}}
  {\sigma \sslash p}\]
  but \(t\) also reduces to this term by \cref{prop:red-sub}.

  Let \(s \red u\) be a second pruning reduction, at a different peak
  \(q : \Peak_{\mathcal{D}}\). By \cref{prop:prune-conf}, there is a
  common reduct:
  \[ \Coh {\lfloor (\mathcal{D} \sslash p) \sslash q_p \rfloor} {A
  \sub {\pi_p} \sub{\pi_{q_p}}} {(\sigma \sslash p) \sslash q_p} \]
  which both reduce to by pruning if \(\lfloor q_p \rfloor\) and
  \(\lfloor p_q \rfloor\) are identities. However:
  \[\lfloor q_p \rfloor \equiv \lfloor q \rfloor \sub {\pi_p}\]
  and \(\lfloor q \rfloor\) must be an identity for \(s \red u\) to
  be a valid instance of pruning. Therefore, as identities are
  preserved by application of substitution, \(\lfloor  q_p \rfloor\)
  is an identity. Similarly, \(\lfloor p_q \rfloor\) is an identity,
  and so both \(t\) and \(u\) reduce to the term above.

  Any remaining cases follow by symmetry, completing the proof.
\end{proof}

\subsection{Disc trivialisation}
\label{sec:properties-cattsu}

We take a brief moment to explore the theory \Cattsu in its entirety.
For this section we will further assume that we take the set of
operations \(\mathcal{O}\) to be the regular operations.

We begin by proving a property of terms over disc contexts, which we
call \emph{disc trivialisation}. This is the following structure
theorem: in a disc context \(D^n\), every term is either a variable,
or the iterated identity on a variable, up to definitional equality.
Restricting to those terms \(t : \Term_{D^n}\) that are full, that is
\(\Supp(t) = \Var{D^n}\), then there is exactly one term (up to
definitional equality) at each dimension \(k \geq n\). Hence, the
type theory \Cattsu trivialises disc contexts.

This property relates the type theory \Cattsu to the definition of
strictly unital \(\infty\)-categories of
\citeauthor{Batanin2013}~\cite{Batanin2013}, whose \emph{reduced
operads} enforce that there is a unique term of each dimension over a
linear tree.

We now state and prove disc trivialisation, recalling the definition
of an iterated canonical identity from \cref{def:canonical-id}.

\begin{theorem}[Disc trivialisation]
  \label{thm:disc-triv}
  Suppose \(D^n \vdash t : A\) in \Cattsu. Then \(t\) is equal to an
  iterated canonical identity on a variable, that is \(t = \id^k(x)\)
  for some variable \(x \in \Var(D^n)\) and \(k \in \mathbb{N}\).
\end{theorem}
\begin{proof}
  Without loss of generality, we may assume that \(t\) is in \Cattsu
  normal form, and proceed to prove that \(t\) is an iterated
  canonical identity. We proceed by induction on subterms of the term
  \(t\). If \(t\) is a variable then we are done. Otherwise, we
  assume \(t\) is a coherence term \(\Coh \Delta U \sigma\).

  We now show that \(\Delta\) must be a disc context by
  contradiction. We therefore assume that \(\Delta\) is not a disc,
  and hence \(t\) is not an identity. By induction on subterms, we
  must have that each term in \(\sigma\) is an iterated canonical
  identity on a variable. No locally maximal argument can be an
  identity, as otherwise pruning could be performed and \(t\) would
  not be in normal form, and so every locally maximal argument is a
  variable. Suppose there is some variable \(x\) such that \(x
  \sub\sigma\) is an identity, and let \(x\) be a variable of maximal
  dimension with this property. As \(x\) cannot be locally maximal,
  there must either be the source or target of a variable \(y\), but
  this variable \(y\) must be sent to a variable of \(D^n\), which
  cannot have an identity as its source or target. Therefore, the
  substitution \(\sigma\) is variable to variable.

  We now let \(\Gamma\) be the smallest ps-context prefix of
  \(\Delta\) such that \(\Gamma\) is not a disc. We must have:
  \[ \Gamma \equiv D^k, (y : A), (f : \arr x A y)\]
  where \(D^k \vdash_{\mathsf{ps}} x : A\). Furthermore, the last
  rule used in this derivation must be \textsc{psd}, as if it were
  \textsc{pse} or \textsc{pss} then \(k = \dim(A)\) and \(\Gamma
  \equiv D^{k+1}\), breaking the assumption that \(\Gamma\) is not a
  disc. Therefore, \(D^k \vdash_{\mathsf{ps}} g : \arr w A x\) for
  some variables \(g\) and \(x\). However, now \(g \sub \sigma\), \(x
  \sub \sigma\), and \(f \sub \sigma\) are variables of \(D^n\) such
  that \(\tgt(g \sub \sigma) \equiv x \sub \sigma \equiv \src(f \sub
  \sigma)\). No such variables exist in \(D^n\) and so we reach a
  contradiction. We therefore deduce that \(\Delta\) is a disc
  \(D^n\) for some \(n\).

  Now \(t \equiv \Coh {D^n} {\arr u A v} \sigma\) and so by induction
  on subterms, \(u\) and \(v\) are equal to iterated canonical
  identities. We now split on whether \(t\) is a composition or
  equivalence. If it is a composition then \(\Supp(u) = \bdry {n-1} -
  {D^n}\) and \(\Supp(v) = \bdry {n-1} + {D^n}\) and therefore
  neither \(u\) or \(v\) are identities (as then \(A\) would have the
  same support as \(u\) or \(v\) respectively) and so \(u =
  d_{n-1}^-\) and \(v = d_{n-1}^*\), but this makes \(t\) a disc
  removal redex, and so \(t\) is not in normal form.

  We therefore assume that \(t\) is an equivalence and \(u\) and
  \(v\) are full. Then \(u\) and \(v\) must be iterated identities on
  \(d_n\), and must have the same dimension and so are syntactically
  equal. To avoid \(t\) being an endo-coherence removal redex, it
  must be an identity \(\id(B,s)\). Now, \(s \equiv \id^k(x)\) for
  some variable \(x\) (as \(s\) is a subterm of \(t\)), and so if \(k
  = 0\) then \(\ty(s) \equiv d_{n-1}^- \to d_{n-1}^+\) and if \(k >
  0\) then \(\ty(s) \equiv \id^{k-1}(x) \to \id^{k-1}(x)\). In either
  case, \(\ty(s)\) is in normal form, and so since \(B\) is also a
  normal form and \(\Gamma \vdash s : B\) (by the well-typing of
  \(t\) and \cref{cor:id-typing}), we have \(B \equiv \ty(s)\) and so
  \(t \equiv \id(s) \equiv \id^{k+1}(x)\) as required.
\end{proof}

Disc trivialisation allows us to prove the following results
concerning terms and substitutions in pasting diagrams.

\begin{theorem}
  Let \(\mathcal{D}\) be a Dyck word. Let \(t\) be a well-formed
  \Cattsu term of \(\lfloor \mathcal{D} \rfloor\). Then \(\Supp(t)\)
  is a ps-context.
\end{theorem}
\begin{proof}
  Suppose, for contradiction, that we have a Dyck word
  \(\mathcal{D}\) and a term \(t\) where \(\Supp(t)\) is not a
  ps-context. Assume further that \(\mathcal{D}\) is minimal (in
  terms of length) where such a term exists.

  Immediately, \(\mathcal{D} \not\equiv \circleddash\), as all terms
  have non-empty support. We now examine the locally maximal
  variables of \(\mathcal{D}\). There must exist some locally maximal
  variable \(f : x \to y\) such that \(f \not\in \Supp(t)\), as
  otherwise \(\Supp(t) = \Var(\lfloor \mathcal{D} \rfloor)\).

  Now suppose that \(y \not\in\Supp(t)\). Then we let \(p\) be the
  peak corresponding to \(f\) and consider the term:
  \[t \sub {\pi_p} : \Term_{\lfloor \mathcal{D}\sslash p \rfloor}\]
  Then \(\Supp(t \sub {\pi_p}) = \Supp(t)\), which contradicts the
  minimality of \(\mathcal{D}\). By a similar argument, \(x\) must
  also be in \(\Supp(t)\). It is also the case that if such a
  variable \(f : x \to y\) with \(f \not\in \Supp(t)\) and \(\{x,y\}
  \subseteq \Supp(t)\) exists, then \(\Supp(t)\) cannot be a
  ps-context, by an argument involving the linear order on
  ps-contexts introduced by \citeauthor{finster2017type}~\cite{finster2017type}.

  Now suppose \(\mathcal{D}\) has a peak \(p\) that is not \(f\).
  Then \(f\sub{\pi_p} : x \sub{\pi_p} \to y \sub{\pi_p}\) is a
  locally maximal variable of \(\lfloor \mathcal{D} \sslash p
  \rfloor\) with \(f\sub{\pi_p} \not\in \Supp(t \sub {\pi_p})\) and
  \(\{x \sub {\pi_p}, y \sub {\pi_p}\} \subseteq \Supp(t \sub
  {\pi_p})\). Hence, \(\Supp(t \sub {\pi_p})\) is not a ps-context,
  again breaking the minimality of \(\mathcal{D}\).

  Therefore, \(\mathcal{D}\) has one peak, and so \(\lfloor
  \mathcal{D} \rfloor \equiv D^n\) for some \(n\). Now by
  \cref{thm:disc-triv}, \(t\) is \Cattsu equal to a variable \(z\) or
  an iterated identity on a variable \(z\). Since \Cattsu preserves
  support, we must have \(\Supp(t) = \Supp(z)\), but \(\Supp(z)\) is
  a disc and so is a ps-context.

  Hence, no such term \(t\) existed.
\end{proof}

Since any \Catt term is also a \Cattsu term, we get the following corollary.

\begin{corollary}
  \label{cor:supp-ps}
  If \(\Gamma \vdash t : A\) in \Catt, and \(\Gamma\) is a
  ps-context, then \(\Supp(t)\) is a ps-context.
\end{corollary}

\section{\texorpdfstring{\Cattsua}{Cattsua}}
\label{sec:cattsua}

We now move on to defining \Cattsua, the type theory for strictly
unital and associative \(\infty\)-categories. \Cattsua extends
\Cattsu by replacing the pruning equality with the more general
insertion equality, which was introduced in \cref{sec:insertion}.
Under certain conditions, insertion can merge more complex terms into
a single coherence. As an example, the term \((f * g) * h\), which is
a composite which has a composite as one of its arguments, is reduced
by insertion to the ternary composite \(f*g*h\), reducing the depth of the term.

As we did for \Cattsu, we will prove in this section that \Cattsua
satisfies standard meta-theoretic properties, and provide a reduction
system for it which is strongly terminating and confluent.

\begin{example}
  We consider the associator term, and its reductions in \Cattsua.
  The associator witnesses the associativity law in a weak
  \(\infty\)-category. Letting \(\Delta\) be the following ps-context:
  \begin{alignat*}{2}
    \Delta = \lfloor [\emp,\emp,\emp] \rfloor ={}& (w : *)\\
    &(x : *)&&(f : w \to x)\\
    &(y : *)&&(g : x \to y)\\
    &(z : *)&&(h : y \to z)
  \end{alignat*}
  we can define the associator as:
  \[ \alpha = \Coh \Delta {(f * g) * h \to f * (g * h)} {\id_\Delta}\]
  This then admits the following reduction sequence in \Cattsua:
  \begin{align*}
    \alpha &\rightsquigarrow \Coh \Delta {f*g*h \to f * (g * h)}
    {\id_\Delta}&\text{by insertion}\\
    &\rightsquigarrow \Coh \Delta {f * g * h \to f * g * h}
    {\id_\Delta}&\text{by insertion}\\
    &\rightsquigarrow \id(f*g*h) &\text{by endo-coherence removal}
  \end{align*}
\end{example}

We formally define \Cattsua as the version of \Cattr generated by the
rule set \sua, which we define below:

\begin{definition}
  We define the equality rule set \sua for \Cattsua by:
  \[ \sua = \dr \cup \ecr \cup \insert \]
  \Cattsua is then the variant of \Cattr where \(\mathcal{R} = \sua\).
\end{definition}

As before, when we do not specify an operation set, it should be
assumed that the regular operation set is used. When we use the
groupoidal operation set, we refer to the resulting theory as
\emph{groupoidal \Cattsua}.

\begin{theorem}
  \label{thm:sua-conds}
  The rule set \sua is tame and satisfies the support condition. If
  \(\mathcal{O}\) supports insertion, then \sua also satisfies the
  preservation condition.
\end{theorem}
\begin{proof}
  By
  \cref{prop:dr-weak,prop:dr-susp,prop:dr-sub,prop:ecr-props,prop:insert-tame},
  each of the disc removal, endo-coherence removal, and insertion
  sets satisfy the weakening, suspension, and \(\sua\)-substitution
  conditions. It follows that \(\sua\) satisfies the weakening,
  suspension, and substitution conditions. Hence, \sua is tame.

  To prove that the support condition holds for \sua, we use the
  strategy introduced in \cref{sec:further-conditions} and instead
  show that \sua satisfies the \(\sua_{\mathsf{S}}\)-support
  condition. By \cref{lem:supp-sat-conds}, the equality rule set
  \(\sua_{\mathsf{S}}\), the restriction of \sua to support
  preserving equalities, is also tame. As it trivially satisfies the
  support condition, we have by
  \cref{prop:dr-supp,item:ecr-supp,prop:insert-supp} that disc
  removal, endo-coherence removal, and insertion satisfy the
  \(\sua_{\mathsf{S}}\)-support condition. Therefore, \sua satisfies
  the \(\sua_{\mathsf{S}}\)-support condition and so by
  \cref{lem:proof-strat-supp} \sua satisfies the support condition.

  The \sua-preservation condition is satisfied by disc removal (by
  \cref{prop:dr-preserve}) and endo-coherence removal (by
  \cref{item:ecr-preserve}). If \(\mathcal{O}\) supports insertion,
  then insertion also satisfies the \sua-preservation condition by
  \cref{prop:insert-preserve}. Therefore, \sua satisfies the
  preservation condition, completing the proof.
\end{proof}

While the groupoidal operation set trivially supports insertion, we
have not yet proven that the regular operation set, \Reg, supports
insertion. This is done now using \cref{thm:sua-conds}.

\begin{proposition}
  The regular operation set, \Reg, supports insertion.
\end{proposition}
\begin{proof}
  Using that the regular operation set is equal to the standard
  operation set, we instead prove that the standard operation set
  supports insertion. For this it will be sufficient to prove that
  for an insertion point \((S, P, T)\), dimension \(n \in
  \mathbb{N}\) and \(\epsilon \in \{-,+\}\) that:
  \[ \bdry n \epsilon S \sub {\kappa_{S,P,T}} = \bdry n \epsilon
  {\insertion S P T}\]
  Then:
  \begin{align*}
    \bdry n \epsilon S \sub {\kappa_{S,P,T}} &= \Supp(\stdtm {\bound
    n S} n \sub {\incbd n \epsilon S}) \sub {\kappa_{S,P,T}}&\text{by
    \cref{lem:std-supp}}\\
    &= \Supp(\stdtm {\bound n S} n \sub {\incbd n \epsilon S \bullet
    \kappa_{S,P,T}})\\
    &= \Supp(\stdtm {\bound n {\insertion S P T}} n \sub {\incbd n
    \epsilon {\insertion S P T}})&\text{by (*)}\\
    &= \bdry n \epsilon {\insertion S P T} &\text{by \cref{lem:std-supp}}
  \end{align*}
  where the equality \((*)\) holds as \sua satisfies the support
  condition by \cref{thm:sua-conds} and:
  \[ \insertion S P T \vdash_\sua \stdtm {\bound n S} n \sub {\incbd
    n \epsilon S \bullet \kappa_{S,P,T}} = \stdtm {\bound n {\insertion
  S P T}} n \sub {\incbd n \epsilon {\insertion S P T}} \]
  by \cref{thm:std-insert-props}.
\end{proof}

\subsection{Reduction for \texorpdfstring{\Cattsua}{Cattsua}}
\label{sec:norm-cattsua}

Using the results of \cref{sec:reduction}, we give a normalisation
algorithm for \Cattsua by defining a reduction system which generates
the equality relation and proving that this reduction system is
strongly terminating and confluent.

As with \Cattsu, we cannot directly use the reduction \(\red_\sua\)
directly, as we have seen already that the reduction \(\red_\ecr\)
alone is non-terminating. Similarly to pruning, allowing insertions
into identity terms also creates non-terminating loops of reductions
when combined with endo-coherence removal, as was explained in
\cref{sec:reduction-cattsu}. We therefore restrict our reduction so
that no head-reductions can be applied to identity terms.

Although these restrictions are sufficient to ensure termination, we
choose to further restrict the set of insertion reductions, in order
to streamline the proof of confluence. Firstly, we only allow
insertions of a locally maximal argument when that argument is either
an identity or a standard composition. The motivation for this
restriction is that identities and standard compositions are the only
standard coherences that are in normal form. Moreover, not allowing
the insertion of endo-coherences avoids a difficult
insertion/argument endo-coherence removal confluence case.

We also disallow insertions into a unary composite and insertions of
a unary composite, as we have already seen in
\cref{sec:further-properties} that discs act as a left and right unit
for insertion, and so these two insertion reductions are subsumed by
disc removal. Further, disallowing the insertion of discs removes
another case where an insertable standard coherence is not in normal
form. We now define the resulting reduction system.

\begin{definition}
  Define the reduction \(\red_{\sua'}\) to be the reduction generated
  by the equality rule set \(\sua'\) where:
  \[\sua' = \dr \cup \ecr' \cup \insert'\]
  where \(\ecr'\) is the endo-coherence removal set without the
  identity-to-identity reductions, and \(\insert'\) is the insertion
  rule set restricted to insertion redexes \((S,P,T,\Gamma,L,M)\) and
  types \(A\) such that \(\SCoh S A L\) is not an identity or a unary
  composite, and \(L(\olsi P) \equiv \stdcoh T {\lh(P)} \sub M\) is
  an identity or a standard composite which is not a unary composite.
\end{definition}

It can be determined by a simple observation that \(\sua'\) is tame,
as suspension and the application of substitution cannot transform a
term into an identity or unary composite where it wasn't before. We
further justify the restrictions made to insertion by showing that
many insertion reductions can still be performed, starting with the
following technical lemma.

\begin{lemma}
  \label{lem:insertion-change-max}
  If \(P\) is a branch of \(S\), and \(L, L' : S \to \Gamma\) are
  labellings differing only on \(\olsi P\), then the following holds
  for insertion redex \((S,P,T,\Gamma,L,M)\):
  \[\insertion L P M \equiv \insertion {L'} P M\]
\end{lemma}
\begin{proof}
  By inspection of the definition, \(\insertion L P M\) does not use
  the term \(L(\olsi P)\).
\end{proof}

We now show that many insertion reductions can still be simulated up
to bounded equality.

\begin{lemma}
  \label{lem:insertable}
  Let \((S,P,T,\Gamma, L, M)\) be an insertion redex. Further suppose
  that \(a \equiv \SCoh S A L\) is not an identity or disc. Then
  there exists a term \(s\) with:
  \[a \red_{\sua'}^* s =_{\dim(a)} \SCoh {\insertion S P T} {A \sub
  {\kappa_{S,P,T}}} {\insertion L P M}\]
  even when \(L(\olsi P)\) is a unary composite or is not a standard
  composite or identity.
\end{lemma}

\begin{proof}
  We proceed by induction on \(\lh(P) - \dep(T)\). If \(\lh(P) -
  \dep(T) = 0\) then \(\stdcoh T {\lh(P)}\) is a composite. The only
  case for which insertion cannot be performed is when \(\stdcoh T
  {\lh(P)}\) is a unary composite, such that \(T = D^{\lh(P)}\). Now
  by \cref{lem:disc-insertion-2}, \(\insertion S P T \equiv S\),
  \(\insertion L P M \equiv^{\max} L\) and \(\kappa_{S,P,T} = \id_S\) and so
  \[a  =_{\dim(a)} \SCoh {\insertion S P T} {A \sub {\kappa_{S,P,T}}}
  {\insertion L P M}\]
  We now assume that \(\lh(P) > \dim(T)\). We may also assume without
  loss of generality that \(\stdcoh T {\lh(P)}\) is not an identity,
  as otherwise it would be immediately insertable. This allows us to
  perform endo-coherence removal to get:
  \[\stdcoh T {\lh(P)} \red \id(\stdty T {\lh(P) - 1}, \stdtm T
  {\lh(P)- 1}) \sub M\]
  Now suppose \(b \equiv \Coh S A {L'}\) where \(L'\) is the result
  of applying the above reduction to the term of \(L\) corresponding
  to \(\olsi P\). Since \(L'(\olsi P)\) is now an identity it can be
  inserted to get \(b \red c\) where:
  \begin{align*}
    c &\equiv \SCoh {S \sslash P} {A \sub {\pi_P}} {\insertion {L'} P
    {(\{\stdtm T {\lh(P) - 1}\} \bullet M)}}\\
    &\equiv \SCoh {S \sslash P} {A \sub {\pi_P}} {\insertion {L'} P
    {(\{\stdcoh T {\lh(P) - 1}\} \bullet M)}}
  \end{align*}
  where \(\stdtm T {\lh(P - 1)} \equiv \stdcoh T {\lh(P - 1)}\) as if
  \(\stdtm T {\lh(P)-1}\) was a variable then \(\stdcoh T {\lh(P)}\)
  would be an identity.

  We now wish to show that \(2 + \bh(P) \leq \lh(P)\) so that \(P'\)
  exists as a branch of \(S \sslash P\). Since we always have \(1 +
  \bh(P) \leq \lh(P)\), we consider the case where \(1 + \bh(P) =
  \lh(P)\). We know that \(\bh(P) \leq \dep(T) \leq \lh(P)\) and so
  one of these inequalities must be an equality. If \(\dep(T) =
  \lh(P)\) then \(\stdcoh T {\lh(P)}\) is a standard composite. If
  \(\dep(T) = \bh(P)\) then \(\th(T) = \dep(T)\) and so \(T\) is
  linear. However, this makes \(\stdcoh T {\lh(P)}\) an identity.
  Either case is a contradiction and so \(2 + \bh(P) \leq \lh(P)\)
  and so \(P'\) is a branch of \(S \sslash P\).

  By \cref{lem:pruned-bp,lem:iota-kappa-comm}, we now have:
  \begin{align*}
    &\phantom{{}\equiv{}}\olsi {P'} \sub {\insertion{L'} P
    {(\{\stdcoh T {\lh(P) - 1}\} \bullet M)}} \\
    &\equiv d_{\lh(P) - 1} \sub {\iota_{S,P,D^{\lh(P) - 1}} \bullet
    (\insertion {L'} P {(\{\stdcoh T {\lh(P) - 1}\} \bullet M)})} \\
    &\equiv d_{\lh(P) - 1} \sub {\{\stdcoh T {\lh(P) - 1}\} \bullet M} \\
    &\equiv \stdcoh T {\lh(P) - 1}\sub M
  \end{align*}
  As \(\lh(P') - \dim(T) = \lh(P) - \dim(T) - 1\) we can use the
  induction hypothesis to get that \(c \leadsto d\) and:
  \begin{align*}
    d =_{\dim(a)}{} &\SCoh {\insertion {(S \sslash P)} {P'} T} {A
    \sub {\pi_P \bullet \kappa_{S\sslash P,P',T}}} {\\&\insertion
    {(\insertion {L'} P {(\{\stdcoh T {\lh(P) - 1}\} \bullet M)})} {P'} {M}}
  \end{align*}
  By \cref{lem:pruned-bp,lem:insertion-change-max},
  \begin{equation*}
    d =_{\dim(a)} \SCoh {\insertion S P T} {A \sub {\kappa_{S,P,T}}}
    {\insertion L P M}
  \end{equation*}
  which completes the proof as \(a \leadsto^* d\).
\end{proof}

We further show that insertions into discs can be simulated by disc removal.

\begin{lemma}
  \label{lem:disc-insertion-red}
  Let \((D^n,P,T,\Gamma,L,M)\) be an insertion redex and let \(a
  \equiv \stdcoh {D^n} n \sub L\). Then:
  \[ a \red_{\sua'} s =_{n} \SCoh {\insertion {D^n} P T} {\stdty
  {D^n} n \sub \kappa} {\insertion L P M}\]
\end{lemma}
\begin{proof}
  We have the equality:
  \begin{align*}
    \SCoh {\insertion {D^n} P T} {\stdty {D^n} n \sub \kappa}
    {\insertion L P M} &\equiv \SCoh T {\stdty {D^n} n \sub
    {\kappa_{D^n,P,T}}} M&\text{ \cref{lem:disc-insertion-1}}\\
    &=_n \SCoh T {\stdty T n} M&\text{by \cref{thm:std-insert-props}}\\
    &\equiv \stdcoh T n \sub M\\
    &\equiv L(\olsi P)
  \end{align*}
  Therefore, the reduction \(a \red s \equiv L(\olsi P)\) is given by
  disc removal.
\end{proof}

Using these lemmas, we now show that the type theories \Cattsua and
\(\Catt_{\sua'}\) are equivalent.

\begin{proposition}
  The type theories generated by \sua and \sua' are equivalent.
  Terms, types, and substitutions are equal or well-formed in one
  theory exactly when they are equal or well-formed in the other.
\end{proposition}
\begin{proof}
  Both directions proceed by \cref{lem:subset-lem}. Since \(\sua'
  \subseteq \sua\), it suffices to show that if \((\Gamma,s,t) \in
  \sua\) with \(\Gamma \vdash_{\sua'} s : A\) for some type \(A\) then:
  \[ \Gamma \vdash_{\sua'} s = t\]
  If \((\Gamma,s,t) \in \sua'\), then there is nothing to do. If it
  is in \(\ecr'\), then the argument is the same as in the proof of
  \cref{prop:suprime-equiv}. We therefore assume \((\Gamma,s,t) \in
  \insert\), and so there must be some insertion redex
  \((S,P,T,\Gamma,L,M)\) such that \(s \equiv \lfloor \SCoh S B L \rfloor\) and
  \[ t \equiv \lfloor  \SCoh {\insertion S P T} {B \sub
  {\kappa_{S,P,T}}} {\insertion L P M} \rfloor \]
  By an induction on dimension, we assume that the theories generated
  by \sua and \(\sua'\) are already equivalent for terms of dimension
  less than \(\dim(s)\). We begin a case analysis of such reductions
  than are not in \insert. If \(s\) is an identity, then \(B \equiv b
  \to b\) for some term \(b\) and so \(t\) is an endo-coherence. If
  \(t\) is already an identity, then \(s \equiv t\). Otherwise:
  \begin{align*}
    \Gamma \vdash_{\sua'} t &= \id(b \sub {\kappa_{S,P,T}}) \sub
    {\insertion L P M}\\
    &\equiv \id(b) \sub {\kappa_{S,P,T} \bullet (\insertion L P M)}\\
    &= \id(b) \sub L\\
    &\equiv s
  \end{align*}
  where the first equality is by endo-coherence removal, and the
  second equality is by \cref{lem:ins-comm-max}, appealing to the
  induction on dimension.

  If \(s\) is a unary composite we apply
  \cref{lem:disc-insertion-red} and use the inductive hypothesis on
  dimension. Otherwise, we are done by \cref{lem:insertable} and the
  inductive hypothesis on dimension.
\end{proof}

Having shown that the reflexive symmetric transitive closure of the
reduction \(\red_{\sua'}\) agrees with the equality of \Cattsua, we
move on to showing that this reduction is strongly terminating. To do
this we appeal to \cref{lem:termination-lem}, and show that all
reductions reduce the syntactic complexity of the terms involved.

\begin{lemma}
  \label{lem:insert-sc-prop}
  The following inequality holds for any insertion redex \((S,P,T,\Gamma,L,M)\):
  \[\sc(\insertion L P M) < \sc(L)\]
\end{lemma}
\begin{proof}
  We extend the notion of syntactic depth to labellings in the
  obvious way. We begin by noting that:
  \begin{align*}
    \sc(L) &= \left(\bighash_{p \neq \olsi P} \sc(L(p))\right) \+
    \sc(L(\olsi P))\\
    &= \left(\bighash_{p \neq \olsi P} \sc(L(p))\right) \+
    \sc(\stdcoh T {\lh(P)} \sub M)\\
    &> \left(\bighash_{p\neq \olsi P} \sc(L(p))\right) \+ \sc(M)
  \end{align*}
  Further, we show that for all labels \(L\) and \(M\) with
  appropriate conditions that:
  \[\sc(\insertion L P M) \leq \bighash_{p\neq \olsi P} \sc(L(p)) \+ \sc(M) \]
  which we do by induction on \(P\). If \(P = [k]\) then it is clear
  that \(\insertion L P M\) contains all the terms of \(M\) and some
  terms of \(L\), and crucially not \(L(\olsi P)\). If instead \(P =
  k :: P_2\) then by induction hypothesis we get that:
  \[\sc(\insertion {L_k} {P_2} {M_0}) \leq \bighash_{p\neq \olsi
  {P_2}} \sc(L_k(p)) \+ \sc(M_1)\]
  It is then clear again that \(\insertion L P M\) contains terms
  from \(M\) and terms of \(L\) which are not \(L(\olsi P)\), and so
  the inequality holds.
\end{proof}

We can now show that insertion reductions reduce syntactic complexity.

\begin{proposition}
  \label{prop:insert-sc}
  Let \(s \red t\) be an instance of insertion. If \(s\) is not an
  identity then \(\sc(s) > \sc(t)\).
\end{proposition}
\begin{proof}
  Let \((S,P,T,\Gamma,L,M)\) be an insertion redex so that:
  \[\SCoh S A L \red \SCoh {\insertion S P T} {A \sub \kappa}
  {\insertion L P M}\]
  by insertion. By assumption \(\Coh S A L\) is not an identity. Then:
  \begin{align*}
    \sc(t) &= \sc(\SCoh {\insertion S P T} {A \sub \kappa} {\insertion L P M})\\
    &\leq 2\omega^{\dim(A)} \+ \sc(\insertion L P M)\\
    &< 2\omega^{\dim(A)} \+ \sc(L) &\text{by \cref{lem:insert-sc-prop}}\\
    &\leq \SCoh S A L\\
    &= \sc(s)
  \end{align*}
  and so \(\sc(s) > \sc(t)\), completing the proof.
\end{proof}

\begin{corollary}
  The reduction system \(\redr\) is strongly terminating.
\end{corollary}
\begin{proof}
  By \cref{lem:termination-lem}, it suffices to show that each rule
  of \(\sua'\) reduces syntactic complexity, which follows from
  \cref{prop:disc-rem-sc,prop:ecr-sc,prop:insert-sc}.
\end{proof}
\subsection{Confluence of \texorpdfstring{\Cattsua}{Cattsua}}
\label{sec:confluence-cattsua}

In this section, we prove the following theorem:

\begin{theorem}
  \label{thm:sua-conf}
  The reduction \(\red_{\sua'}\) is confluent.
\end{theorem}

The confluence proof for \Cattsua is significantly more complex than
the corresponding proof for \Cattsu. The primary difficulty with
\Cattsua is that a term can have an insertion redex where the term to
be inserted admits a head reduction. In particular, consider the case
where $a \equiv \SCoh S A L \red b$ is an instance of insertion along
some branch \(P\), and \(a \red c\) is an insertion on the argument
\(L(\olsi P)\). The difficulty of this critical pair is that
\(L(\olsi P)\) need not be in head normal form, and furthermore, the
reduction \(a \leadsto c\) can make the original insertion invalid.
This does not occur in the predecessor theory \Cattsu, where only
identities can be pruned, and all reducts of identities are again identities.

We will prove this theorem using \cref{lem:conf-strat}. It is
therefore sufficient to show that whenever \(b \leftsquigarrow a \red
c\), with \(a \red b\) being a reduction derived from \textsc{rule},
that the following diagram can be formed:

\[
  \begin{tikzcd}[column sep=tiny]
    && a \\
    b &&&& c \\
    & {b'} & {\mathclap{=_{\dim(s)}}} & {c'}
    \arrow[squiggly, from=1-3, to=2-5]
    \arrow[squiggly, from=1-3, to=2-1]
    \arrow["{*}"', squiggly, from=2-1, to=3-2]
    \arrow["{*}", squiggly, from=2-5, to=3-4]
  \end{tikzcd}
\]

We split by cases on the reduction \(a \red b\), ignoring cases where
both reductions are identical and ignoring cases which follow by
symmetry of other cases. Any cases which do not mention insertion
will follow from an identical argument to the one given in
\cref{thm:su-conf}, and so we omit these here. We can therefore
assume without loss of generality that \(a \leadsto b\) is an
insertion along redex \((S,P,T,\Gamma,L,M)\) such that \(a\) is not
an identity or unary composite and \(\stdcoh T {\lh(P)}\) is an
identity or a standard composite which is not unary. We now split on
the reduction \(a \red c\).

\paragraph{Insertion on the inserted argument \(\bm{L(\olsi P)}\)}
Suppose \(\stdcoh T {\lh(P)} \sub M\) admits an insertion along redex
\((T, Q, U, \Gamma, M, N)\). Then:
\[\stdcoh T {\lh(P)} \sub M \red \SCoh {\insertion T Q U} {\stdty T
{\lh(P)}\sub{\kappa_{T,Q,U}}} {\insertion M Q N}\]
We then have \(c \equiv \SCoh S A {L'}\) where \(L'\) is \(L\) with
the reduction above applied. We can conclude that \(\stdcoh T
{\lh(P)}\) must be a composite (i.e.\ not an identity) as otherwise
the second insertion would not be possible. Similarly, \(T\) cannot
be linear as otherwise \(\stdcoh T {\lh(P)}\) would be a unary composite.

We now need the following lemmas, the second of which is a directed
version of \cref{thm:std-insert-props} with more conditions.

\begin{lemma}
  \label{lem:comp-to-tm}
  For all \(n\) and \(S\), \(\stdcoh S n \red^* \stdtm S n\).
\end{lemma}
\begin{proof}
  The only case in which \(\stdcoh S n \neq \stdtm S n\) is when \(S
  = D^n\), in which case a single disc removal gives the required reduction.
\end{proof}

\begin{lemma}
  \label{lem:standard-type-exterior-reduct}
  Let \((S,P,T)\) be an insertion point. Then if \(S\) is not linear
  or \(n \leq \dep(S)\), \(\stdty S n \sub{\kappa_{S,P,T}} \red^*
  \stdty {\insertion S P T} n\) and if \(\dep(S) \leq n\) and \(S\)
  is not linear or \(\dep(S) = n\) then \(\stdtm S n
  \sub{\kappa_{S,P,T}} \red^* \stdtm {\insertion S P T} n\).
\end{lemma}
\begin{proof}
  We proceed by induction on \(n\), starting with the statement for
  types. If \(n = 0\) then both standard types are \(\star\), so we
  are done. Otherwise, we have:
  \begin{alignat*}{3}
    \stdty S {n+1} \sub {\kappa_{S,P,T}} \equiv{} &\stdtm {\bound n
    S} n \sub {\incbd n - S} \sub {\kappa_{S,P,T}} &\qquad& \stdty
    {\insertion S P T} {n+1} \equiv{}&&\stdtm {\bound n {\insertion S
    P T}} n \sub {\incbd n - {\insertion S P T}} \\
    &\to_{\stdty S n \sub {\kappa_{S,P,T}}} &&&&\to_{\stdty
    {\insertion S P T} n}\\
    &\stdtm {\bound n S} n \sub {\incbd n + S} \sub
    {\kappa_{S,P,T}}&&&&\stdtm {\bound n {\insertion S P T}} n \sub
    {\incbd n + {\insertion S P T}}
  \end{alignat*}
  By inductive hypothesis: \(\stdty S n \sub{\kappa_{S,P,T}} \red^*
  \stdty {\insertion S P T} n\), and so we need to show that:
  \[\stdtm {\bound n S} n\sub{\incbd n \epsilon S \bullet
    \kappa_{S,P,T}} \red^* \stdty {\bound n {\insertion S P T}} n
  \sub{\incbd n \epsilon {\insertion S P T}}\]
  We now note that either the conditions for
  \cref{lem:insertion-bd-1} or \cref{lem:insertion-bd-2} must hold.
  If conditions for \cref{lem:insertion-bd-1} hold then (as
  everything is well-formed in \Catt) we get that the required
  reduction is trivial. Therefore, we focus on the second case. Here
  we get from \cref{lem:insertion-bd-2} that:
  \[\stdtm {\bound n S} n\sub{\incbd n \epsilon S \bullet
    \kappa_{S,P,T}} \equiv \stdtm {\bound n S} n\sub{\kappa_{\bound n
  S,\bound n P,\bound n T} \bullet \incbd n \epsilon {\insertion S P T}}\]
  Then we can apply the inductive hypothesis for terms as if \(n \leq
  \dim(S)\) then \(\dep(\bound n S) = n\) and otherwise \(\bound n S
  = S\) is not linear, and so we get the required reduction.

  Now we move on to the case for terms. If \(\stdtm S n\) is a
  variable, then we must have that \(S\) is linear and so \(S =
  D^n\). We must also have in this case that \(\stdtm S n = \olsi
  P\). Then by \cref{lem:iota-kappa-comm}, \(\stdtm S n \sub
  {\kappa_{S,P,T}} \equiv \stdcoh T n \sub {\iota_{S,P,T}}\) and then
  by \cref{lem:disc-insertion-1,lem:comp-to-tm} this reduces to
  \(\stdtm {\insertion S P T} n\) as required. If \(\stdtm S n\) is
  not a variable, then \(\stdtm S n \equiv \stdcoh S n\), and
  \(\stdcoh S n\) cannot be an identity (as either \(S\) is
  non-linear or \(n = \dim(S)\)). By \cref{lem:iota-kappa-comm} and
  other assumptions we get that \(\stdcoh S n \sub {\kappa_{S,P,T}}\)
  admits an insertion along branching point \(P\) and so:
  \begin{alignat*}{2}
    \stdtm S n\sub{\kappa_{S,P,T}} &\equiv{} &&\stdcoh S n\sub
    {\kappa_{S,P,T}}\\
    &\red{} &&\SCoh {\insertion S P T} {\stdty S n \sub
    {\kappa_{S,P,T}}} {\insertion {\kappa_{S,P,T}} P {\iota_{S,P,T}}}\\
    &\equiv{} &&\SCoh {\insertion S P T} {\stdty S n \sub
    {\kappa_{S,P,T}}} {\id}\\
    &\red^*{} &&\SCoh {\insertion S P T} {\stdty {\insertion S P T} n} {\id}\\
    &\equiv{} &&\stdcoh {\insertion S P T} n\\
    &\red^*{} &&\stdtm {\insertion S P T} n
  \end{alignat*}
  With the second equivalence coming from
  \cref{lem:kappa-iota-insert}, the second reduction coming from
  inductive hypothesis (which is well-founded as the proof for types
  only uses the proof for terms on strictly lower values of \(n\)),
  and the last reduction coming from \cref{lem:comp-to-tm}.
\end{proof}

By this lemma (as \(T\) is not linear), we have
\[\stdty T {\lh(P)}\sub{\kappa_{T,Q,U}} \red^* \stdty {\insertion T P
Q} {\lh(P)}\]
and so \(\stdcoh T {\lh(P)} \sub M \red^* \stdcoh {\insertion T Q U}
{\lh(P)} \sub {\insertion M Q N}\). Let \(c'\) be the term obtained
by applying this further reduction to the appropriate argument. Now
by \cref{lem:insert-lin-height}, we have that \(\th(\insertion T Q U)
\geq \th(T)\) and so by \cref{lem:insertable}, there is \(c'
\leadsto^* c''\) with:
\begin{equation*}
  c'' =_{\dim(a)} \SCoh {\insertion S P {(\insertion T Q U)}} {A \sub
  {\kappa_{S,P,\insertion T Q U}}} {\insertion L P {(\insertion M Q N)}}
\end{equation*}
We now examine how \(b\) reduces. As \(T\) is not linear, there is a
branch \(\insertion S P Q\) of \(\insertion S P T\) and we get the
following by \cref{lem:ins-comm-max}:
\begin{equation*}
  \olsi {\insertion S P Q} \sub {\insertion L P M}
  \equiv \olsi Q \sub {\iota_{S,P,T} \bullet (\insertion L P M)}
  \equiv \olsi Q \sub M
  \equiv \stdcoh U {\lh(Q)}\sub N
\end{equation*}
Since \(\th(U) \geq \bh(Q) = \bh(\insertion S P Q)\) we can reduce
\(b\) to \(b'\) by insertion as follows:
\begin{equation*}
  b' \equiv{} \SCoh {\insertion {(\insertion S P T)} {\insertion S P
  Q} U} {A \sub {\kappa_{S,P,T} \bullet \kappa_{\insertion S P T,
  \insertion S P Q, U}}} {\insertion {(\insertion L P M)} {\insertion S P Q} N}
\end{equation*}
and then by \cref{lem:inserted-insertion} we get \(b' =_{\dim(a)}
c''\) as required.

\paragraph{Argument reduction on the inserted argument \(\bm{L(\olsi P)}\)}

Suppose \(M \leadsto M'\), and \(L'\) is \(L\) but with the argument
for \(\olsi P\) replaced by \(\stdcoh T {\lh(P)} \sub {M'}\), such
that \(L \red L'\) and \(a \red c \equiv \Coh S A {L'}\). Then \(c\)
admits an insertion and reduces as follows:
\[c \leadsto c' \equiv \Coh {\insertion S P T} {A \sub
{\kappa_{S,P,T}}} {\insertion {L'} P {M'}}\]
Since each term in \(\insertion {L} P {M}\) is a term of \(L\) or a
term of \(M\), we can simply apply the same reductions from \(L \red
L'\) and \(L \red M'\) to get \(\insertion L P M \red^* \insertion
{L'} P {M'}\). Therefore, \(b \red^* c'\).

\paragraph{Other reduction on the inserted argument \(\bm{L(\olsi P)}\)}

The argument \(L(\olsi P)\) is either a standard composite which is
not unary or an identity. Therefore, the type contained in the
coherence is in normal form and hence a cell reduction cannot be
applied. Further, disc removal cannot be applied, as \(L(\olsi P)\)
is not a unary composite, and endo-coherence removal cannot be
applied as if \(L(\olsi P)\) is an endo-coherence then it is an
identity. Hence, there are no other reductions that can be applied to
the inserted argument and so this case is vacuous.

\paragraph{Reduction of non-inserted argument}
Suppose \(L \leadsto L'\) along an argument which is not \(\olsi P\)
and \(c \equiv \Coh S A {L'}\). Then as \(L'(\olsi P) \equiv
\mathcal{C}_T^{\lh(P)}\), an insertion can still be performed on \(c\) to get:
\[ c \leadsto c' \equiv \SCoh {\insertion S P T} {A \sub
{\kappa_{S,P,T}}} {\insertion {L'} P M}\]
Since the terms of \(\insertion L P M\) are a subset of the terms of
\(L\) and \(M\), we get \(\insertion L P M \red^* \insertion {L'} P
M\) and so \(b \red^* c'\).

\paragraph{Disc removal}

By assumption, insertion cannot be applied to unary composites, and
so this case is vacuous.

\paragraph{Endo-coherence removal}

Suppose \(A \equiv \arr s B s\) and \(a \red c\) by endo-coherence
removal. In this case \(c \equiv \id(A,s) \sub L\) and
\[ b \equiv \Coh {\insertion S P T} {(\arr s B s) \sub
{\kappa_{S,P,T}}} {\insertion L P M}\]
which reduces by endo-coherence removal to:
\[b' \equiv \id(A, s) \sub {\kappa_{S,P,T} \bullet (\insertion L P M)}\]
By \cref{lem:ins-comm-max}, we have that \(\kappa_{S,P,T} \circ
(\insertion L P M) =_{\dim(S)} L\) and so \(b' = _{\dim(S)} c\) and
since \(\dim(S) \leq \dim(a)\), we get \(b' =_{\dim(a)} c\) as required.

\paragraph{Cell reduction}

If \(A \red B\) and \(c \equiv \SCoh S B L\) from cell reduction,
then if \(c\) is not an identity or disc it admits an insertion to reduce to:
\[c' \equiv \SCoh {\insertion S P T} {B \sub {\kappa_{S,P,T}}}
{\insertion L P M}\]
As reduction is compatible with substitution, \(b\) also reduces
to~\(c'\). If instead \(c\) was an identity then
\begin{align*}
  b &\equiv \SCoh {\insertion {D^n} P T} {A \sub {\kappa_{S,P,T}}}
  {\insertion L P M}\\
  &\red \SCoh {\insertion {D^n} P T} {\stdty {D^n} {n+1}\sub
  {\kappa_{S,P,T}}} {(\insertion L P M)}\\
  &\red^* \id(d_n) \sub {\kappa_{S,P,T} \bullet \insertion L P M}\\
  &=_{n+1} \id(d_n) \sub L\\
  &\equiv c
\end{align*}
Where the second reduction is due to \cref{lem:always-ecr} and the
equality is due to \cref{lem:ins-comm-max}. If \(c\) is a disc then
\cref{lem:disc-insertion-red} can be applied to get that \(c\)
reduces to a term \(c''\) with \(c'' =_{n+1} c'\) and \(b \red c'\),
completing this case.

\paragraph{Insertion}

Suppose \(a \leadsto c\) is also an insertion, along a branch \(Q\)
of \(S\). We now split on whether \(\olsi P = \olsi Q\). First
suppose \(\olsi P = \olsi Q\); then by \cref{lem:insertion-irrel}, we
have \(b =_{\dim(a)} c\). Suppose now that \(\olsi P \neq \olsi Q\),
and that \(L(\olsi Q) \equiv \stdcoh U {\lh(Q)} \sub N\), such that:
\[c \equiv \SCoh {\insertion S Q U} {A \sub {\kappa_{S,Q,U}}}
{\insertion L Q N}\]
We now consider the case where \(b\) is an identity. As \(P\) and
\(Q\) are distinct branches of \(S\), we must have that \(S\) itself
is not linear. Therefore, the insertion along \(P\) must be an
insertion of an identity. Further, for \(b\) to have the correct type
for an identity, we must have that \(A \sub {\pi_P} \equiv
\SPath(\olsi Q) \to \SPath(\olsi Q)\). The only path sent to \(\olsi
Q\) by \(\pi_P\) is \(\olsi Q\) itself, and so \(A \equiv
\SPath(\olsi Q) \to \SPath(\olsi Q)\). Now, by \cref{lem:iota-kappa-comm}:
\begin{align*}
  c &\equiv \SCoh {\insertion S Q U} {\stdcoh U {\lh(Q)} \sub {\iota}
  \to \stdcoh U {\lh(Q)} \sub {\iota}} {\insertion L Q N}\\
  &\red \id(\stdcoh U {\lh(Q)} \sub \iota) \sub {\insertion L Q
  N}&\text{ by endo-coherence removal}\\
  &\equiv \id(\stdcoh U {\lh(Q)}) \sub N & \text{by \cref{lem:ins-comm-max}}
\end{align*}
Then, \(\insertion L P M\) sends \(\olsi Q\) to \(L(\olsi Q) \equiv
\stdcoh U {\lh(Q)} \sub N\), and so \(b \equiv \id(\stdcoh U {\lh(Q)}) \sub N\).

The case where \(c\) is an identity is symmetric, so we now consider
when neither \(b\) or \(c\) are identities. We now observe that \(b\)
and \(c\) further reduce as follows:
\begin{align*}
  b &\red^* b' =_{\dim(a)} \SCoh {\insertion {(\insertion S P T)}
  {\insertion Q P T} U} {A\sub{\kappa_{S,P,T} \bullet
  \kappa_{\insertion S P T, \insertion Q P T, U}}} {\insertion
  {(\insertion L P M)} {\insertion Q P T} N}\\
  c &\red^* c' =_{\dim(a)} \SCoh {\insertion {(\insertion S Q U)}
  {\insertion P Q U} T} {A\sub{\kappa_{S,Q,U} \bullet
  \kappa_{\insertion S Q U, \insertion P Q U, T}}} {\insertion
  {(\insertion L Q N)} {\insertion P Q U} M}
\end{align*}
We show that the first reduction is valid with the validity of the
second holding by symmetry. If \(b\) is a unary composite then we
apply \cref{lem:disc-insertion-red} to obtain a suitable \(b'\):
Otherwise, we obtain the reduction via insertion, noting that:
\begin{align*}
  \olsi{\insertion Q P T} \sub {\insertion L P M} &\equiv \olsi{Q}
  \sub{\kappa} \sub{\insertion L P M}\\
  &\equiv L(Q)\\
  &\equiv \stdcoh U {\lh(Q)}\sub N\\
  &\equiv \stdcoh U {\lh(\insertion Q P T)}\sub N
\end{align*}
as required for the insertion, with the third equality coming from
\cref{lem:ins-comm-max}. Lastly, the trunk height condition is
satisfied as \(\bh(Q) = \bh(\insertion Q P T)\).

Therefore, both reductions are valid. We now need the following lemma
to complete the proof:

\begin{lemma}
  Let \((S, P, T, \Gamma, L, M)\) be an insertion redex. Then:
  \[ \insertion L P M =_{\bh(P)+1} \insertionprime L P M\]
\end{lemma}
\begin{proof}
  By \cref{prop:insertion-prime-eq}, the two labellings are equal. By
  inspection of the definition, the maximum dimension of terms that
  differ is \(\dim(\bh(P))\).
\end{proof}

By the above and \cref{lem:insertion-different}, \(b' =_{\dim(a)}
c'\). This completes all cases of \cref{thm:sua-conf}.

\section{Towards normalisation by evaluation}
\label{sec:towards-nbe}

In this section, the Rust implementation of \Catt, \Cattsu, and
\Cattsua, which can be found at \cite{alex_rice_2024_10964705}, is
introduced. This implementation takes the form of an interpreter,
allowing terms of \Catt to be written in a convenient syntax which
can be mechanically checked. The implementation aids the user in
writing \Catt terms by automatically constructing standard
composites, allowing terms to be bound to top level syntax,
implicitly suspending terms, automatically filling arguments which
are not locally maximal, and providing informative error messages to
the user when typechecking fails.

We highlight three points of our implementation:
\begin{itemize}
  \item The typechecker uses \emph{bidirectional
    typing}~\cite{10.1145/3450952} to mix ``inference'' and
    ``checking'' rules. Although types for \Catt can always be
    inferred, we find ourselves in the unusual situation where in
    some cases the context a term lives in can be inferred, and in
    some cases it must be provided. We expand on this type system in
    \cref{sec:typechecking}.
  \item Tree contexts (see \cref{sec:trees}) are given an explicit
    representation in the tool. The syntax in the theory is then
    split into syntax over a tree context and syntax over an
    arbitrary context. Syntax over a tree context can then use paths
    instead of de Bruijn levels to reference positions in the
    context, and substitutions from tree contexts can be given by
    labellings. We explore this syntax in \cref{sec:nbe-syntax}.
  \item During typechecking, the equality between types must be
    checked, which is done by syntactically comparing the normal form
    of each type. In this implementation, an approach inspired by
    \emph{normalisation by evaluation} is taken, as opposed to the
    reduction based approaches used in the previous sections.
\end{itemize}

Normalisation by evaluation (NbE) (see \cite{abel2013normalization}
for an introduction), can be viewed as a method of evaluating terms
with ``unknowns''. Equivalently, NbE defines a semantic model of the
theory, and interprets each constructor of the type theory in these
semantics. When equipped with a method for transforming elements of
this model back to terms of the type theory (referred to as
\emph{quoting}), the normal form of a term can be calculated directly
by recursion on its structure. Compared to the reduction based
approach taken in the previous sections, which simplifies the term
via a series of locally applied reduction rules, NbE takes a more
global approach, deconstructing the original term and using it to
synthesise a normal form.

The form of NbE implemented in the tool is largely inspired by the
paper
\citetitle{gratzer2019implementing}~\cite{gratzer2019implementing},
although we note that the form of the theory \Catt is vastly
different to the modal type theory they present; \Catt does not have
lambda abstraction or application in the usual sense, which makes
adapting NbE techniques from the literature difficult. Nevertheless,
the overall form of the evaluation is similar.

\begin{figure}[ht]
  \centering
  % https://q.uiver.app/#q=WzAsMyxbMCwwLCJcXHRleHR7UmF3IHN5bnRheH0iXSxbMCwyLCJcXHRleHR7Q2hlY2tlZCBzeW50YXh9Il0sWzAsNCwiXFx0ZXh0e05vcm1hbC1mb3JtIHN5bnRheH0iXSxbMCwxLCJcXG1hdGhzZntjaGVja30iLDAseyJjdXJ2ZSI6LTR9XSxbMCwxLCJcXG1hdGhzZntpbmZlcn0iLDIseyJjdXJ2ZSI6NH1dLFsxLDAsIlxcbWF0aHNme3RvXFxfcmF3fSIsMV0sWzEsMiwiXFxtYXRoc2Z7ZXZhbH0iLDAseyJjdXJ2ZSI6LTR9XSxbMiwxLCJcXG1hdGhzZntxdW90ZX0iLDAseyJjdXJ2ZSI6LTR9XV0= % tex-fmt: skip
  \begin{tikzcd}
    {\text{Raw syntax}} \\
    \\
    {\text{Core syntax}} \\
    \\
    {\text{Normal form syntax}}
    \arrow["{\mathsf{check}}", curve={height=-24pt}, from=1-1, to=3-1]
    \arrow["{\mathsf{infer}}"', curve={height=24pt}, from=1-1, to=3-1]
    \arrow["{\mathsf{to\_raw}}"{description}, from=3-1, to=1-1]
    \arrow["{\eval}", curve={height=-24pt}, from=3-1, to=5-1]
    \arrow["{\quote}", curve={height=-24pt}, from=5-1, to=3-1]
  \end{tikzcd}
  \caption{Implementation overview.}
  \label{fig:overview}
\end{figure}

A high-level overview of the implementation is given in
\cref{fig:overview}. We pause to explain the purpose of each component:
\begin{itemize}
  \item The \emph{raw syntax} is the syntax that the user of the tool
    interacts with. We maintain no invariants over the
    well-formedness of the raw syntax, and it allows the user to omit
    arbitrary arguments. The primary purpose of the raw syntax is to
    be the target of parsing, and conversely to facilitate the
    pretty-printing of terms. We also specify a command language
    around this raw syntax which is used to interact with the tool.
  \item The \emph{core syntax} is the result of the typechecking
    procedure. Syntax of this form is known to be well-formed, and
    all implicit arguments have been filled in at this point. The
    terms of this syntax resemble the structured terms of
    \cref{sec:structured-terms}, with various common operations of
    \Catt being defined as constructors. Contrary to previous
    representations of \Catt in this thesis, the application of
    substitution is treated as a term former, instead of an operation.
  \item The \emph{normal form syntax} represents the normal forms of
    each of the type theories \Cattsua, \Cattsu, and \Catt itself.
    This syntax is also always assumed to be well-formed, and is the
    closest to original syntax of \Catt.
  \item The \textsf{eval} and \textsf{quote} functions convert syntax
    between core syntax and normal form syntax. For each constructor
    in the core syntax, evaluation computes the result of the
    corresponding operation, quotienting by the rules of \Cattsu or
    \Cattsua when applicable. We note that despite \Catt itself
    having no computation, evaluation must still process operations
    such as suspension and substitution application. Quotation
    converts normal form syntax back to core syntax, and in our case
    is a trivial inclusion.
  \item The \textsf{infer} and \textsf{check} functions perform
    typechecking while converting raw syntax into core syntax. Both
    functions are mutually dependent on each other, and also may need
    to convert types to normal form syntax to check equality. The
    \textsf{to\_raw} functions ``forget'' that a piece of core syntax
    is well-formed, returning a piece of raw syntax, and can
    optionally remove all non-locally maximal arguments from terms.
\end{itemize}
In the following subsections, we expand on these points, fully
defining each class of syntax, and describing the typechecking and
evaluation procedures.

\subsection{Syntax}
\label{sec:nbe-syntax}

Before defining each of the syntactic classes in the tool, we
introduce some common notation that will be used in the definitions below:
\begin{itemize}
  \item The letter \(v\) will be used to represent \emph{names} in
    the syntax: strings that represent a valid identifier.
  \item A \(\mathsf{Maybe}(x)\) is either of the form
    \(\mathsf{Some}(x)\) or \(\mathsf{None}\).
  \item The notation \(\mathsf{Tree}(x)\) represents a tree structure
    which is given by a list of \(x\)'s which we call the
    \emph{elements} and a list of trees, which we call the
    \emph{branches}, whose length is one less than the list of
    elements. These resemble labellings from
    \cref{sec:tree-contexts}, but will allow trees to be labelled
    with arbitrary objects.
\end{itemize}
We begin our study of the syntax with the raw syntax, which is
defined by the following grammar:
\begin{alignat*}{4}
  &(\text{Terms})&\quad&s,t &\ \ &{}::={}&\ \ &{v} \mid
  \mathsf{coh}[T:A] \mid \_ \mid \mathsf{id} \mid \mathsf{comp} \mid
  \mathsf{inc}_n^m(s) \mid s \sub \sigma \mid \Sigma(s)\\
  &(\text{Types})&&A&&{}::={}&& \star \mid \arr s {\mathsf{Maybe}(A)}
  t \mid \_ \mid A \sub \sigma \mid \Sigma(A)\\
  &(\text{Arguments})&&\sigma&&{}::={}&&
  (\mathsf{Tree}(\mathsf{Maybe}(s)), \mathsf{Maybe}(A)) \mid
  (\mathsf{Maybe}(A,)s_0,\dots,s_n)\\
  &(\text{Contexts})&&\Gamma&&{}::={}&& T \mid (v_0, A_0), \dots, (v_n : A_n)\\
  &(\text{Tree Contexts})&&T&&{}::={}&& \mathsf{Tree}(\mathsf{Maybe}(v))
\end{alignat*}

The primary purpose of the raw syntax is to accurately represent the
written plaintext syntax. For most cases, each constructor is written
in plaintext exactly how it is written above, apart from a few cases:
\begin{itemize}
  \item The application of substitution \(s \sub \sigma\) and \(A
    \sub \sigma\) is simply written \(s\ \sigma\) and \(A\ \sigma\)
    respectively.
  \item The constructor \(\mathsf{inc}_n^m\) is not parsed and is
    used as an internal operation for defining the external
    substitution (see \cref{sec:insertion}). It is displayed as
    \(\mathsf{inc}\verb|<n-m>|\).
  \item The suspension can be given by the characters \(\Sigma\) or
    \(S\), to avoid the user being forced to type Unicode characters.
  \item The type \(\arr s {\mathsf{None}} t\) is written simply as
    \(s \to t\), and the type \(\arr s {\mathsf{Some}(A)} t\) is
    written as \(A \mid s \to t\), where the symbol \(\to\) can be
    replaced by \verb|->| in either case.
  \item For the construction \(\mathsf{Maybe}\), \(\mathsf{Some}(s)\)
    is printed the same as \(s\), and \(\mathsf{None}\) is printed as
    the empty string.
  \item We provide two ways to write trees:
    \begin{itemize}
      \item The curly bracket notation from \cref{sec:trees} can be
        used. The string:
        \[ s_0\{T_0\}s_1\cdots\{T_n\}s_{n+1}\]
        is parsed as a tree with elements given by (the parse of)
        \(s_0\) to \(s_{n+1}\) and branches given by the parse of
        \(T_0\) to \(T_n\).
      \item We provide a notation for specifying the locally maximal
        arguments of a tree. We parse the string:
        \[ [a_1,a_2,\dots,a_n]\]
        As a tree that has \(\mathsf{None}\) as each of elements
        branches by given by each of the \(a_i\), where if \(a_i\)
        does not recursively parse as a tree, it is parsed as an
        element and wrapped in a singleton tree.
    \end{itemize}
    To compare these two notations, the two trees below are equal:
    \[ \{f\}\{\{a\}\{b\}\} = [f,[a,b]]\]
    When using the full (curly bracket) notation to specify a
    labelling, it must be wrapped in angle brackets to avoid parse ambiguity.
\end{itemize}

We highlight the use of the extended substitution introduced in
\cref{sec:extend-subst} in the raw syntax. This allows the tool to
perform ``implicit suspension'', the automatic suspension of a term,
by reducing it to a problem of type inference. These extended
substitutions are converted to regular substitutions by the
evaluation function introduced in \cref{sec:evaluation}, which
applies the appropriate number of suspensions to the head term. An
example of this is given in \cref{sec:examples}.

We also provide a command language on top of the raw syntax for
\Catt, which allows the user to perform various operations on terms,
such as binding them to a top-level name, or normalising them. These
commands are given by the following syntax:
\begin{alignat*}{3}
  &\mathsf{def}\ v = s &&{}\mathrel{\big\vert}
  \mathsf{def}\ v\ \Gamma = s &&{}\mathrel{\big\vert}
  \mathsf{def}\ v\ \Gamma : A = s \\
  \mathrel{\big\vert}{}&\mathsf{normalise}\ s\ \mathsf{in}\ \Gamma
  &&{}\mathrel{\big\vert} \mathsf{assert}\ s = t\ \mathsf{in}\ \Gamma
  &&{}\mathrel{\big\vert} \mathsf{size}\ s\ \mathsf{in}\ \Gamma\\
  \mathrel{\big\vert}{}&\mathsf{import}\ \mathtt{filename}
\end{alignat*}
The first three commands define the name \(v\) to be given by the
term \(s\), where the context \(\Gamma\) and type \(A\) can
optionally be given, determining whether the term \(s\) will be
inferred or checked. The next three commands take a context
\(\Gamma\) and respectively calculate the normal form of \(s\) in
\(\Gamma\), assert that \(s\) and \(t\) are equal in \(\Gamma\), or
count the number of coherence constructors in \(s\). The last command
parses the file \texttt{filename} and runs the commands it contains.

In the implementation, each piece of syntax is paired with a piece of
span information, which specifies where in the source file it
originated. This is done by making the raw syntax generic over a type
\(S\) of spans. When obtaining the raw syntax from parsing, this
\(S\) is given by a range \(n<m\) specifying the start and end
indices of where the syntax appeared in the text. When the syntax is
obtained from the \(\mathsf{to\_raw}\) functions, \(S\) is given by
the unit type.

The span information allows more informative error messages to be
given, which are formatted using the
\href{https://docs.rs/ariadne/latest/ariadne/}{ariadne} crate. An
example error message is given below:
\begin{Verbatim}[baselinestretch=0.8,commandchars=\\\{\},frame=lines]
\textcolor{Diag2}{Error}: Given term "\textcolor{Diag2}{x}" does not match inferred term "\textcolor{Diag1}{y}"
    ╭─[examples/test.catt:19:34]
    │
 19 │ def error_test x\{f\}y = comp ⟨x\{f\}\textcolor{Diag2}{x}⟩
    │                                  \textcolor{Diag2}{┬}
    │                                  \textcolor{Diag2}{╰──} Given term
────╯
\end{Verbatim}
In the above example, the span attached to the erroneous term \(x\)
is used to identify the line of the code which should be displayed
and determine where the ``Given term'' label should point to.

We now move on to the core syntax of the tool. The overall form of
the core syntax is similar to that of the raw syntax with the
following differences:
\begin{itemize}
  \item Span information is no longer included in the core syntax.
  \item Terms that were optional in the raw syntax (as specified
    using the \textsf{Maybe} construction) are no longer optional.
    The hole constructor is also removed.
  \item The syntax is generic over a type \(P\) of positions. In
    practice, the type \(P\) is either the type of de Bruijn levels,
    for terms over a standard context, or paths, for terms over a
    tree context. Both of these types implement the
    \(\mathsf{Position}\) trait, allowing us to call the required
    functions that depend on the type of positions.
  \item The constructor for variables \(v\) in the raw syntax is
    replaced by two separate constructions, a constructor
    \(\mathsf{var}_p\) for local variables, where \(p : P\), and
    \(\mathsf{top\_lvl}(v, s)\) for a top-level symbol \(v\) which is
    bound to term \(s\).
  \item Application of substitutions and labellings are now separate
    constructors. This is primarily due to a limitation of the Rust
    type system which doesn't allow an \textsf{App} constructor to
    existentially quantify over the type of positions in the input term.
  \item The \(\mathsf{comp}\) constructor now records the tree it is
    composing over, and will be written \(\mathsf{comp}_T\).
    Similarly, the \(\mathsf{id}\) constructor becomes
    \(\mathsf{id}_n\), with \(n\) recording the dimension of the term
    the identity is applied to.
\end{itemize}
The core syntax is defined by the following grammar. Following
\cite{gratzer2019implementing}, a different colour is used for each
of the three categories of syntax.
\begin{alignat*}{4}
  &(\text{Terms})&\quad&\color{Diag1}s,t
  &\ \ &{}::={}&\ \ &{\color{Diag1}\mathsf{var}_p} \mid
  {\color{Diag1}\mathsf{top\_lvl}(v,s)} \mid
  {\color{Diag1}\mathsf{coh}[T:A]} \mid \\
  &&&&&&&{\color{Diag1}\mathsf{id}_n} \mid
  {\color{Diag1}\mathsf{comp}_T} \mid
  {\color{Diag1}\mathsf{inc}_n^m(s)} \mid {\color{Diag1}s \sub
  \sigma} \mid {\color{Diag1}s \sub L} \mid {\color{Diag1}\Sigma(s)}\\
  &(\text{Types})&&{\color{Diag1}A}&&{}::={}&& {\color{Diag1}\star}
  \mid {\color{Diag1}\arr s {A} t} \mid {\color{Diag1}A \sub \sigma}
  \mid {\color{Diag1}A \sub L} \mid {\color{Diag1}\Sigma(A)}\\
  &(\text{Substitutions})&&
  {\color{Diag1}\sigma}&&{}::={}&&{\color{Diag1}(A,s_0,\dots,s_n)}\\
  &(\text{Labellings})&&{\color{Diag1}L}
  &&{}::={}&&{\color{Diag1}(\mathsf{Tree}(s), A)}\\
  &(\text{Contexts})&&{\color{Diag1}\Gamma}&&{}::={}&&
  {\color{Diag1}(v_0, A_0), \dots, (v_n : A_n)}\\
  &(\text{Tree Contexts})&&{\color{Diag1}T}&&{}::={}&&
  {\color{Diag1}\mathsf{Tree}(\mathsf{Maybe}(v))}
\end{alignat*}
The above classes of syntax are parameterised by the type of
positions \(P\). We enforce that for the application of substitution
\(s \sub \sigma\) that the term \(s\) is a term with \(P =
\mathbb{N}\), the type of de Bruijn levels, and for \(s \sub L\), the
application of a labelling, that \(s\) is a term with \(P =
\mathsf{Path}\). The type of paths is given by a non-empty list of
natural numbers, as in \cref{sec:trees}.

The type of positions must satisfy the \(\mathsf{Position}\) trait,
which provides:
\begin{itemize}
  \item An associated \(\mathsf{Ctx}\) type, given by contexts for
    \(\mathbb{N}\) and tree contexts for \(\mathsf{Path}\).
  \item An associated container type \(\mathsf{Container}\) which can
    be indexed by \(P\). This is given by \(\mathsf{Vec}\) (Rust's
    dynamically sized array type) for levels and \(\mathsf{Tree}\)
    for paths. This container type defines the form of substitutions
    and labellings respectively.
  \item A function \(\mathsf{to\_name}\), which gives a canonical
    name to each \(p : P\), used when converting back to a raw term.
\end{itemize}
In the core syntax, all the variable names have been replaced by an
index into the context, with all the original variable names being
moved to the context. The \(\mathsf{to\_raw}\) function then takes a
piece of syntax and a context and simply undo this, mapping a
\(\mathsf{var}_p\) to \(v\), where \(p\) maps to \(v\) in the
supplied context. The context argument of \(\mathsf{to\_raw}\) is
optional as the context is not always available, for instance with
the term \(s \sub \sigma\), we do not know the context that \(s\)
should live in. When the context is not available, or no name is
known for \(p\) in the context, \(\mathsf{var}_p\) is mapped to
\(\mathsf{to\_name}(p)\).

For the remainder of the syntax, \(\mathsf{to\_raw}\) removes extra
information that was obtained during typechecking, such as the tree
associated to a \(\mathsf{comp}\), the dimension of an
\(\mathsf{id}\), or the term associated to a top-level binding. We
also accept a boolean parameter declaring whether implicit arguments
should be kept; if this is true then the \(\mathsf{Some}\)
constructor of the \(\mathsf{Maybe}\) type is used whenever possible,
otherwise only locally maximal arguments are kept, replacing the rest
with the \(\mathsf{None}\) constructor.

Lastly, we introduce the normal form syntax. This is the simplest of
the three categories of syntax and is closest to the base syntax of
\Catt. It is given by the following grammar:
\begin{alignat*}{4}
  &(\text{Head})&\quad&\color{Diag2}H
  &\ \ &{}::={}&\ \ &{\color{Diag2}\mathsf{coh}[T:A]} \mid
  {\color{Diag2}\mathsf{id}_n} \mid {\color{Diag2}\mathsf{comp}_T}\\
  &(\text{Terms})&&\color{Diag2}s,t&&{}::={}&&{\color{Diag2}\mathsf{var}_p}
  \mid {\color{Diag2}H \sub L}\\
  &(\text{Labellings})&&\color{Diag2}L
  &&{}::={}&&{\color{Diag2}\mathsf{Tree}(s)}\\
  &(\text{Types})&&{\color{Diag2}A}&&{}::={}&&
  {\color{Diag2}[(s_0,t_0),\dots,(s_n,t_n)]}\\
  &(\text{Tree Contexts})&&{\color{Diag2}T}&&{}::={}&&
  {\color{Diag2}\mathsf{Tree}(\mathsf{Maybe}(v))}
\end{alignat*}
As with the core syntax, this syntax is parameterised by a type of
positions \(P\). In the normal form syntax, application of
substitution is removed, and labellings can no longer be applied to
an arbitrary term, but only to a head term, which is a single
coherence, composite, or identity. The composite and identity
constructions are prioritised as head terms due to the role they play
in \Cattsua, being the only insertable arguments.

Many of the extra operations such as suspension are not present in
the normal form syntax. In particular, the syntax for types is far
simpler, allowing them to be represented by a vector of pairs of
terms. A type \({\color{Diag2}[(s_0,t_0),\dots,(s_n,t_n)]}\)
represents the type \({\color{Diag2}s_0} \to {\color{Diag2}t_0}\),
with the tail of the list giving the lower-dimensional part of the
type. The type \(\star\) is represented by the empty list.

\subsection{Evaluation}
\label{sec:evaluation}

We now describe the core technical part of the tool, the evaluation
of a piece of core syntax to its normal form, which is crucial in
checking the equality of types. Various pieces of reduction can be
configured in the evaluation process:
\begin{itemize}
  \item Disc removal can be turned off or on.
  \item Endo-coherence removal can be enabled or disabled.
  \item Insertion can be set to never happen, only allow insertion of
    identities, or allow insertion of identities and standard composites.
\end{itemize}

Following the NbE style we have already introduced, we define
\emph{(semantic) environments}, which are required to evaluate a term
to a normal form.

\begin{definition}
  For a type of positions \(P\), a \(P\)-\emph{environment} takes the
  form of a \(P\)-\textsf{Container} of normal form terms, along with
  a normal form type. More concretely this is given by a tree of
  terms when \(P = \mathsf{Path}\) and a vector of terms when \(P =
  \mathbb{N}\).
\end{definition}

For an environment \(\color{Diag2}\rho\), we write
\(\color{Diag2}\ty(\rho)\) for the type associated with
\(\color{Diag2}\rho\) and \(\color{Diag2}\rho(p)\) for the
\(p^{\text{th}}\) element of the container, where \(p : P\). This
mirrors the syntax used for a term-labelling in \cref{sec:trees}, as
environments are simply an abstraction over labellings and
substitutions. Due to this similarity, the \emph{restriction} of an
environment can be defined, similarly to its definition for
substitutions. A \(\mathsf{Path}\)-environment can be repeatedly
\emph{unrestricted} until the contained type is \(\star\), returning
a labelling.

\begin{definition}
  We define the restricted environment \(\color{Diag2}\restrict
  \rho\) for each \(P\)-environment \(\color{Diag2}\rho\). For \(P =
  \mathbb{N}\), we let \({\color{Diag2}\ty(\restrict \rho)} =
  {\color{Diag2}(\rho(0),\rho(1)):: \ty(\rho)}\) and
  \({\color{Diag2}\restrict \rho(n)} = {\color{Diag2}\rho(n+2)}\).
  For \(P = \mathsf{Path}\), we let \({\color{Diag2}\ty(\restrict
  \rho)} = {\color{Diag2}(\rho([0]), \rho([1])) :: \ty(\rho)}\) and
  \({\color{Diag2}\unrestrict \rho(p) = \rho(0::p)}\).

  If \(\color{Diag2}\rho\) is a \(\mathsf{Path}\)-environment, then
  there is a labelling \(\color{Diag2}\unrestrict \rho\), obtained by
  popping pairs of terms \(({\color{Diag2}s},{\color{Diag2}t})\) from
  \(\color{Diag2}\ty(\rho)\) and pushing them to the bottom of the
  tree contained in \(\color{Diag2}\rho\) (applying the map \(T
  \mapsto s\{T\}t\)) until \({\color{Diag2}\ty(\rho)} =
  {\color{Diag2}\emp}\). Lastly, for the same \(\color{Diag2}\rho\),
  its inclusion \(\color{Diag2}\rho_n^m\) can be defined letting
  \({\color{Diag2}\ty(\rho_n^m)} = {\color{Diag2}\ty(\rho)}\) and if
  the tree part of \(\color{Diag2}\rho\) is given by:
  \[s_0\{T_0\}\cdots s_n\{T_n\}\cdots \{T_{m-1}\}s_m\cdots \{T_k\}s_{k+1}\]
  then the tree part of \(\color{Diag2}\rho_n^m\) is given by
  \(s_n\{T_n\} \cdots \{T_{m-1}\}s_m\).
\end{definition}

There are also identity environments, that play the role of the
identity substitution and identity labelling.

\begin{definition}
  From a (core) context \(\color{Diag1}\Gamma\), an
  \(\mathbb{N}\)-environment \(\color{Diag2}\id_\Gamma\) can be
  formed with type \(\color{Diag2}\emp\) and
  \({\color{Diag2}\id_\Gamma(i)} = {\color{Diag2}\mathsf{var}_i}\)
  for each \(i < \len({\color{Diag1}\Gamma})\). For each tree
  \(\color{Diag1}T\), a \(\mathsf{Path}\)-environment
  \(\color{Diag2}\id_T\) can be formed again with type
  \(\color{Diag2}\emp\) such that \({\color{Diag2}\id_T(p)} =
  {\color{Diag2}\mathsf{var}_p}\) for each path \(p\) of \(\color{Diag1}T\).
\end{definition}

We can now define the functions \(\eval_{\color{Diag2}\rho}\), which
takes a piece of syntax and evaluates it in the environments
\(\color{Diag2}\rho\) to a normal form, and \(\quote\) which includes
normal forms back into core syntax. Intuitively,
\(\eval_{\color{Diag2}\rho}({\color{Diag1}s})\) computes the normal
form of \(s \sub \rho\).

We define \(\eval_{\color{Diag2}\rho}({\color{Diag1}s})\),
\(\eval_{\color{Diag2}\rho}({\color{Diag2}A})\),
\(\eval_{\color{Diag2}\rho}({\color{Diag2}\sigma})\),
\(\eval_{\color{Diag2}\rho}({\color{Diag2}L})\), to be respectively a
normal form term, a normal form type, an \(\mathbb{N}\)-environment,
and a \(\mathsf{Path}\)-environment. We proceed by case analysis,
giving the easier cases below.
\begin{mathpar}
  \eval_{\color{Diag2}\rho}({\color{Diag1}\mathsf{var}_p}) =
  {\color{Diag2}\rho(p)} \and
  \eval_{\color{Diag2}\rho}({\color{Diag1}\mathsf{top\_lvl}(v,s)}) =
  \eval_{\color{Diag2}\rho}({\color{Diag1}s})\and
  \eval_{\color{Diag2}\rho}({\color{Diag1}\mathsf{id}_n}) =
  {\color{Diag2}\mathsf{id}_n \sub {\unrestrict \rho}}\and
  \eval_{\color{Diag2}\rho}({\color{Diag1}\mathsf{inc}_n^m(s)}) =
  \eval_{{\color{Diag2}\rho_n^m}}({\color{Diag1}s}) \and
  \eval_{\color{Diag2}\rho}({\color{Diag1}s \sub \sigma}) =
  \eval_{\eval_{\color{Diag2}\rho}
  ({\color{Diag1}\sigma})}({\color{Diag1}s})\and
  \eval_{\color{Diag2}\rho}({\color{Diag1}s \sub L}) =
  \eval_{\eval_{\color{Diag2}\rho}({\color{Diag1}L})}({\color{Diag1}s})\and
  \eval_{\color{Diag2}\rho}({\color{Diag1}\Sigma(s)}) =
  \eval_{\color{Diag2}\restrict\rho}({\color{Diag1}s}) \\
  \eval_{\color{Diag2}\rho}({\color{Diag1}\star}) =
  {\color{Diag2}\ty(\rho)} \and
  \eval_{\color{Diag2}\rho}({\color{Diag1}\arr s A t}) =
  (\eval_{\color{Diag2}\rho}({\color{Diag1}s}),
  \eval_{\color{Diag2}\rho}({\color{Diag1}t})) ::
  \eval_{\color{Diag2}\rho}({\color{Diag1}A})\and
  \eval_{\color{Diag2}\rho}({\color{Diag1}A \sub \sigma}) =
  \eval_{\eval_{\color{Diag2}\rho}
  ({\color{Diag1}\sigma})}({\color{Diag1}A})\and
  \eval_{\color{Diag2}\rho}({\color{Diag1}A \sub L}) =
  \eval_{\eval_{\color{Diag2}\rho}({\color{Diag1}L})}({\color{Diag1}A})\and
  \eval_{\color{Diag2}\rho}({\color{Diag1}\Sigma(A)}) =
  \eval_{\color{Diag2}\restrict\rho}({\color{Diag1}A})
\end{mathpar}
The environments \(\eval_{\color{Diag2}\rho}({\color{Diag1}\sigma})\)
and \(\eval_{\color{Diag2}\rho}({\color{Diag1}L})\) are then obtained
by evaluating the type and all the terms in \(\sigma\) and \(L\)
respectively. We note that suspension is evaluated by restricting the
environment, and does not require a full traversal of the term,
demonstrating the further utility of the extended substitution
introduced in \cref{sec:extend-subst}.

This leaves the cases for the \({\color{Diag1}\mathsf{coh}}\) and
\({\color{Diag1}\mathsf{comp}}\) terms. For these cases, we need
definitions of the standard type of dimension \(n\) over a tree \(S\)
and the exterior labelling for an insertion point \((S,P,T)\), which
we define as the core syntax \({\color{Diag1}\stdty T n}\) and
\({\color{Diag1}\kappa_{S,P,T}}\). We omit the definitions of these,
as they are similar to those given in \cref{sec:insertion}, using the
\({\color{Diag1}\mathsf{inc}}\) and \({\color{Diag1}\Sigma}\)
constructors in place of the \(\Inc\) constructor of structured
terms. For (labelled) trees \(S\) and \(T\) such that \((S,P,T)\) is
an insertion point, we also define the inserted (labelled) tree
\(\insertion S P T\) identically to the inserted labelling.

We now proceed with the case for \({\color{Diag1}\mathsf{coh}[S :
A]}\), assuming we are evaluating it in environment
\({\color{Diag2}\rho}\). We begin by letting \(d =
\dim({\color{Diag2}\ty(\rho)})\) and obtaining the labelling
\({\color{Diag2}L} = {\color{Diag2}\unrestrict \rho}\). The number
\(d\) represents the number of times the term must be suspended, and
so \(S\) and \({\color{Diag1}A}\) are each suspended \(d\) times,
where the type \({\color{Diag1}A}\) is suspended by applying a
\({\color{Diag1}\Sigma}\) constructor, and \(S\) is suspended by
replacing it with the tree \(\mathsf{None}\{S\}\mathsf{None}\).

We now search for insertion redexes in the labelling
\({\color{Diag2}L}\), splitting on the type of insertion that is enabled:
\begin{itemize}
  \item If no insertion is enabled, this phase is skipped.
  \item If insertion of identities is enabled, and there is a locally
    maximal argument given by branch \(P\) (where we take \(P\) to be
    the branch of minimal branching height) that is of the form
    \({\color{Diag2}\mathsf{id}_n\sub{M}}\), we return the insertion
    redex \((S,P,D^n,\_,{\color{Diag2}L}, {\color{Diag2}M})\), where
    the target of \({\color{Diag2}L}\) and \({\color{Diag2}M}\) is unspecified.
  \item If full insertion is enabled, and there is a locally maximal
    argument given by branch \(P\) that is of the form
    \({\color{Diag2}\mathsf{comp}_T \sub M}\), where \(\bh(P) >
    \lh(T)\), we return the insertion redex
    \((S,P,T,\_,{\color{Diag2}L}, {\color{Diag2}M})\).
\end{itemize}
If an insertion redex \((S,P,T,\_,{\color{Diag2}L},
{\color{Diag2}M})\) is found, then \(S\) is replaced by \(\insertion
S P T\), \({\color{Diag2}L}\) is replaced by \(\insertion
{\color{Diag2}L} P {\color{Diag2}M}\), and \(\color{Diag1}A\) is
replaced by \(\color{Diag1}A \sub {\kappa_{S,P,T}}\). This step is
then repeated until no insertion redexes are found.

\begin{remark}
  At this critical step, the evaluation proceeds in a fashion closer
  to reduction than NbE, with insertions repeatedly applied by
  searching for redexes and applying reductions to the head term.
  This seems unavoidable; even if one could define a parallel
  insertion which inserted all insertable arguments at once, it is
  not clear how to deal with locally maximal arguments that are
  iterated identities. Despite this, we still claim that the overall
  structure of the evaluation follows an NbE style, especially
  regarding the treatment of suspension and application of
  substitutions and labellings.
\end{remark}

We next obtain the type \({\color{Diag2}B} =
\eval_{\color{Diag2}\id_S}({\color{Diag1}A})\), and split into cases:
\begin{itemize}
  \item If endo-coherence removal is enabled, and \(\color{Diag2}B\)
    is of the form \(\color{Diag2}(s,s) :: B'\), then we let
    \({\color{Diag1}\arr t C t} = \quote({\color{Diag2}B})\),
    interpret \(\color{Diag2}L\) as an environment by letting
    \({\color{Diag2}\ty(L)} = {\color{Diag2}\star}\) and let:
    \[ \eval_{\color{Diag2}\rho}({\color{Diag1}\mathsf{coh}[S : A]})
      = {\color{Diag2}\mathsf{id}_{\dim(B')}} \sub
      {\{\eval_{\color{Diag2}L}({\color{Diag1}C}),
    \eval_{\color{Diag2}L}({\color{Diag1}t})\}}\]
    where the labelling \(\{\_,\_\}\) from a disc can be trivially
    constructed by deconstructing the type.
  \item Suppose endo-coherence removal is disabled, \(S\) is a disc
    \(D^n\), and \(\color{Diag2}B\) is of the form
    \(\color{Diag2}(\mathsf{var}_{p^n}, \mathsf{var}_{p^n}) :: B'\),
    where we recall the path \(p^n\) is the unique locally maximal
    variable of \(D^n\), then we let:
    \[ \eval_{\color{Diag2}\rho}({\color{Diag1}\mathsf{coh}[S : A]})
    = {\color{Diag2}\mathsf{id}_n \sub {L}}\]
  \item If disc-removal is enabled, \(S = D^n\), and
    \(\color{Diag2}B\) is equal to the standard type of dimension \(n\), then:
    \[ \eval_{\color{Diag2}\rho}({\color{Diag1}\mathsf{coh}[S : A]})
    = {\color{Diag2}L(p^n)}\]
  \item If none of the above cases hold, and \(\color{Diag2}B\) is
    equal to the standard type of dimension \(\dim(S)\), then:
    \[ \eval_{\color{Diag2}\rho}({\color{Diag1}\mathsf{coh}[S : A]})
    = {\color{Diag2}\mathsf{comp}_S \sub L}\]
  \item If none of the above cases hold, then:
    \[ \eval_{\color{Diag2}\rho}({\color{Diag1}\mathsf{coh}[S : A]})
    = {\color{Diag2}\mathsf{coh}[S : B] \sub L}\]
\end{itemize}

The \(\color{Diag1}\mathsf{comp}_T\) case is treated in much the same
way, removing any step involving \(\color{Diag1}A\) and instead
setting \({\color{Diag2}B} =
\eval_{\color{Diag2}\id_T}({\color{Diag1}\stdty T n})\), where \(n\)
is given by the dimension of \(T\) before any insertion was
performed. This completes all cases for the evaluation function.

In contrast, the \(\quote\) function is defined completely trivially
by recursion, converting head terms and normal form terms to core
terms, normal form labellings to core labellings, and converting
normal form types to an iterated arrow type in the obvious way. We
note that this is unusual for NbE, where the \(\quote\) function is
often mutually defined with evaluation, and performs a significant
portion of the work of converting terms to normal form.

\subsection{Typechecking}
\label{sec:typechecking}

Now that the three classes of syntax and the evaluation function have
been introduced, the bidirectional typechecking algorithm in the tool
can be described. Bidirectional typing allows us to mix typing rules
which ``check'' a term, and typing rules which ``infer'' the type for
a term. In the implementation, this will determine which pieces of
data are inputs to a procedure, and which pieces of data are outputs.

By \cref{lem:ty-unique}, all \Catt terms \(s\) have a unique type,
which is given by the canonical type \(\ty(s)\). However, for certain
terms, such as the coherence term \(\mathsf{coh}[T : A]\), we will be
able to further infer the context that a term lives in, which in this
case is the tree context \(T\). In this case the pair of the inferred
context and type is known as a \emph{principal
typing}~\cite{10.1145/237721.237728}, which is not to be confused
with a \emph{principal type} of a term in a fixed context.

Due to our unique case where all types are inferable, but the context
in a judgement may or may not be inferable, we refer to judgements
where the context is an input as \emph{checking} judgements and
judgements where the context is output as \emph{inferring} judgements.

\begin{remark}
  We justify this choice of terminology by noting the similarity of
  the judgements \(\Gamma \vdash s : A\) and \(\cdot \vdash
  \Pi_{\Gamma}\,s : \Gamma \to A\) in a type theory with (dependent)
  function types, where inferring the type of the second judgement
  would infer the context of the first. Of course, \Catt does not
  have function types, yet the intuition can still apply.
\end{remark}

The typing system will be defined with respect to a \emph{Signature}
\(\Psi\), which contains a mapping from names to triples
\(({\color{Diag1}\U}, {\color{Diag1}s}, {\color{Diag1}A})\) where
\(\color{Diag1}s\) is a term of type \(\color{Diag1}A\) in (tree)
context \(\color{Diag1}\U\). In the implementation, the signature
also stores all relevant settings for the tool: which reductions are
active, the operation set \(\mathcal{O}\) (which can only be
configured to the groupoidal or regular operation sets), and whether
implicit variables should be kept in the \(\mathsf{to\_raw}\)
functions. We write:
\[ \Psi(v) = ({\color{Diag1}\U}, {\color{Diag1}s}, {\color{Diag1}A})\]
if the signature \(\Psi\) maps \(v\) to the triple above.

We further define the notation \({\color{Diag1}\U}(i) = (v :
{\color{Diag1}A})\) to mean that at the \(i^{\text{th}}\) index of
\(\color{Diag1}\U\) (with \(\color{Diag1}\U\) being a tree or a
context), contains a variable name \(v\), which is given type
\(\color{Diag1}A\) by \(\color{Diag1}\U\).

Lastly we define two conversion functions: \(\mathsf{from\_sub}\) and
\(\mathsf{flatten}\). The first is a (partial) function which takes a
tree \(T\) and a substitution \(\sigma\) and creates a labelling
\(\mathsf{from\_sub}_T(\sigma)\) by letting the locally maximal
arguments be given by the terms of \(\sigma\), if \(\sigma\) contains
the correct number of terms. The function \(\mathsf{flatten}\) acts
on the \(\mathsf{Maybe}\) construction applied to a term or type. It
takes \(\mathsf{Some}(s)\) and \(\mathsf{Some}(A)\) to \(s\) and
\(A\) respectively, and \(\mathsf{None}\) to \(\_\), the hole
constructor for terms and types.

Our bidirectional typing system will be based on the following
judgements, letting \(\color{Diag1}\U\) refer to either a context or
tree context:
\begin{alignat*}{2}
  &s \rightsquigarrow {\color{Diag1}\U} \vdash {\color{Diag1}t} :
  {\color{Diag1}A}&\qquad&\text{Convert \(s\) to \(\color{Diag1}t\)
    inferring its type \(\color{Diag1}A\) in inferred (tree) context
  \(\color{Diag1}\U\)}\\
  &{\color{Diag1}\U} \vdash s \rightsquigarrow {\color{Diag1}t} :
  {\color{Diag1}A} &&\text{Given \(\color{Diag1}\U\), convert \(s\)
    to \(\color{Diag1}t\) checking it has some type \(\color{Diag1}A\)
  in \(\color{Diag1}\U\)}\\
  &{\color{Diag1}\U} \vdash s = {\color{Diag2}t} \rightsquigarrow ()
  &&\text{In \(\color{Diag1}\U\), check \(s\) has normal form
  \(\color{Diag2}t\)}\\
  &{\color{Diag1}\U} \vdash A \rightsquigarrow {\color{Diag1}B} =
  {\color{Diag2}C} &&\text{In \(\color{Diag1}\U\), convert \(A\) to
  \(\color{Diag1}B\), inferring its normal form \(\color{Diag2}C\)}\\
  &{\color{Diag1}\U} \vdash A = {\color{Diag2}C} \rightsquigarrow ()
  &&\text{In \(\color{Diag1}\U\), check \(A\) has normal form
  \(\color{Diag2}C\)}\\
  &\Gamma \vdash{} \rightsquigarrow {\color{Diag1}\U}&&\text{Check
  \(\Gamma\), producing (tree) context \(\color{Diag1}\U\)}\\
  &{\color{Diag1}\U} \vdash \sigma : {\color{Diag1}\Gamma}
  \rightsquigarrow {\color{Diag1}\tau} &&\text{Check \(\sigma\) is a
    substitution from \(\color{Diag1}\Gamma\) to \(\color{Diag1}\U\),
  producing \(\color{Diag1}\tau\)}\\
  &{\color{Diag1}\U} \vdash L : T \rightsquigarrow {\color{Diag1}M} :
  {\color{Diag2}A}&&\text{Check labelling \(L\) in
    \(\color{Diag1}\U\), producing \(\color{Diag1}M\) with type
  \(\color{Diag2}A\)}
\end{alignat*}
for each judgement, the syntax to the left of \(\rightsquigarrow\)
are the inputs to the judgements, and the syntax to the right are the outputs.

\newcommand{\ruleone}{\alpha}
\newcommand{\ruletwo}{\beta}
\newcommand{\rulethree}{\gamma}
\newcommand{\rulefour}{\delta}
\newcommand{\rulefive}{\varepsilon}

The typing rules for all judgements of this system are given in
\cref{fig:bidirectional}. In this figure, \(D^n\) always refers to
the linear tree of depth \(n\), rather than the disc context,
\(\emptyset\) refers to the empty context, and \(\emp\) refers to the
singleton tree. In the final rules, \(i\) should be treated as if it
is universally quantified. We pause to highlight some of these rules:
\begin{itemize}
  \item In the rule for coherences, marked \(\ruleone\), the support
    conditions are checked. This is done using the normal form syntax
    for the type, due to the simplicity of this syntax. The variable
    sets of a term can easily be collected by recursion, and in the
    implementation are stored in a hash set, using Rust's \textsf{HashSet} type.
  \item The rule for composites, marked \(\ruletwo\), is crucially a
    checking rule as there is no way to infer the tree \(T\) for the
    term \(\color{Diag1}\mathsf{comp}_T\).
  \item For the rule for the application of labellings, marked
    \(\rulethree\), the premise for the typing of the term is given
    by a checking judgement instead of an inferring judgement, as the
    tree \(T\) can be inferred form the labelling. This is in
    contrast to the corresponding rule for application of
    substitutions, where the context must be inferred from the inner
    term before the substitution can be checked. Combined with the
    point above, this allows a labelling applied to a
    \(\mathsf{comp}\) term to be checked.
  \item The rule marked \(\rulefour\) allows a substitution to be
    applied to a term over a tree context, by converting the
    substitution to a labelling. This is mainly a convenience
    feature, as given a term \(s\) where it can be inferred that the
    context of \(s\) is a tree \(T\), it can be easier to give the
    locally maximal arguments for \(s\) as a list rather than
    describing the labelling.
  \item Lastly, we explain each component of the rule for the typing
    of a substitution, marked \(\rulefive\). We note that the first
    type in any \Catt context, which in the rule is given by the type
    \(\color{Diag1}A_0\), is always \(\star\). Therefore, the type of
    the first term in a substitution \(\sigma\) should be equal to
    \(\star \sub \sigma \equiv \ty(\sigma)\). In the rule, the type
    of the first term is given by \(\color{Diag1}B_0\), explaining
    its presence as the type of the substitution that gets evaluated
    to \(\color{Diag2}\rho\). We further note that
    \(\color{Diag2}\ty(\rho)\) is simply the evaluation of
    \(\color{Diag1}B_0\), which is why \(X\) is checked against it.

    Due to the choice to use de Bruijn levels instead of indices,
    weakening a term is the identity, and so \(s \sub \sigma \equiv s
    \sub {\langle \sigma,t \rangle}\) for any \(t\). Therefore, by
    inspecting the typing rules for substitutions in \Catt, it can be
    proven that to type \(\Gamma \vdash \sigma : \Delta\), it is
    sufficient to show that \(\Gamma \vdash x \sub \sigma : A \sub
    \sigma\) for all \((x : A) \in \Delta\). Observing the rule
    \(\rulefive\), this translates to proving that \(A_i \sub
    {(B_0,t_0,\dots,t_n)} = B_i\) recalling that \(B_0\) is the core
    syntax version of the type of the substitution. These equations
    can be shown by proving that the evaluation of each side is the
    same, but the evaluation of the left-hand side is given by
    \(\eval_{\color{Diag2}\rho}({\color{Diag1}A_i})\) for each \(i\),
    and so for efficiency we factor out the calculation of
    \(\color{Diag2}\rho\).
\end{itemize}

\begin{figure}[p]
  \centering
  \begin{mathpar}
    \inferrule{\Psi(v) = ({\color{Diag1}\U}, {\color{Diag1}t},
    {\color{Diag1}A})}{v \rightsquigarrow {\color{Diag1}\U} \vdash
    {\color{Diag1}\mathsf{top\_lvl}(v,t)} : {\color{Diag1}A}}\and
    \inferrule{{\color{Diag1}T} \vdash A \rightsquigarrow
      {\color{Diag1}B} = {\color{Diag2}C} \\ (T,
      \src({\color{Diag2}C}), \tgt({\color{Diag2}C})) \in
    \mathcal{O}}{\mathsf{coh}[T : A] \rightsquigarrow
      {\color{Diag1}T} \vdash {\color{Diag1}\mathsf{coh}[T : B]} :
    {\color{Diag1}B}}\ \ruleone\and
    \inferrule{ }{\mathsf{id} \rightsquigarrow {\color{Diag1}D^1}
      \vdash {\color{Diag1}\mathsf{id}_0} : {\color{Diag1}\arr
    {\mathsf{var}_{[0]}} {\star} {\mathsf{var}_{[0]}}}}\and
    \inferrule{s \rightsquigarrow {\color{Diag1}\U} \vdash
    {\color{Diag1}t} : {\color{Diag1}A}}{\Sigma(s) \rightsquigarrow
      {\color{Diag1}\Sigma(\U)} \vdash {\color{Diag1}\Sigma(t)}:
    {\color{Diag1}\Sigma(A)}}\\

    \inferrule{s \rightsquigarrow {\color{Diag1}T} \vdash
    {\color{Diag1}t} : {\color{Diag1}A}}{{\color{Diag1}T} \vdash s
    \rightsquigarrow {\color{Diag1}t} : {\color{Diag1}A}}\and
    \inferrule{{\color{Diag1}\U}(i) = (v :
    {\color{Diag1}A})}{{\color{Diag1}\U} \vdash v \rightsquigarrow
    {\color{Diag1}\mathsf{var}_i} : {\color{Diag1}A}}\and
    \inferrule{ }{{\color{Diag1}D^n} \vdash \mathsf{id}
      \rightsquigarrow {\color{Diag1}\mathsf{id}_n} :
    {\color{Diag1}\stdty {D^n} {n + 1}}}\and
    \inferrule{ }{{\color{Diag1}T} \vdash \mathsf{comp}
      \rightsquigarrow {\color{Diag1}\mathsf{comp}_T} :
    {\color{Diag1}\stdty T n}}\ \ruletwo\and
    \inferrule{s \rightsquigarrow {\color{Diag1}\Gamma} :
      {\color{Diag1}t} : {\color{Diag1}A} \\ {\color{Diag1}\U} \vdash
      \sigma : {\color{Diag1}\Gamma} \rightsquigarrow {\color{Diag1}\tau}\\
      {\color{Diag1}\U} \vdash \ty(\sigma) = {\color{Diag2}B}
    \rightsquigarrow ()}{{\color{Diag1}\U} \vdash s \sub \sigma
      \rightsquigarrow {\color{Diag1}t \sub \tau} : {\color{Diag1}A
    \sub \tau}}\and
    \inferrule{{\color{Diag1}T} \vdash s \rightsquigarrow
      {\color{Diag1}t} : {\color{Diag1}A} \\ {\color{Diag1}\U} \vdash
      \mathsf{from\_sub}_T(\sigma) : {\color{Diag1}T} \rightsquigarrow
      {\color{Diag1}M} : {\color{Diag2}B}\\ {\color{Diag1}\U} \vdash
      \ty(\sigma) = {\color{Diag2}B} \rightsquigarrow
    ()}{{\color{Diag1}\U} \vdash s \sub \sigma \rightsquigarrow
    {\color{Diag1}t \sub M} : {\color{Diag1}A \sub M}}\ \rulethree\and
    \inferrule{{\color{Diag1}T} : s \rightsquigarrow {\color{Diag1}t}
      : {\color{Diag1}A} \\ {\color{Diag1}\U} \vdash L : T
      \rightsquigarrow {\color{Diag1}M} :
      {\color{Diag2}B}\\ {\color{Diag1}\U} \vdash \ty(L) =
    {\color{Diag2}B} \rightsquigarrow ()}{{\color{Diag1}\U} \vdash s
      \sub L \rightsquigarrow {\color{Diag1}t \sub M} : {\color{Diag1}A
    \sub M}}\ \rulefour\\

    \inferrule{ }{{\color{Diag1}\U} \vdash \_ = {\color{Diag2}t}
    \rightsquigarrow ()}\and
    \inferrule{{\color{Diag1}\U} \vdash s \rightsquigarrow
    {\color{Diag1}t} : {\color{Diag1}A}}{{\color{Diag1}\U} \vdash s =
      {\eval_{\color{Diag2}\id_{\color{Diag1}\U}}({\color{Diag1}t})}
    \rightsquigarrow ()}\\

    \inferrule{ }{{\color{Diag1}\U} \vdash \star \rightsquigarrow
    {\color{Diag1}\star} = {\color{Diag2}\emp}}\and
    \inferrule{{\color{Diag1}\U} \vdash s \rightsquigarrow
      {\color{Diag1}s'} : {\color{Diag1}A} \\ {\color{Diag1}\U} \vdash
      t \rightsquigarrow {\color{Diag1}t'} : {\color{Diag1}B}
      \\ \eval_{\color{Diag2}\id_{\color{Diag1}\U}}{\color{Diag1}A} =
    \eval_{\color{Diag2}\id_{\color{Diag1}\U}}{\color{Diag1}B}}%
    {{\color{Diag1}\U} \vdash \arr s {} t \rightsquigarrow
      {\color{Diag1}\arr {s'} {A} {t'}} =
      {\color{Diag2}(\eval_{\id_{\color{Diag1}\U}}{\color{Diag1}s'},
        \eval_{\id_{\color{Diag1}\U}}{\color{Diag1}t'}) ::
    \eval_{\id_{\color{Diag1}\U}}{\color{Diag1}A}}}\and
    \inferrule{{\color{Diag1}\U} \vdash s \rightsquigarrow
      {\color{Diag1}s'} : {\color{Diag1}B} \\
      {\color{Diag1}\U} \vdash t \rightsquigarrow {\color{Diag1}t'} :
      {\color{Diag1}C} \\
      {\color{Diag1}\U} \vdash A \rightsquigarrow {\color{Diag1}A'} =
      {\color{Diag2}A''}\\
      {\color{Diag2}A''} =
      \eval_{\color{Diag2}\id_{\color{Diag1}\U}}{\color{Diag1}B} =
    \eval_{\color{Diag2}\id_{\color{Diag1}\U}}{\color{Diag1}C}}%
    {{\color{Diag1}\U} \vdash \arr s {A} t \rightsquigarrow
      {\color{Diag1}\arr {s'} {A'} {t'}} =
      {\color{Diag2}(\eval_{\id_{\color{Diag1}\U}}{\color{Diag1}s'},
    \eval_{\id_{\color{Diag1}\U}}{\color{Diag1}t'}) :: A''}}\\

    \inferrule{{\color{Diag1}\U} \vdash A \rightsquigarrow
    {\color{Diag1}B} = {\color{Diag2}C}}{{\color{Diag1}\U} \vdash A =
    {\color{Diag2}C} \rightsquigarrow ()}\and
    \inferrule{{\color{Diag1}\U} \vdash s = {\color{Diag2}s'}
      \rightsquigarrow ()\\
      {\color{Diag1}\U} \vdash t = {\color{Diag2}t'} \rightsquigarrow ()
    }{{\color{Diag1}\U} \vdash \arr s {} t = {\color{Diag2}\arr {s'}
    {A} {t'}} \rightsquigarrow ()}\and
    \inferrule{{\color{Diag1}\U} \vdash s = {\color{Diag2}s'}
      \rightsquigarrow ()\\
      {\color{Diag1}\U} \vdash t = {\color{Diag2}t'} \rightsquigarrow ()\\
      {\color{Diag1}\U} \vdash A = {\color{Diag2}A'} \rightsquigarrow ()\\
    }{{\color{Diag1}\U} \vdash \arr s {A} t = {\color{Diag2}\arr {s'}
    {A'} {t'}} \rightsquigarrow ()}\and
    \inferrule{ }{{\color{Diag1}\U} \vdash \_ = {\color{Diag2}C}
    \rightsquigarrow ()}\\

    \inferrule{ }{T \vdash {} \rightsquigarrow {\color{Diag1}T}}\and
    \inferrule{ }{\emptyset \vdash {} \rightsquigarrow
    {\color{Diag1}\emptyset}}\and
    \inferrule{\Gamma \vdash {} \rightsquigarrow
      {\color{Diag1}\Delta} \\ {\color{Diag1}\Delta} \vdash A
    \rightsquigarrow {\color{Diag1}B} = {\color{Diag2}C}}{\Gamma, (v
    : A) \vdash {} \rightsquigarrow {\color{Diag1}\Delta, (v : B)}}\\

    \inferrule{{\color{Diag1}\U} \vdash s_i \rightsquigarrow
      {\color{Diag1}t_i} : {\color{Diag1}B_i}\\ {\color{Diag2}\rho}
      := \eval_{\color{Diag2}\id_{\color{Diag1}\U}}%
      ({\color{Diag1}(B_0,t_0,\dots,t_n)})\\\\
      \eval_{\color{Diag2}\id_{\color{Diag1}\U}}({\color{Diag1}B_i}) =
      \eval_{\color{Diag2}\rho}({\color{Diag1}A_i})\\ {\color{Diag1}\U}
      \vdash \mathsf{flatten}(X) = {\color{Diag2}\ty(\rho)}
    \rightsquigarrow ()}{{\color{Diag1}\U} \vdash (X,s_0,\dots,s_n) :
      {\color{Diag1}(v_0:A_0),\dots,(v_n:A_n)} \rightsquigarrow
    {\color{Diag1}(B,t_0,\dots,t_n)} }\ \rulefive\\

    \inferrule{{\color{Diag1}\U} \vdash \mathsf{flatten}(x)
    \rightsquigarrow {\color{Diag1}A}}{{\color{Diag1}\U} \vdash x :
      \emp \rightsquigarrow {\color{Diag1}t} :
    \eval_{\color{Diag2}\id_{\color{Diag1}\U}}({\color{Diag1}A})}\and
    \inferrule{{\color{Diag1}\U} \vdash L_i \rightsquigarrow
      {\color{Diag1}M_i} : {\color{Diag2}(s_i,s_{i+1})::A}
      \\ {\color{Diag1}\U} \vdash \mathsf{flatten}(x_i) =
    {\color{Diag2}s_i} \rightsquigarrow ()}{{\color{Diag1}\U} \vdash
      x_0\{L_0\}\cdots\{L_n\}x_{n+1} \rightsquigarrow
    {\color{Diag1}s_0\{M_0\}\cdots\{M_n\}s_{n+1}} : {\color{Diag2}A}}
  \end{mathpar}
  \caption{Bidirectional typing rules.}
  \label{fig:bidirectional}
\end{figure}

The typing rules in \cref{fig:bidirectional} can easily be translated
into an algorithm for mechanically checking each of these typing
judgements. In some cases, some equalities of normal forms are left
implicit, such as in the final rule concerning the typing of a
non-singleton labelling, and must be made explicit in the final algorithm.

Many of the choices for the form of these rules was made to improve
the quality of error messages. Each of these rules can fail for a
variety of reasons, at which point an error is created by converting
the relevant syntax back to raw syntax using the \(\mathsf{to\_raw}\)
functions so that it can be displayed to the user. The use of Rust's
\textsf{Result} type, which allows each of these functions to return
either the well-formed core syntax or an appropriate error message,
is essential, and benefits greatly from the question mark syntax in
Rust, which allows errors to easily be propagated through the code.

We end this section by describing the function of each of the
commands introduced in \cref{sec:nbe-syntax}. Each of these commands
is run with a mutable reference to a signature \(\Psi\). The commands
use this signature for typechecking, and may modify the signature.

The three \(\mathsf{def}\) commands are used to add a new binding to
the signature \(\Psi\). For the first command, which omits the
context, the term \(s\) must be inferred, producing a core syntax
context, term, and type, which is inserted into the signature with
key \(v\) and printed to the user. The second command is given a raw
context and so first checks this raw context to produce a core
(possibly tree) context \(\color{Diag1}\U\), before checking the term
\(s\) in this context. Checking the term then produces a core syntax
term and type, which are inserted into the signature along with the
context \(\color{Diag1}\U\). The last \(\mathsf{def}\) command
proceeds as before, checking the context to get a context
\(\color{Diag1}\U\) and then checking the term in
\(\color{Diag1}\U\), producing a core term \(\color{Diag1}t\) and
type \(\color{Diag1}B\). The supplied type \(A\) is then checked
against
\(\eval_{\color{Diag2}\id_{\color{Diag1}\U}}({\color{Diag1}B})\). If
this check succeeds, the key-value pair \((v,
({\color{Diag1}\U},{\color{Diag1}t},{\color{Diag1}B}))\) is added to
the signature \(\Psi\), identically to the previous case.

The \(\mathsf{normalise}\) command is used to print the normal form
of a term \(s\). As with the final two \(\mathsf{def}\) cases, we
begin by checking the context, and checking the term \(s\) in the
resulting core context to get term \(\color{Diag1}t\) of type
\(\color{Diag1}A\). Both \(\color{Diag1}t\) and \(\color{Diag1}A\)
are then evaluated to normal form, quoted, and converted back to raw
syntax, before being pretty-printed to the user. The
\(\mathsf{size}\) command calculates a primitive estimate of the
complexity of a term (which we note is not the same as the syntactic
complexity given in \cref{sec:termination}) by counting the number of
constructors in the normal form. To run this command, the term \(s\)
is checked as before, and converted to a normal form term
\(\color{Diag2}t\). Then \(\mathsf{size}({\color{Diag2}t})\) is then
calculated by induction by the rules given in \cref{fig:size} and
this size is printed to the user. The \(\mathsf{assert}\) command
checks both input terms \(s\) and \(t\), and evaluates the resulting
core syntax terms to normal form to check that they are equal. None
of the \(\mathsf{normalise}\), \(\mathsf{size}\), or
\(\mathsf{assert}\) commands modify the signature \(\Psi\).

\begin{figure}[ht]
  \centering
  \begin{mathpar}
    \mathsf{size}({\color{Diag2}\mathsf{coh}[T : A]}) = 1 +
    \mathsf{size}({\color{Diag2}A})\and
    \mathsf{size}({\color{Diag2}\mathsf{id}_n}) =
    \mathsf{size}({\color{Diag2}\mathsf{comp}_T}) = 1 \and
    \mathsf{size}({\color{Diag2}\mathsf{var}_p}) = 0 \and
    \mathsf{size}({\color{Diag2}H \sub L}) =
    \mathsf{size}({\color{Diag2}H}) + \mathsf{size}({\color{Diag2}L})\and
    \mathsf{size}({\color{Diag2}L}) = \sum_{p : \Path_T}
    \mathsf{size}({\color{Diag2}L(p)})\and
    \mathsf{size}({\color{Diag2}[(s_0,t_0), \dots, (s_n,t_n)]}) =
    \sum_{i= 0}^n \left(\mathsf{size}({\color{Diag2}s_i}) +
    \mathsf{size}({\color{Diag2}t_i})\right)
  \end{mathpar}
  \caption{Size of normal form syntax.}
  \label{fig:size}
\end{figure}

Finally, the \(\mathsf{import}\) command reads the contents of the
supplied file, parses it as a list of commands, and runs each of
these commands with the same signature. The tool has a command line
interface, which allows files to be loaded at startup, as well as
providing a REPL (read-eval-print loop) which parses one command at a time.

\subsection{Examples}
\label{sec:examples}

We now demonstrate the use of the tool with some examples. All the
examples below can be found in the \texttt{/examples} directory of
the implementation code base~\cite{alex_rice_2024_10964705}.

We begin defining some standard operations that can be found in a
monoidal category or bicategory, which can be found in the file
\texttt{/examples/monoidal.catt}. We start by defining
\(1\)-composition as a coherence:
\begin{lstlisting}[language=Catt]
  def comp1coh [f,g] = coh [ x{}{}z : x -> z ] (f,g)
\end{lstlisting}
This example demonstrates the two ways of giving a tree context: in
the \(\mathsf{def}\) command we give the context using the square
bracket notation, which only labels the maximal elements, and in the
coherence it is given by the full labelling, as we require access to
the variables \(x\) and \(z\) (we note that all other variables of
the context have been omitted). This example further demonstrates
that a substitution can be applied to a term over a tree context,
where we have only specified the locally maximal arguments.

This composite can of course also be given using the
\(\mathsf{comp}\) construction.
\begin{lstlisting}[language=Catt]
  def comp1 [f,g] = comp
  assert comp1coh(f,g) = comp1(f,g) in [f,g]
\end{lstlisting}
The tree for \(\mathsf{comp}\) is inferred from the labelling
\texttt{[f,g]}. The assert statement ensures that these two ways of
giving the \(1\)-composition are equal in the theory. The assert
passes even with no reduction enabled, demonstrating the value of
evaluation in the fully weak case. The horizontal and vertical
composites of \(2\)-cells can be given similarly:
\begin{lstlisting}[language=Catt]
  def horiz [[a],[b]] = comp
  def vert [[a,b]] = comp
\end{lstlisting}
As the vertical composite is the suspension of \(1\)-composition, it
can also be given using implicit suspension:
\begin{lstlisting}[language=Catt]
  def vertsusp [[a,b]] = comp1[a,b]
  assert vert(a,b) = vertsusp(a,b) in [[a,b]]
\end{lstlisting}
In this case, the labelling applied to \texttt{comp1} is a tree of
depth \(1\) where the locally maximal arguments are given by
\(2\)-dimensional terms. Type inference then deduces that the type
component of this labelling should be \(1\)-dimensional, and hence
evaluation causes the head term \texttt{comp1} to be suspended,
making it equal to the composite \texttt{vert}, as demonstrated by
the assertion.

The unitors and associator are then given by the following
coherences, using the \(\mathsf{id}\) builtin for the unitors:
\begin{lstlisting}[language=Catt]
  def unitor_l = coh [ x{f}y : comp1(id(x),f) -> f ]
  def unitor_r = coh [ x{f}y : comp1(f, id(y)) -> f ]
  def assoc    = coh [ {f}{g}{h} : comp1(comp1(f,g),h) -> comp1(f,comp1(g,h)) ]
\end{lstlisting}
which allows definitions to be given for terms which witness the
triangle and pentagon equations of monoidal categories:
\begin{lstlisting}[language=Catt]
  def triangle = coh [ x{f}y{g}z
                     : vert(assoc(f,id(y),g), horiz(id(f),unitor_l(g)))
                       ->
                       horiz(unitor_r(f),id(g))
                     ]

  def pentagon = coh [ v{f}w{g}x{h}y{i}z
                     : vert(assoc(comp1(f,g),h,i),assoc(f,g,comp1(h,i)))
                       ->
                       comp [
                         horiz(assoc(f,g,h),id(i)),
                         assoc(f,comp1(g,h),i),
                         horiz(id(f),assoc(g,h,i))
                       ]
                     ]
\end{lstlisting}
We note the direct use of the \(\mathsf{comp}\) constructor to easily
provide a ternary composite without needing to give a new top-level
definition. Using the \(\mathsf{normalise}\) command, it can be shown
that the triangle reduces to the identity with \Cattsu normalisation
enabled, and the pentagon reduces to the identity with \Cattsua
normalisation enabled.

In the files \texttt{/examples/eh.catt} and
\texttt{/examples/eh-cyll.catt}, we give two \Catt proofs of the
Eckmann-Hilton argument (see \cref{prop:eh}). In \Cattsu, these both
normalise to the following vastly smaller term:
\begin{lstlisting}[language=Catt]
  def swap = coh [ x{f{a}g}y{h{b}k}z
                 : comp[comp [[a],h], comp[g,[b]]]
                   ->
                   comp[comp [f,[b]], comp[[a],k]]
                 ]
\end{lstlisting}
The \(\mathsf{size}\) command demonstrates that the \Catt
Eckmann-Hilton proof in \texttt{/examples/eh.catt} has size 1807
whereas its \Cattsu normalisation has a size of only 19. Due to the
simplicity of Eckmann-Hilton in \Cattsu, we are able to give \Cattsu
and \Cattsua proofs of the syllepsis (see \cref{sec:cattsu}) in
\texttt{/examples/syllepsis-su.catt} and
\texttt{/examples/syllepsis.catt} respectively. It can be verified
that in \Cattsua, the \Cattsu proof of syllepsis, which has size
2745, reduces to the \Cattsua proof, which has size 1785.

\subsection{Further work}
\label{sec:further-work}

We end the discussion of this implementation with some options for
improving the tool. Each of these suggestions could make the tool
easier to use and interact with, which in turn extends what can be
achieved with it.

Currently, the tool completely relies on the bidirectional typing
rules to perform all of its type inference. While this is effective
in some scenarios, for example labellings and implicit suspension, it
is lacking in others, such as the lack of implicit arguments in substitutions.

One could try to implement such features by adding metavariables and
a unification procedure to the typechecker. Contrary to the situation
for the fully weak \Catt, unification for \Cattsu and \Cattsua is
non-trivial. Suppose we wished to unify the following two terms:
\[ f *_0 g = h *_0 i\]
where \(f\),\(g\),\(h\), and \(i\) may contain metavariables. In
\Catt, this problem could be reduced to the unification problems \(f
= h\) and \(g = i\). In \Cattsu however, this cannot be done, as a
potential solution is \(f = h *_0 i\) and \(g = \id\). It is likely
that any unification that can be implemented for \Cattsu (and
\Cattsua) is quite limited, but an investigation into the limits of
unification in these settings could be valuable.

Even without a powerful unification algorithm, there are still
instances where an argument could be inferred by the tool. One such
example is the Eckmann-Hilton term presented in the previous section.
This term is defined in the context:
\[ (x : \star)\ (\alpha : \id(x) \to \id(x))\ (\beta : \id(x) \to \id(x)) \]
Here, the \(x\) should be inferable as it is the \(0\)-source of
\(\alpha\). The tool currently has no way to deduce this.

Separately, improvements could be made to the treatment of unfolding
of top-level definitions in the tool. Whenever a term is evaluated by
the tool, any top-level definition is unfolded to its normal form.
This is not always desirable, as it means that error messages
frequently contain fully expanded terms, increasing the length and
readability of terms in addition to losing the information associated
with the name given to the definition.

Conversely, the full unfolding of evaluation often means that we
avoid evaluating terms before displaying them to the user, even when
a (partial) evaluation would simplify the term. A notable example is
that when giving a new definition, its type is not simplified before
being displayed, often resulting in terms such as \verb|p0{x{f}y}|.

A better approach would likely add top-level definitions to the
normal form syntax as a head term, allowing their unfolding to be
optional. One potential approach for efficient unfolding is given by
\citeauthor{andrastalk}~\cite{andrastalk}.

Finally, the accessibility of the tool could be improved with proper
editor integration, for example by implementing the language server
protocol (see
\url{https://microsoft.github.io/language-server-protocol/}), which
would allow errors to be displayed directly in the editor, among
other code refactoring features.

\section{Models}
\label{sec:models}

Despite claiming that the type theories \Cattsu and \Cattsua model
semistrict \(\infty\)-categories, we are yet to discuss their models.
In this section we recall the definition of a model for these
theories, and discuss some properties of these models.

The definitions of \emph{globular category} and \emph{globular sum}
were given in \cref{sec:background}. Any variant of \Cattr can be
equipped with the structure of a globular category by choosing the
disc objects to be the disc contexts and letting the source and
target maps be given by the inclusions \(\lfloor \incbd  {n} \epsilon
{D^{n+1}} \rfloor\) for \(\epsilon \in \{-,+\}\). We then define the
category of models of \Cattsu and \Cattsua.

\begin{definition}
  Recall that for any tame variant of \Cattr, the category
  \(\mathsf{Catt}_{\mathcal{R}}^{\mathsf{ps}}\) is defined to be the
  restriction of the syntactic category
  \(\mathsf{Catt}_{\mathcal{R}}\) to the ps-contexts. We define the
  category of models to be the full subcategory of the presheaf
  category on \(\mathsf{Catt}_{\mathcal{R}}^{\mathsf{ps}}\)
  consisting of functors:
  \[F : \left( \mathsf{Catt}_{\mathcal{R}}^{\mathsf{ps}}
  \right)^{\text{op}} \to \mathbf{Set}\]
  such that \(F^{\text{op}}\) preserves globular sums.
\end{definition}

Each element of the category of models has the structure of a weak
\(\infty\)-category. For a model \(F : \left(
  \mathsf{Catt}_{\mathcal{R}}^{\mathsf{ps}} \right)^{\text{op}} \to
\mathbf{Set}\), the set of \(n\)-cells is given by \(F(D^n)\), with
source and target maps given by the functions:
\[ F(\lfloor \incbd {n-1} - {D^n} \rfloor), F(\lfloor \incbd {n-1} +
{D^n} \rfloor) : F(D^n) \to F(D^{n-1})\]
for which the globularity equations follow from the globularity of
the inclusion maps. For each term over a ps-context in \Cattr, an
operation on each of the models can be derived. We consider the
action of the \(1\)-composition term, given by \(\stdcoh
{[\emp,\emp]} 1\). For the model \(F\), this induces an operation:
\[ F(\{\lfloor \stdcoh {[\emp, \emp]} 1 \rfloor\}) : F(D^1\vee D^1) \to F(D^1)\]
Due to the preservation of globular sums, we have \(F(D^1 \vee D^1) =
F(D^1) \amalg_{F(D^0)} F(D^1)\), which is exactly the set of
composable \(1\)-cells, which the function above sends to their
composition. Similarly, the identity \(\id(d_0)\) induces a map
\(F(D^0) \to F(D^1)\), giving the identity on each \(0\)-cell.

These operations can be combined, to get a compound operation of the
following form:
\[
  \begin{tikzcd}[column sep = large]
    {F(D^1)} = {F(D^1) \amalg_{F(D^0)} F(D^0)} & {F(D^1)
    \amalg_{F(D^0)} F(D^1)} & {F(D^1)}
    \arrow["{\id \amalg F(\id(d_0))}", from=1-1, to=1-2]
    \arrow["{F(\{\stdcoh {[\emp,\emp]} 1\})}", from=1-2, to=1-3]
  \end{tikzcd}
\]
By the functoriality of \(F\) (and preservation of globular sums),
this composite should be equal to:
\[ F(\{\stdcoh {[\emp,\emp]} 1\} \bullet \langle d_1 , \id_{d_0^+}
\rangle) : F(D^1) \to F(D^1)\]
Therefore, if \(F\) is further a \Cattsu model, then this operation
must equal \(F(\id) = \id\), enforcing the semistrict properties of
\Cattsu onto the model.

Throughout the thesis, contexts in \Cattr have been viewed as
semistrict \(\infty\)-categories themselves. This viewpoint can be
made precise by the Yoneda embedding, as for each context \(\Gamma\)
of \Cattr, we obtain the presheaf:
\[Y(\Gamma) :  \mathsf{Catt}_{\mathcal{R}}^{\mathsf{op}} \to \mathbf{Set}\]
which sends \(\Delta\) to \(\mathrm{Hom}(\Delta, \Gamma)\), the
substitutions from \(\Delta\) to \(\Gamma\). This map preserves all
limits, so in particular its opposite preserves the globular sums,
meaning it can be restricted to a model of \Cattr. Furthermore, the
\(n\)-cells are given by substitutions \(D^n \to \Gamma\), which are
precisely the \(n\)-dimensional terms of \(\Gamma\) up to definitional equality.

Since every \Catt term is also a \Cattr term, there is an evident functor:
\[ K_{\mathcal{R}} : \mathsf{Catt} \to \mathsf{Catt}_{\mathcal{R}}\]
which sends each context and substitution to its equivalence class in
\Cattr. This functor can be restricted to the functor:
\[ K_{\mathcal{R}}^{\mathsf{ps}} : \mathsf{Catt}^{\mathsf{ps}} \to
\mathsf{Catt}_{\mathcal{R}}^{\mathsf{ps}} \]
which is the identity on objects. We now prove that this functor
preserves globular sums. By \cite[Lemma 64]{benjamin2021globular},
the functor \(\mathbf{FinGlob} \to \mathsf{Catt}\) from the category
of finite globular sets preserves globular sums, and so it suffices
to show that the functor \(\mathbf{FinGlob} \to
\mathsf{Catt}_{\mathcal{R}}\) preserves globular sum. By \cite[Lemmas
25 and 29]{benjamin2021globular}, it suffices to show that this
functor preserves the initial object and preserves pushouts along the
inclusion maps \(S^n \to D^n\). The empty context is clearly the
initial object, and this is preserved by the above functor. For the
second property it suffices to show that:
\[
  \begin{tikzcd}
    {S^n} & \Gamma \\
    {D^n} & {\Gamma, (x: A)}
    \arrow["\{\wk(U^n)\}"', from=1-1, to=2-1]
    \arrow["{\{A\}}", from=1-1, to=1-2]
    \arrow["{\{A,x\}}"', from=2-1, to=2-2]
    \arrow[from=1-2, to=2-2]
\end{tikzcd}\]
is a pushout for each \(\Gamma \vdash A\) in \Cattr. Suppose there is
context \(\Delta\) with substitutions \(\sigma : \Gamma \to \Delta\)
and \(\{B,t\} : D^n \to \Delta\) such that:
\[\{B\} \equiv \{\wk(U^n)\} \bullet \{B, t\} = \{A\} \bullet \sigma
\equiv \{A \sub \sigma\}\]
Then the universal map is given by \(\langle \sigma, t \rangle\),
with this map being well-formed as \(\Delta \vdash t : B\) and \(B =
A \sub \sigma\). The uniqueness of this universal map is clear.
Hence, the square above is cocartesian. From this we get the
following proposition.

\begin{proposition}
  The functors \(K_{\mathcal{R}}\) and
  \(K_{\mathcal{R}}^{\mathsf{ps}}\) preserve globular sums.
\end{proposition}
\begin{proof}
  As the maps \(\mathbf{FinGlob} \to \mathsf{Catt}\) and
  \(\mathbf{FinGlob} \to \mathsf{Catt}_{\mathcal{R}}\) preserve
  globular sums, the globular sums in both \(\mathsf{Catt}\) and
  \(\mathsf{Catt}_{\mathcal{R}}\) are given exactly by the
  ps-contexts. The two functors \(K_{\mathcal{R}}\) and
  \(K_{\mathcal{R}}^{\mathsf{ps}}\) are the identity on ps-contexts,
  and hence preserve globular sums.
\end{proof}

Due to this proposition, any model of \Cattr can be also seen as a
model of \Catt, by precomposing with the functor
\(K_{\mathcal{R}}^{\mathsf{ps}}\). This is to be expected, as
intuitively every semistrict \(\infty\)-category should also be a
weak \(\infty\)-category, where certain operations are given by identities.

\subsection{Rehydration for pasting diagrams}
\label{sec:rehydration}

We have shown a way in which every model of \Cattr can be viewed as a
model of \Catt. In this section we prove that this mapping from
\Cattr models to \Catt models is injective. This implies that being
semistrict is a \emph{property} of the model, a particular \Catt
model can only arise from a unique \Cattr model, if such a \Cattr model exists.

We prove this result by demonstrating a partial conservativity result
for \Cattr, which we call \emph{rehydration for pasting contexts}.
Rehydration refers to the process of taking a term in the semistrict
theory, and inserting the necessary coherence morphisms into the term
such that it can be typed in \Catt. We discuss the difficulties
involved with rehydrating an arbitrary term in
\cref{sec:towards-gener-rehydr}, but for now we are only concerned
with the simpler case of rehydrating a term \(t : \Term_\Gamma\)
where \(\Gamma\) is a ps-context. We work towards the following theorem:
\begin{theorem}
  \label{thm:rehydration}
  Let \(\mathcal{R}\) be a tame equality rule set that satisfies the
  support condition and has pruning, disc removal, and endo-coherence
  removal. Then for any ps-context \(\Delta\) and term \(t :
  \Term_\Delta\), there is a \Catt term \(s : \Term_\Delta\) such
  that \(\Delta \vdash s = t\) in \Cattr.
\end{theorem}

We begin with an example for \Cattsu. Take the pasting context given
by the following diagram:
\[ \Delta =
  \begin{tikzcd}
    w & x & y & z
    \arrow["f", from=1-1, to=1-2]
    \arrow["g", from=1-2, to=1-3]
    \arrow["h", from=1-3, to=1-4]
  \end{tikzcd}
\]
The associator \(\alpha\) is a \Cattsu normal form term over
\(\Delta\), and we can further define the term:
\[ \eta : \id((f*g)*h) \to \alpha_{f,g,h} * \alpha_{f,g,h}^{-1}\]
as a single coherence over \(\Delta\). This term is also a \Cattsu
normal form. Finally the term:
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJcXGJ1bGxldCJdLFsyLDAsIlxcYnVsbGV0Il0sWzQsMCwiXFxidWxsZXQiXSxbMCwxLCJcXGlkIiwyXSxbMSwyLCJcXGlkIiwyXSxbMCwxLCJcXGFscGhhICogXFxhbHBoYV57LTF9IiwwLHsiY3VydmUiOi00fV0sWzEsMiwiXFxhbHBoYSAqIFxcYWxwaGFeey0xfSIsMCx7ImN1cnZlIjotNH1dLFswLDIsIlxcYWxwaGEgKiBcXGFscGhhXnstMX0iLDIseyJjdXJ2ZSI6NX1dLFszLDUsIlxcZXRhIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs0LDYsIlxcZXRhIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs3LDEsIlxcZXRhXnstMX0iLDIseyJzaG9ydGVuIjp7InNvdXJjZSI6MjB9fV1d % tex-fmt: skip
\[
  \begin{tikzcd}
    \bullet && \bullet && \bullet
    \arrow[""{name=0, anchor=center, inner sep=0}, "\id"', from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0}, "\id"', from=1-3, to=1-5]
    \arrow[""{name=2, anchor=center, inner sep=0}, "{\alpha *
    \alpha^{-1}}", curve={height=-24pt}, from=1-1, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0}, "{\alpha *
    \alpha^{-1}}", curve={height=-24pt}, from=1-3, to=1-5]
    \arrow[""{name=4, anchor=center, inner sep=0}, "{\alpha *
    \alpha^{-1}}"', curve={height=30pt}, from=1-1, to=1-5]
    \arrow["\eta"', shorten <=3pt, shorten >=3pt, Rightarrow, from=0, to=2]
    \arrow["\eta"', shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=3]
    \arrow["{\eta^{-1}}"', shorten <=3pt, Rightarrow, from=4, to=1-3]
  \end{tikzcd}
\]
is a \Cattsu normal form term over a pasting context, which is not
well-formed in \Catt. Such a term can be rehydrated by inserting the
equivalence \(\id \cong \id * \id\) into the centre of the term.
Performing a similar construction with the interchanger instead of
the associator creates a \Cattsua normal form term over a pasting
context which is not a \Catt term.

We now proceed with the proof of \cref{thm:rehydration}. We introduce
three operations, which are mutually defined on terms of \Cattr over
pasting contexts.
\begin{itemize}
  \item The \emph{rehydration} \(R(t)\) of a term \(t\) recursively
    rehydrates all subterms of \(t\), and then pads the resulting
    term. For any \Cattr term \(t\), the rehydration is a \Catt term
    over the same context. For any term \(t\), we call \(R(N(t))\)
    its \emph{rehydrated normal form}, where \(N\) is the function
    taking any term to its normal form. We similarly define the
    rehydration \(R(A)\) of a type \(A\) over a pasting context and
    \(R(\sigma)\) of a substitution \(\sigma\) whose domain and
    codomain are pasting contexts.
  \item The \emph{padding} \(P(t)\) of a \Catt term \(t\), which
    composes the term with coherences to ensure that its boundaries
    are in rehydrated normal form.
  \item The normaliser \(\phi(t)\), a coherence term from \(t\) to
    its rehydrated normal form \(R(N(t))\) for any \Catt term \(t\).
\end{itemize}
We give formal definitions for each of these, which we define
mutually with proofs of the following statements, where we assume
\(\Delta\) and \(\Gamma\) are pasting contexts:
\begin{enumerate}
  \item Suppose \(\Delta \vdash_{\mathcal{R}} t : A\). Then \(\Delta
    \vdash R(t) : R(N(A))\). Similarly, if \(\Delta
    \vdash_{\mathcal{R}} A\) or \(\Delta \vdash_{\mathcal{R}} \sigma
    : \Gamma\), then \(\Delta \vdash R(A)\) and \(\Delta \vdash
    R(\sigma) : \Gamma\).
  \item For a \Cattr well-formed term \(t\), type \(A\), and
    substitution \(\sigma\), we have \(\Delta \vdash t = R(t)\),
    \(\Delta \vdash A = R(A)\), and \(\Delta \vdash \sigma =
    R(\sigma)\) in \Cattr.
  \item Suppose \(\Delta \vdash t : A\) for a \Catt term \(t\), then
    \(P_k(t)\) is well-formed for \(k \leq \dim(t)\) and \(\Delta
    \vdash P(t) : R(N(A))\).
  \item Suppose \(t\) is a well-formed \Catt term. Then for each \(k
    \leq \dim(t)\), \(P_k(t) = t\).
  \item If \(\Delta \vdash t : R(N(A))\) in \Catt, then \(\Delta
    \vdash \phi(t) : \arr t A {R(N(t))}\).
  \item Let \(t\) be a well-formed \Catt term over a pasting context.
    Then \(\phi(t) = \id(t)\).
\end{enumerate}
Each of these definitions and proofs are given by an induction on
dimension and subterms, ensuring that they are well-founded.

We begin with the definition of the rehydrated term, type, and substitution.
\begin{definition}
  Let \(\Delta\) and \(\Gamma\) be a pasting context. For a term
  \(t\) or type \(A\) over \(\Delta\), or a substitution \(\sigma :
  \Gamma \to \Delta\), we define the rehydrations:
  \[ R(t) : \Term_\Delta \qquad R(A) : \Type_\Delta \qquad R(\sigma)
  : \Gamma \to \Delta\]
  by mutual recursion. For a variable \(x\), we let \(R(x) = x\), and
  for a coherence term we define:
  \[ R(\Coh \Gamma A \sigma) = P(\Coh {\Gamma} {R(A)} {R(\sigma)})\]
  For types and substitutions, we recursively apply the rehydration
  to all subterms.
\end{definition}

To define the padding, we need the composites over certain trees
\(T_k^n\) for \(k < n\) which are defined by:
\[ T_0^n = [\emp, D^{n-1}, \emp] \qquad T_{k+1}^{n+1} = \Sigma(T_k^n)\]
As an example \(T_1^3\) produces the following context:
% https://q.uiver.app/#q=WzAsMixbMCwwLCJcXGJ1bGxldCJdLFsyLDAsIlxcYnVsbGV0Il0sWzAsMSwiIiwwLHsiY3VydmUiOi0zfV0sWzAsMSwiIiwyLHsiY3VydmUiOjN9XSxbMCwxLCIiLDEseyJjdXJ2ZSI6LTV9XSxbMCwxLCIiLDEseyJjdXJ2ZSI6NX1dLFs1LDMsIiIsMSx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbMiw0LCIiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzMsMiwiIiwyLHsib2Zmc2V0IjotNSwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFszLDIsIiIsMix7Im9mZnNldCI6NSwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs4LDksIiIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XV0= % tex-fmt: skip
\[
  \begin{tikzcd}
    \bullet && \bullet
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-18pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=18pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0},
    curve={height=-40pt}, from=1-1, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0},
    curve={height=40pt}, from=1-1, to=1-3]
    \arrow[shorten <=2pt, shorten >=2pt, Rightarrow, from=3, to=1]
    \arrow[shorten <=2pt, shorten >=2pt, Rightarrow, from=0, to=2]
    \arrow[""{name=4, anchor=center, inner sep=0}, shift left=5,
    shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow[""{name=5, anchor=center, inner sep=0}, shift right=5,
    shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow[shorten <=4pt, shorten >=4pt, Rightarrow, scaling nfold=3,
    from=4, to=5]
  \end{tikzcd}
\]
The composite over this context allows us to ``fix'' the
\(1\)-dimensional boundary of a \(3\)-dimensional term.

\begin{definition}
  Let \(t\) be an \(n\)-dimensional term of a pasting diagram
  \(\Delta\). Define its padding \(P(t)\) to be equal to \(P_n(t)\) where:
  \[ P_0(t) = t \qquad P_{k+1}(t) = \stdcoh {T_{k}^{n}} n \sub
    {\langle \phi(\src_{k}(P_{k}(t)))^{-1}, P_{k}(t),
  \phi(\tgt_k(P_{k}(t))) \rangle}\]
  where \(\src_k\) and \(\tgt_k\) give the \(k\) dimensional source
  and target of a term.
\end{definition}

Consider the term \(\alpha : \arr f {\arr x \star y} g\). As an
example, we build the following sequence of paddings:
\begin{center}
  \begin{tabular}{P{3cm} P{9cm}}
    \(P_0(\alpha)\)&{
      \begin{tikzcd}[ampersand replacement=\&]
        x \& y
        \arrow[""{name=0, anchor=center, inner sep=0}, "f"',
        curve={height=12pt}, from=1-1, to=1-2]
        \arrow[""{name=1, anchor=center, inner sep=0}, "g",
        curve={height=-12pt}, from=1-1, to=1-2]
        \arrow["\alpha"', shorten <=3pt, shorten >=3pt, Rightarrow,
        from=0, to=1]
    \end{tikzcd}}\\
    \(P_1(\alpha)\)&{
      \begin{tikzcd}[ampersand replacement=\&]
        {R(N(x))} \& x \& y \& {R(N(y))}
        \arrow[""{name=0, anchor=center, inner sep=0}, "f"',
        curve={height=12pt}, from=1-2, to=1-3]
        \arrow[""{name=1, anchor=center, inner sep=0}, "g",
        curve={height=-12pt}, from=1-2, to=1-3]
        \arrow["{\phi(x)}"', from=1-2, to=1-1]
        \arrow["{\phi(y)}", from=1-3, to=1-4]
        \arrow["\alpha"', shorten <=3pt, shorten >=3pt, Rightarrow,
        from=0, to=1]
    \end{tikzcd}}\\
    \(P_2(\alpha)\)&{
      \begin{tikzcd}[ampersand replacement=\&]
        {R(N(x))} \& x \& y \& {R(N(y))}
        \arrow[""{name=0, anchor=center, inner sep=0},
        "f"{description}, curve={height=12pt}, from=1-2, to=1-3]
        \arrow[""{name=1, anchor=center, inner sep=0},
        "g"{description}, curve={height=-12pt}, from=1-2, to=1-3]
        \arrow["{\phi(x)}"', from=1-2, to=1-1]
        \arrow["{\phi(y)}", from=1-3, to=1-4]
        \arrow[""{name=2, anchor=center, inner sep=0}, "R(N(g))",
        curve={height=-60pt}, from=1-1, to=1-4]
        \arrow[""{name=3, anchor=center, inner sep=0}, "R(N(f))"',
        curve={height=60pt}, from=1-1, to=1-4]
        \arrow["\alpha"', shorten <=4pt, shorten >=3pt, Rightarrow,
        from=0, to=1]
        \arrow["{\phi(\phi(x)^{-1} * g * \phi(y))}"{description},
        shorten <=5pt, shorten >=3pt, Rightarrow, from=1, to=2]
        \arrow["{\phi(\phi(x)^{-1}*f*\phi(y))}"{description}, shorten
        <=5pt, shorten >=3pt, Rightarrow, from=0, to=3]
    \end{tikzcd}}
  \end{tabular}
\end{center}

We lastly define the normaliser coherences. As these are each built
from a coherence constructor with the rule for equivalences, they can
all be inverted.

\begin{definition}
  Let \(t\) be a term of a pasting diagram \(\Delta\). By
  \cref{cor:supp-ps}, \(\Supp(t)\) is a pasting diagram, and we let
  \(i_t\) be the inclusion \(\Supp(t) \to \Delta\). Then we define
  the normaliser \(\phi(t)\):
  \[ \phi(t) = \Coh {\Supp(t)} {t \to R(N(t))} {i_t}\]
  By assumption, \(R(N(t)) = N(t) = t\) and so \(\Supp(R(N(t))) =
  \Supp(t)\), making the above term well-formed.
\end{definition}

We now prove the required properties, starting with statement 1. The
statements for types and substitutions follow by a simple induction
using the case for terms, as if \(A = B\) then \(R(N(A)) = R(N(B))\)
(as \(N(A) = N(B)\)). The case for a variable is also trivial, so assume that:
\[\Delta \vdash_{\mathcal{R}} \Coh \Gamma B \sigma : A\]
Then it follows from induction on subterms that \(\Gamma \vdash
R(B)\) and \(\Delta \vdash R(\sigma) : \Gamma\), and so:
\[ \Delta \vdash \Coh \Gamma {R(B)} {R(\sigma)} : R(B) \sub {R(\sigma)}\]
Then by induction on statement (3), we get:
\[ \Delta \vdash P(\Coh \Gamma {R(B)} {R(\sigma)} : R(N(R(B) \sub
{R(\sigma)}))) \]
By induction on statement (2), we have \(R(B) \sub {R(\sigma)} = B
\sub \sigma\). By inspection of the original typing derivation, we
have \(B \sub \sigma = A\), and so \(R(N(R(B) \sub {R(\sigma)}))
\equiv R(N(A))\), as required.

Now consider statement 2. The cases for types and substitutions
follow by an easy induction from the result for terms. Since the case
for variables is trivial, we restrict to the cases for the coherence
terms, where we must prove that:
\[ \Gamma \vdash_{\mathcal{R}} \Coh \Delta A \sigma = P(\Coh \Delta
{R(A)} {R(\sigma)})\]
By (1), \(\Coh \Delta {R(A)} {R(\sigma)}\) is a well-formed \Catt
term, and so by (4) and induction on subterms we have:
\[ P(\Coh \Delta {R(A)} {R(\sigma)}) = \Coh \Delta {R(A)} {R(\sigma)}
= \Coh \Delta A \sigma\]

For statement 3, we let \(\Delta \vdash t : A\) and prove for each
\(k\) that \(P_k(t)\) is well-formed and that \(\src_m(P_k(t)) \equiv
R(N(\src_m(t)))\) and \(\tgt_m(P_k(t)) \equiv R(N(\tgt_m(t)))\) for
\(m \leq k\). We proceed by induction on \(k\). The case for \(k =
0\) is trivial, so we must prove that \(P_{k+1}(t)\) is well-formed,
which is the term:
\[\stdcoh {T_{k}^{n}} n \sub {\langle \phi(\src_{k}(P_{k}(t)))^{-1},
P_{k}(t), \phi(\tgt_k(P_{k}(t))) \rangle} \]
By (5), noting that the inductive hypothesis on \(k\) implies that
the types of \(\src_k(P_k(t))\) and \(\tgt_k(P_k(t))\) are in
rehydrated normal form, we have that the normalisers are well-typed.
Therefore, \(P_{k+1}(t)\) is well-formed by the previous fact and the
inductive hypothesis on \(k\). By simple calculation it follows that:
\[\src_m(P_k(t)) \equiv \src_m(P_m(t)) \equiv
\src(\phi(\src_m(t))^{-1}) \equiv R(N(\src_m(t)))\]
with a similar equation holding for the target. It then follows that
\(\Delta \vdash P(t) : R(N(A))\).

Statement 4 holds by a simple induction on \(k\), using statement (6)
to reduce each normaliser to an identity, and then using pruning and
disc removal to get the equality:
\[ \stdcoh {T_{k}^{n}} n \sub {\langle \id(\src_{k}(P_{k}(t))),
P_{k}(t), \id(\tgt_k(P_{k}(t))) \rangle} = P_k(t) \]
which along with the inductive hypothesis on \(k\) is sufficient.

For statement 5, we assume \(\Delta \vdash t : R(N(A))\). Then, by
(1) and the preservation rule, we have \(\Delta \vdash \Delta \vdash
R(N(t)) : R(N(R(N(A)))) \equiv R(N(A))\), where the equality follows
from (2) and the idempotency of the normal form functor. The typing
for the normaliser then trivially follows, as \(t\) and \(R(N(t))\)
are full in \(\Supp(t)\).

For statement 6, we apply statement (1) to get that \(t = N(t) =
R(N(t))\). Therefore:
\begin{align*}
  \phi(t) &\equiv \Coh {\Supp(t)} {t \to R(N(t))} {i_t}\\
  &= \Coh {\Supp(t)} {t \to t} {i_t}\\
  &= \id(t) \sub {i_t}&\text{by endo-coherence removal}\\
  &\equiv \id(t)
\end{align*}

This completes all parts of the definitions and proofs. Then for any
well-formed \Cattr term \(t\), \(R(N(t))\) is a well-formed \Catt
term with \(R(N(t)) = t\) in \Cattr completing the proof of
\cref{thm:rehydration}. Moreover, if \(t = t'\) then \(R(N(t)) \equiv
R(N(t'))\), and so the rehydrated of \Cattr terms over pasting
contexts can be chosen to respect \Cattr equality. From this we get
the following corollary.

\begin{corollary}
  Semistrictness is a property. Let \(\mathcal{R}\) is a tame
  equality rule set satisfying the support and preservation
  conditions in addition to having pruning, disc removal, and
  endo-coherence removal. If \(F\) and \(G\) are \Cattr models such
  that: \[F \circ K_{\mathcal{R}}^{\mathsf{ps}}  = G \circ
  K_{\mathcal{R}}^{\mathsf{ps}}\]
  then \(F = G\).
\end{corollary}
\begin{proof}
  Since \(K_{\mathcal{R}}^{\mathsf{ps}}\) is the identity on objects,
  it follows that \(F\) and \(G\) must be equal on objects. Now let
  \(\Gamma\) and \(\Delta\) be pasting diagrams, and let \(\Gamma
  \vdash_{\mathcal{R}} \sigma : \Delta\). Then by
  \cref{thm:rehydration} we have, \(\Gamma \vdash R(\sigma) : \Delta\) and so:
  \[ F(K_{\mathcal{R}}^{\mathsf{ps}}(R(\sigma))) =
  G(K_{\mathcal{R}}^{\mathsf{ps}}(R(\sigma))) \]
  but \(K_{\mathcal{R}}^{\mathsf{ps}}\) is simply an inclusion, so
  \(F(R(\sigma)) = G(R(\sigma))\) and since \(R(\sigma) = \sigma\) in
  \Cattr, we have \(F(\sigma) = G(\sigma)\). The substitution
  \(\sigma\) was arbitrary, so \(F = G\) as required.
\end{proof}

The above result holds in particular for the equality rule sets \su
and \sua, meaning that a model of \Catt can be a model of \Cattsu or
\Cattsua in at most one way.

\subsection{Towards generalised rehydration}
\label{sec:towards-gener-rehydr}

The rehydration result of the previous section can be viewed as a
partial conservativity result, stating that in a pasting context,
\Cattsu and \Cattsua have the same expressive power as \Catt. The
original motivation of semistrictness was to strictify parts of the
theory without losing the expressiveness of the fully weak setting.
We would therefore hope that the rehydration results of
\cref{sec:rehydration} extend to arbitrary contexts.

Such a result would be a powerful tool for constructing terms in a
weak setting; a term could be constructed by constructing it in the
semistrict setting, before applying rehydration to the resulting term
to get term in the fully weak setting. Such a technique would allow a
\Catt proof of Eckmann-Hilton to be constructed mechanically from the
vastly simpler \Cattsu Eckmann-Hilton proof, or even give a proof of
the Syllepsis in \Catt, for which no proof has been given as of writing.

By observing the proof of \cref{thm:rehydration}, we see that the
main part that would need replacing for a general rehydration result
is the construction of the normalisers, as we can no longer rely on
the source and target term of our normaliser living over a pasting
diagram that allows the construction of a single coherence. A natural
way to proceed is to attempt to build a normaliser \(\phi(t) : t \to
R(N(t))\) by recursion on the reduction sequence \(t \red^* N(t)\).
We consider a context with \(x : *\) and a scalar \(\alpha : \id(x)
\to \id(x)\), and consider the reduction by pruning:
\[ \alpha *_0 \id(x) \red (\alpha)\]
where \((\alpha)\) is the unary composite on \(\alpha\). We
immediately encounter two problems:
\begin{itemize}
  \item For each individual reduction, the source and target of the
    reduction may not have the same type. In the example above, the
    source has type \(\id(x) * \id(x) \to \id(x) * \id(x)\), but the
    target has type \(\id(x) \to \id(x)\). A normaliser between these
    two terms can therefore not be directly constructed.
  \item If the source term is padded such that it has the same type
    as the target term, we can run into a separate problem. Consider
    the reduction given above again. The following normaliser can be formed:
    \[ \Coh {D^2} {\rho_{d_1^-}^{-1} *_1 (d_2 *_0 \id(d_0^+)) *_1
    \rho_{d_1^+} \to (d_2)} {\langle \{\alpha\} \rangle}\]
    which has source given by the padded term:
    \[
      \begin{tikzcd}
        x & x & x
        \arrow[""{name=0, anchor=center, inner sep=0}, "{\id(x)}",
        curve={height=-12pt}, from=1-1, to=1-2]
        \arrow[""{name=1, anchor=center, inner sep=0}, "{\id(x)}"',
        curve={height=12pt}, from=1-1, to=1-2]
        \arrow["{\id(x)}"', from=1-2, to=1-3]
        \arrow[""{name=2, anchor=center, inner sep=0}, "{\id(x)}",
        controls=+(90:1.5) and +(90:1.5), from=1-1, to=1-3]
        \arrow[""{name=3, anchor=center, inner sep=0}, "{\id(x)}"',
        controls=+(270:1.5) and +(270:1.5), from=1-1, to=1-3]
        \arrow["\alpha"', shorten <=3pt, shorten >=3pt, Rightarrow,
        from=1, to=0]
        \arrow["{\rho_{\id(x)}}", shorten >=3pt, Rightarrow, from=1-2, to=3]
        \arrow["{\rho_{\id(x)}}"', shorten >=3pt, Rightarrow, from=1-2, to=2]
      \end{tikzcd}
    \]
    However this term is padded by the right unitor on each side,
    which is not the canonical normaliser from \(\id(x) * \id(x)\) to
    \(\id(x)\), the unbiased unitor.
\end{itemize}

The reduction above was not only chosen to demonstrate both of these
problems, but was chosen as it is the problematic reduction that is
encountered if one tries to rehydrate the Eckmann-Hilton term from
\Cattsu. To give a proof of Eckmann-Hilton, one reaches a critical
point where a left unitor and right unitor on the identity must be
cancelled out, highlighting the second of the two problems.

To solve the second problem one could attempt to prove that for any
two reductions paths from \(t\) to \(N(t)\), that there is a higher
cell between the normalisers generated from each reduction path,
critically relying on the confluence proof for the theory to
modularise the problem into finding fillers for each confluence
diamond. Such an approach seems infeasible for the following reasons:
To find fillers for a confluence diamond, we presumably must already
know the form of all rehydrations in the dimension below, which
themselves could depend on filling confluence diamonds of the
dimension below. This seems to necessitate rehydrating on a dimension
by dimension basis, making the full rehydration problem infeasible.
It is also likely that at some point it would be necessary to show
that two different fillers of a confluence diamond have a higher cell
between them, leading to some form of \(\infty\)-groupoid flavoured
confluence problem. Such a problem also seems infeasible with the
tools currently available to us.

An alternative approach could be to show that the ``space'' of all
rehydrations is contractible. This can be made precise in the
following way. Let \(t\) be a \Cattr term. Then consider the globular
set whose \(0\)-cells are \Catt terms \(s\) which are equal to \(t\)
in \Cattr, \(1\)-cells are given by \Catt terms \(f : s \to s'\)
which are equal to \(\id(t)\) in \Cattr, in general \(n\)-cells given
by \Catt terms that are equal to \(\id^n(t)\). The contractability of
such a globular set is exactly the property needed for rehydration,
as it gives the existence of a \(0\)-cell \(s\) which gives the
rehydration, and witnesses the essential uniqueness of this rehydration.

Such a contractability proof can be given when the term \(t\) is a
term of a pasting diagram, as any higher cells can be given by a
simple coherence. This allows us to fix the padding in the example
above, observing that the right unitor is equivalent to the unbiased
unitor. It is however unclear how such a contractability proof could
be extended to arbitrary contexts.

We now turn our attention to the first problem presented above. One
method for tackling this problem is to give normalisers as a
\emph{cylindrical equivalence} instead of a regular equivalence. A
cylindrical equivalence can be viewed as the canonical notion of
equivalence between two objects of different types. We introduce the
first few dimensions of cylinder terms. A \(0\)-cylinder is simply a
\(1\)-dimensional term. A \(1\)-cylinder from a cylinder \(f : w \to
x\) to a cylinder \(g : y \to z\) can be defined by the square:
\[
  \begin{tikzcd}
    x & z \\
    w & y
    \arrow["f", from=2-1, to=1-1]
    \arrow["g"', from=2-2, to=1-2]
    \arrow["a"', from=2-1, to=2-2]
    \arrow["a'", from=1-1, to=1-2]
    \arrow[Rightarrow, from=2-2, to=1-1]
  \end{tikzcd}
\]
where the central arrow has type \(a * g \to f * a'\). If such a
cylinder was invertible, which is the case when \(a\), \(b\), and the
two-dimensional cell are invertible, then it would be a cylindrical
equivalence and would witness the equivalence of \(f\) and \(g\).
Suppose two \(1\)-cylinders \(\alpha : f \to g\) and \(\beta : g \to
h\) as below:
\[
  \begin{tikzcd}
    x & z & v \\
    w & y & u
    \arrow["f", from=2-1, to=1-1]
    \arrow["g"{description}, from=2-2, to=1-2]
    \arrow["a"', from=2-1, to=2-2]
    \arrow["{a'}", from=1-1, to=1-2]
    \arrow[Rightarrow,from=2-2, to=1-1]
    \arrow["h"', from=2-3, to=1-3]
    \arrow["b"', from=2-2, to=2-3]
    \arrow["{b'}", from=1-2, to=1-3]
    \arrow[Rightarrow,from=2-3, to=1-2]
  \end{tikzcd}
\]
Then a composite cylinder \(f \to h\) could be formed by letting the
front ``face'' be given by \(a * b\), the back ``face'' be given by
\(a' * b'\) and the filler given by a combination of associators and
whiskerings of the two fillers in the diagram. A \(2\)-cylinder could
be given by the following diagram:
% https://q.uiver.app/#q=WzAsNCxbMCwxLCJcXGJ1bGxldCJdLFsyLDEsIlxcYnVsbGV0Il0sWzIsMCwiXFxidWxsZXQiXSxbNCwwLCJcXGJ1bGxldCJdLFswLDEsIiIsMCx7ImN1cnZlIjotNX1dLFswLDEsIiIsMix7ImN1cnZlIjo1fV0sWzIsMywiIiwyLHsiY3VydmUiOi01fV0sWzIsMywiIiwwLHsiY3VydmUiOjV9XSxbMCwyXSxbMSwzXSxbNCw2LCIiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzUsNywiIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dXQ== % tex-fmt: skip
\[
  \begin{tikzcd}
    && \bullet && \bullet \\
    \bullet && \bullet
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-40pt}, from=2-1, to=2-3]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=40pt}, from=2-1, to=2-3]
    \arrow[""{name=2, anchor=center, inner sep=0},
    curve={height=-40pt}, from=1-3, to=1-5]
    \arrow[""{name=3, anchor=center, inner sep=0},
    curve={height=40pt}, from=1-3, to=1-5]
    \arrow[from=2-1, to=1-3]
    \arrow[from=2-3, to=1-5]
    \arrow[shorten <=13pt, shorten >=13pt, Rightarrow, from=0, to=2]
    \arrow[shorten <=13pt, shorten >=13pt, Rightarrow, from=1, to=3]
    \arrow[shorten <=8pt, shorten >=8pt, Rightarrow, from=1, to=0]
    \arrow[shorten <=8pt, shorten >=8pt, Rightarrow, from=3, to=2]
\end{tikzcd}\]
where the top and bottom faces of this diagram are \(1\)-cylinders,
and the whole digram should be filled by a \(3\)-dimensional term
with appropriate source and target. The shape of this diagram gives
the name to this construction.

When using cylinders to represent the normalisers in a rehydration
process, the inductive step for coherences would require a cylinder
to be generated from a cylindrical version of the substitution
attached to the coherence. We have seen that this can be done when
the coherence is given by \(1\)-composition, but achieving full
rehydration would involve giving cylindrical versions of every
operation in \Catt. No such proof has been given for any variety of
globular weak \(\infty\)-categories.

We offer an alternative solution which avoids defining cylinder
composition, which we call \emph{rehydration by dimension}. From an
equality rule set \(\mathcal{R}\), we can form the rule sets
\(\mathcal{R}_n\) which consists of the rules in \((\Gamma,s,t) \in
\mathcal{R}\) such that \(\dim(s) = \dim(t) \leq n\). Rehydration by
dimension attempts to rehydrate an \(n\)-dimensional term \(t\) by
constructing terms \(t_n,\dots,t_0\) such that \(t_i\) is a term
which is well-formed in \(\Catt_{\mathcal{R}_i}\), creating a
rehydration sequence:
\[ \Catt_{\mathcal{R}_n} \to \Catt_{\mathcal{R}_{n-1}} \to \cdots \to
\Catt_{\mathcal{R}_1} \to \Catt_{\mathcal{R}_0}\]
The term \(t_n\) is given immediately by \(t\), and \(t_0\) is then a
term of \(\Catt_{\mathcal{R}_0} = \Catt\), giving the rehydration of
\(t\). The key insight of this method is that when generating the
normaliser for a particular \(k\)-dimensional generating rule \(s
\red t\), we know by the preservation property that the types of
\(s\) and \(t\) are equal, and so are further equal in
\(\Catt_{\mathcal{R}_{k-1}}\). By factoring through these partial
rehydrations, the normaliser of a dimension \(k\) generating rule
only has to be valid in \(\Catt_{\mathcal{R}_{k-1}}\), meaning that
the normalisers can again be given by regular equivalences.

Unfortunately, this method does not avoid the need to define new
classes of operations in \Catt, as we could be required to prove that
arbitrary \Catt operations are natural in their lower-dimensional
arguments. Consider terms \(f : x \to y\) and \(g : y \to z\) and
suppose the \(\Catt_{\mathcal{R}_1}\) normal form of \(y\) is \(y'\)
with normaliser \(\phi(y)\). Then, during a rehydration proof to
\(\Catt_{\mathcal{R}_0}\), it may be required to give a normaliser
from \(f * g\) to \((f * \phi(y)) * (\phi(y)^{-1} * g)\), effectively
requiring us to prove that \(1\)-composition is natural in its
central \(0\)-cell. Similarly to the case with cylinders, in this
case for \(1\)-composition, such a normaliser can easily be given,
but we possess no way of creating such naturality arguments on
arbitrary coherences.

The proofs of Eckmann-Hilton given in \cref{sec:examples} give an
example of the result of each of these methods, with the proof in
\texttt{/examples/eh.catt} proceeding by ``rehydration by
dimension'', and the proof in \texttt{/examples/eh-cyll.catt} using
cylinders. In both proofs, the only example of the second problem we
encounter is proving that the left and right unitors on the identity
are equivalent to the unbiased unitor. For the cylinder proof, the
composition of \(1\)-cylinders is used and is given by the term
\texttt{cyl\_comp}, which is then implicitly suspended by the tool.
The rehydration by dimension proof needs a naturality move like the
one described above, which is given by the term \texttt{compat\_move}.

\section{Future ideas}
\label{sec:future-work}

In this final section, we collect together some ideas for the
continuation of this work, including ideas for different semistrict
theories based on \Cattr, and modifications to the existing theories.
Some ideas for future avenues of research have already been
discussed, such as the potential improvements to the implementation
discussed in \cref{sec:further-work}, and the discussion of full
rehydration given in \cref{sec:towards-gener-rehydr}, which we will
not repeat here.

\paragraph{Further results for \Cattsua}

The metatheory of \Cattsua is more complicated than the corresponding
metatheory of \Cattsu, though at first glance the relative increase
in power does not match this complexity. The jump from \Catt to
\Cattsu vastly simplified the proof of Eckmann-Hilton, allowed the
syllepsis to be proven, and lead to results such as disc
trivialisation. In contrast, \Cattsua provides no further
simplification to Eckmann-Hilton and only slightly simplifies the
syllepsis, removing some associators from the proof.

One potential utility of \Cattsua could be simplifying the composites
of cylinders, as briefly introduced in
\cref{sec:towards-gener-rehydr}. Consider the following diagram from
that section which contains two composable \(1\)-cylinders.
\[
  \begin{tikzcd}
    x & z & v \\
    w & y & u
    \arrow["f", from=2-1, to=1-1]
    \arrow["g"{description}, from=2-2, to=1-2]
    \arrow["a"', from=2-1, to=2-2]
    \arrow["{a'}", from=1-1, to=1-2]
    \arrow["X", Rightarrow,from=2-2, to=1-1]
    \arrow["h"', from=2-3, to=1-3]
    \arrow["b"', from=2-2, to=2-3]
    \arrow["{b'}", from=1-2, to=1-3]
    \arrow["Y", Rightarrow,from=2-3, to=1-2]
  \end{tikzcd}
\]
In \Catt, the \(1\)-composite of these cylinders is a term \((a*b)*h
\to f*(a'*b')\) given by:
\[ \alpha_{a,b,h} *_1 (a *_0 Y) *_1 \alpha_{a,g,b'}^{-1} *_1 (X *_0
b') * \alpha_{f,a',b'}\]
where each \(\alpha\) term is an associator. This would of course
simplify in \Cattsua to \((a *_0 Y) *_1 (X *_0 b')\). Such a
simplification could make it simpler to define higher cylinder
coherences, such as associator for \(1\)-cylinders, which would be
trivial in \Cattsua, but far more involved in \Catt.

Further future work for \Cattsua could involve the search for an
analogue of disc trivialisation for \Cattsua. We would expect there
to be a more general class of contexts that are trivialised by
\Cattsua but are not trivialised. The contexts present in the
cylinder contexts presented above could form a starting point for such a study.

A separate avenue for further study is to explore the links between
\Cattsua and more graphical presentations of semistrict
\(\infty\)-categories. String diagrams are a common graphical method
for working with monoidal categories and
bicategories~\cite{selinger2011survey}, and their higher-dimensional
counterparts, such as those implemented in the tool
\textsf{homotopy.io}, can be viewed as strictly associative and
unital finitely presented \(\infty\)-categories, much like contexts
of \Cattsua. Translation results in either direction between these
two settings, while highly non-trivial due to the contrast in the way
each system approaches composition, would be valuable.

\paragraph{Generalised insertion}

The conditions given for insertion in \cref{sec:insertion} were not
the most general conditions possible. In this section, we stated that
to perform insertion we required an insertion redex
\((S,P,T,\U,L,M)\), and one of the conditions of this insertion redex was that:
\[ L(\olsi P) \equiv \stdcoh T {\lh(P)} \sub M\]
It turns out that it is sufficient to give the weaker condition that
the locally maximal argument is a coherence where the type contained
in the coherence is sufficiently suspended:
\[ L(\olsi P) \equiv \Coh T {\Sigma^{\bh(P)}(A)} M\]
As \(\stdcoh {\Sigma(T)} {n+1} \equiv \Sigma (\stdcoh T n)\), and the
original condition required that \(\th(T) \geq \bh(P)\), this
alternative condition is a strict generalisation of the previous
condition. Under the new condition, the exterior labelling must be
modified. It firstly must take the type \(A\) as an argument. The
case for \(P = [k]\) is then modified such that
\(\kappa_{S,[k],T,A}\) (noting the extra type subscript) is given by:
\[
  \begin{tikzcd}[column sep=smaller,row sep = 20pt]
    {[S_0,\dots,S_{k-1}]} & \doubleplus & {T} & \doubleplus &
    {[S_{k+1},\dots,S_n]} \\
    \\
    {[S_0,\dots,S_{k-1}]} & \vee & {\Sigma S_k} & \vee & {[S_{k+1},\dots,S_n]}
    \arrow["{\{A, \Coh T A {\id_T}\}}"{description, font =
    \normalsize}, from=3-3, to=1-3]
    \arrow["\id"{font = \normalsize}, from=3-1, to=1-1]
    \arrow["\id"{font = \normalsize}, from=3-5, to=1-5]
  \end{tikzcd}
\]
when \(S = [S_0,\dots,S_n]\). The inductive step of the exterior
labelling then relies on the type \(A\) being sufficiently suspended
to proceed, just as the original version depends on the trunk height
of \(T\) being sufficient to proceed (we note that the trunk height
condition is still needed in this generalisation). For the necessary
typing judgements to be satisfied, we must have \(\src_0(A) \equiv
\fst(\lfloor T \rfloor)\) and \(\tgt_0(A) \equiv \snd(\lfloor T
\rfloor)\), but no other extra condition is necessary.

In some ways, this definition of insertion is more natural than the
definition given earlier. We no longer rely on the syntactic
condition of the locally maximal argument being a standard coherence,
only relying on the far weaker suspendability property. In the proof
for confluence of \Cattsua, a large focus was cases where a reduction
modified a standard coherence into a term which was no longer a
standard coherence. Cases like these do not happen with generalised
insertion, as reductions do not break the suspendability property.
More generally, a confluence proof for generalised insertion does not
require any proof about the interaction of insertion with boundary
inclusion maps and standard coherences (given in
\cref{sec:further-properties} for the original definition).

Unfortunately, this generalised form of insertion cannot be directly
used in \Cattsua without breaking confluence. Let \(\Gamma\) be the
following context given by the following diagram:
% https://q.uiver.app/#q=WzAsNCxbMCwwLCJcXGJ1bGxldCJdLFsyLDAsIlxcYnVsbGV0Il0sWzMsMCwiXFxidWxsZXQiXSxbNCwwLCJcXGJ1bGxldCJdLFswLDEsIiIsMCx7ImN1cnZlIjotNX1dLFswLDEsIiIsMix7ImN1cnZlIjo1fV0sWzEsMiwiZyIsMCx7ImN1cnZlIjotMn1dLFsxLDIsImYiLDIseyJjdXJ2ZSI6Mn1dLFsyLDMsImkiLDAseyJjdXJ2ZSI6LTJ9XSxbMiwzLCJoIiwyLHsiY3VydmUiOjJ9XSxbNSw0LCIiLDIseyJvZmZzZXQiOi01LCJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzUsNCwiIiwwLHsib2Zmc2V0Ijo1LCJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzcsNiwiXFxhbHBoYSIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbOSw4LCJcXGJldGEiLDIseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzEwLDExLCJcXHBoaSIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XV0= % tex-fmt: skip
\[
  \begin{tikzcd}
    \bullet && \bullet & \bullet & \bullet
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-30pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=30pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0}, "g",
    curve={height=-12pt}, from=1-3, to=1-4]
    \arrow[""{name=3, anchor=center, inner sep=0}, "f"',
    curve={height=12pt}, from=1-3, to=1-4]
    \arrow[""{name=4, anchor=center, inner sep=0}, "i",
    curve={height=-12pt}, from=1-4, to=1-5]
    \arrow[""{name=5, anchor=center, inner sep=0}, "h"',
    curve={height=12pt}, from=1-4, to=1-5]
    \arrow[""{name=6, anchor=center, inner sep=0}, "\gamma", shift
    left=5, shorten <=8pt, shorten >=8pt, Rightarrow, from=1, to=0]
    \arrow[""{name=7, anchor=center, inner sep=0}, "\delta"', shift
    right=5, shorten <=8pt, shorten >=8pt, Rightarrow, from=1, to=0]
    \arrow["\alpha"', shorten <=3pt, shorten >=3pt, Rightarrow, from=3, to=2]
    \arrow["\beta"', shorten <=3pt, shorten >=3pt, Rightarrow, from=5, to=4]
    \arrow["\phi"', shorten <=4pt, shorten >=4pt, Rightarrow,
    nfold=3, from=6, to=7]
  \end{tikzcd}
\]
and consider the terms:
\begin{align*}
  I &= (\alpha *_0 h) *_1 (g *_0 \beta)\\
  E &= \Coh {\Supp(I)} {I \to I} {\id}\\
  X &= \phi *_0 E
\end{align*}
We now have the following critical pair: \(X\) can reduce by
inserting the locally maximal argument \(E\), as the branch has
branching height \(0\) making the suspendability condition vacuous,
but \(E\) also reduces by endo-coherence removal. By performing the
generalised insertion we obtain the coherence:
\[ \Coh \Gamma {\gamma *_0 I \to \delta *_0 I} \id\]
Let \(W(x,y,z)\) refer to the standard composite over the diagram:
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJcXGJ1bGxldCJdLFsxLDAsIlxcYnVsbGV0Il0sWzIsMCwiXFxidWxsZXQiXSxbMCwxLCIiLDAseyJjdXJ2ZSI6LTN9XSxbMCwxLCIiLDIseyJjdXJ2ZSI6M31dLFsxLDIsIiIsMix7ImN1cnZlIjotNX1dLFsxLDIsIiIsMix7ImN1cnZlIjo1fV0sWzEsMl0sWzQsMywiIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs2LDcsIiIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbNyw1LCIiLDIseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV1d % tex-fmt: skip
\[
  \begin{tikzcd}
    \bullet & \bullet & \bullet
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-18pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=18pt}, from=1-1, to=1-2]
    \arrow[""{name=2, anchor=center, inner sep=0},
    curve={height=-30pt}, from=1-2, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0},
    curve={height=30pt}, from=1-2, to=1-3]
    \arrow[""{name=4, anchor=center, inner sep=0}, from=1-2, to=1-3]
    \arrow["x"',shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow["y"',shorten <=4pt, shorten >=4pt, Rightarrow, from=3, to=4]
    \arrow["z"',shorten <=4pt, shorten >=4pt, Rightarrow, from=4, to=2]
  \end{tikzcd}
\]
Then the coherence term above admits further cell reductions which
convert the composites \(\gamma *_0 I\) and \(\delta *_0 I\) to
\(W(\gamma, (\alpha *_0 h), (g *_0 \beta))\) and \(W(\delta, (\alpha
*_0 h), (g *_0 \beta))\). The resulting term reduces no further. If
the endo-coherence removal is performed, then \(E\) reduces to
\(\id(I)\), which can be pruned from the original composite. After
further reductions, we obtain a coherence over the context \(\Delta\)
given by the following diagram:
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJcXGJ1bGxldCJdLFsyLDAsIlxcYnVsbGV0Il0sWzMsMCwiXFxidWxsZXQiXSxbMCwxLCIiLDAseyJjdXJ2ZSI6LTV9XSxbMCwxLCIiLDAseyJjdXJ2ZSI6NX1dLFsxLDIsIiIsMCx7ImN1cnZlIjotMn1dLFsxLDIsIiIsMix7ImN1cnZlIjoyfV0sWzQsMywiIiwwLHsib2Zmc2V0IjotNSwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs0LDMsIiIsMix7Im9mZnNldCI6NSwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs2LDUsIkIiLDIseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzcsOCwiQSIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XV0= % tex-fmt: skip
\[
  \begin{tikzcd}
    \bullet && \bullet & \bullet
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-30pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=30pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0},
    curve={height=-30pt}, from=1-3, to=1-4]
    \arrow[""{name=6, anchor=center, inner sep=0},
    curve={height=0pt}, from=1-3, to=1-4]
    \arrow[""{name=3, anchor=center, inner sep=0},
    curve={height=30pt}, from=1-3, to=1-4]
    \arrow[""{name=4, anchor=center, inner sep=0}, "x", shift left=5,
    shorten <=8pt, shorten >=8pt, Rightarrow, from=1, to=0]
    \arrow[""{name=5, anchor=center, inner sep=0}, "y"', shift
    right=5, shorten <=8pt, shorten >=8pt, Rightarrow, from=1, to=0]
    \arrow["B"', shorten <=3pt, shorten >=3pt, Rightarrow, from=3, to=6]
    \arrow["C"', shorten <=3pt, shorten >=3pt, Rightarrow, from=6, to=2]
    \arrow["A"', shorten <=4pt, shorten >=4pt, Rightarrow, nfold=3,
    from=4, to=5]
  \end{tikzcd}
\]
In particular, the result of these reductions is the following coherence:
\[ \Coh \Delta {W(x,B,C) \to W(y,B,C)} {\langle \phi, (\alpha *_0 h),
(g *_0 \beta) \rangle}\]
which admits no further reductions, hence breaking confluence. It is
even unclear which of these reduction paths is the more canonical for
such a system, the first moves the complexity of \(I\) to the type in
the coherence, whereas the second keeps the complexity of \(I\) in
the arguments of the coherence. Conjecturally, one could consider
generalisations to endo-coherence removal which could factor out the
common structure of \(W(\gamma, (\alpha *_0 h), (g *_0 \beta))\) and
\(W(\delta, (\alpha *_0 h), (g *_0 \beta))\), reducing the result of
the first reduction path to the result of the second reduction path,
though we have not explored any such definition.

\paragraph{A further strictification to \Cattsua}

\citeauthor{douglas2016internal} give an explicit representation a
Gray category~\cite[Definition~2.8]{douglas2016internal}, which can
be used as a direct point of comparison to \Cattsua, as Gray
categories are semistrict \(3\)-categories with strict unitors and
associators. The weak structure in their presentation of Gray
categories is given by an invertible \(3\)-cell they call
\emph{switch}, which has the same form as the \Catt term which we
called \(\mathsf{swap}\) in \cref{sec:cattsu}.

In their paper, all of the equalities between \(2\)-cells are
generated by a set of axioms [S2-4] to [S2-15]. Each of these
equalities is contained in the definitional equality of \Cattsua,
with the exception of [S2-9] and [S2-10], which witness a
compatibility between whiskering and vertical composition. We
consider the axiom [S2-9], as [S2-10] can be treated symmetrically.
Let \(\Delta\) be the context given by diagram:
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJcXGJ1bGxldCJdLFsxLDAsIlxcYnVsbGV0Il0sWzIsMCwiXFxidWxsZXQiXSxbMCwxLCJmIl0sWzEsMiwiIiwwLHsiY3VydmUiOi00fV0sWzEsMiwiIiwwLHsiY3VydmUiOjR9XSxbMSwyXSxbNSw2LCJcXGFscGhhIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs2LDQsIlxcYmV0YSIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XV0= % tex-fmt: skip
\[
  \begin{tikzcd}
    \bullet & \bullet & \bullet
    \arrow["f", from=1-1, to=1-2]
    \arrow[""{name=0, anchor=center, inner sep=0},
    curve={height=-24pt}, from=1-2, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0},
    curve={height=24pt}, from=1-2, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0}, from=1-2, to=1-3]
    \arrow["\alpha"', shorten <=3pt, shorten >=3pt, Rightarrow, from=1, to=2]
    \arrow["\beta"', shorten <=3pt, shorten >=3pt, Rightarrow, from=2, to=0]
  \end{tikzcd}
\]
and consider the following terms of \(\Delta\):
\[ (f *_0 \alpha) *_1 (f *_0 \beta) \qquad f *_0 (\alpha *_1 \beta)\]
while the second term reduces to the standard composite over
\(\Delta\), the first does not reduce, as no insertion can be
performed due to the condition on trunk height, and hence these two
terms are not equal in \Cattsua, unlike in Gray categories. Although
it could be argued that these axioms reside in the interchange family
of laws for \(\infty\)-categories, one could attempt to define a
stricter version of \Cattsua which incorporates these equalities,
with the aim of proving that \(3\)-truncated models of this stricter
type theory are equivalent to Gray categories.

\paragraph{Strict interchange}

In contrast to the reductions in this thesis which strictify units,
one could instead consider reductions that strictify all composition,
making the associativity and interchange laws strict, leaving only
units weak. Such a form of semistrictness is often called
\emph{Simpson semistrictness}, due to a conjecture of
\citeauthor{simpson1998homotopy}~\cite{simpson1998homotopy} that
leaving units weak is sufficient to retain the full expressiveness of
weak \(\infty\)-categories.

To achieve this, one could try an approach similar to insertion of
merging arguments of a term into the head coherence, when all the
involved terms are standard coherences. To be able to strictify terms
such as the \(\mathsf{swap}\) term given in \cref{sec:cattsu}, the
trunk height condition of insertion must be dropped. This immediately
leads to composites over contexts which are not pasting diagrams:
Consider the context generated by the diagram:
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJ4Il0sWzEsMCwieSJdLFsyLDAsInoiXSxbMCwxLCJnIiwwLHsiY3VydmUiOi0zfV0sWzAsMSwiZiIsMix7ImN1cnZlIjozfV0sWzEsMiwiaSIsMCx7ImN1cnZlIjotM31dLFsxLDIsImgiLDIseyJjdXJ2ZSI6M31dLFs0LDMsIlxcYWxwaGEiLDIseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzYsNSwiXFxiZXRhIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dXQ== % tex-fmt: skip
\[
  \begin{tikzcd}
    x & y & z
    \arrow[""{name=0, anchor=center, inner sep=0}, "g",
    curve={height=-18pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
    curve={height=18pt}, from=1-1, to=1-2]
    \arrow[""{name=2, anchor=center, inner sep=0}, "i",
    curve={height=-18pt}, from=1-2, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0}, "h"',
    curve={height=18pt}, from=1-2, to=1-3]
    \arrow["\alpha"', shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow["\beta"', shorten <=5pt, shorten >=5pt, Rightarrow, from=3, to=2]
  \end{tikzcd}
\]
and then consider the following composite in this context:
\[ \alpha *_0 ((\beta *_0 \id(\id(z))) *_1 \rho_i)\]
where \(\rho_i\) is the right unitor on \(i\). Allowing a more
general form of merging would lead to this term becoming a composite
of the following form:
% https://q.uiver.app/#q=WzAsNCxbMCwwLCJ4Il0sWzEsMCwieSJdLFsyLDAsInoiXSxbMywwLCJ6Il0sWzAsMSwiZyIsMCx7ImN1cnZlIjotM31dLFswLDEsImYiLDIseyJjdXJ2ZSI6M31dLFsxLDIsImkiXSxbMSwyLCJoIiwyLHsiY3VydmUiOjN9XSxbMiwzLCJcXGlkKHopIiwyXSxbMSwzLCIiLDAseyJjdXJ2ZSI6LTV9XSxbMiwzLCJcXGlkKHopIiwyLHsiY3VydmUiOjN9XSxbNSw0LCJcXGFscGhhIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs3LDYsIlxcYmV0YSIsMix7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbMiw5LCJcXHJob19pIiwyLHsic2hvcnRlbiI6eyJ0YXJnZXQiOjIwfX1dLFsxMCw4LCJcXGlkKFxcaWQoeikpIiwyLHsic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dXQ== % tex-fmt: skip
\[
  \begin{tikzcd}
    x & y & z & z
    \arrow[""{name=0, anchor=center, inner sep=0}, "g",
    curve={height=-18pt}, from=1-1, to=1-2]
    \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
    curve={height=18pt}, from=1-1, to=1-2]
    \arrow[""{name=2, anchor=center, inner sep=0}, "i", from=1-2, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0}, "h"',
    curve={height=25pt}, from=1-2, to=1-3]
    \arrow[""{name=4, anchor=center, inner sep=0}, "{\id}", from=1-3, to=1-4]
    \arrow[""{name=5, anchor=center, inner sep=0}, "i",
    curve={height=-40pt}, from=1-2, to=1-4]
    \arrow[""{name=6, anchor=center, inner sep=0}, "{\id}"',
    curve={height=25pt}, from=1-3, to=1-4]
    \arrow["\alpha"', shorten <=5pt, shorten >=5pt, Rightarrow, from=1, to=0]
    \arrow["\beta"', shorten <=2pt, shorten >=2pt, Rightarrow, from=3, to=2]
    \arrow["{\rho_i}"'{pos=0.4}, shorten >=3pt, Rightarrow, from=1-3, to=5]
    \arrow["{\id^2}"', shorten <=2pt, shorten >=2pt, Rightarrow, from=6, to=4]
  \end{tikzcd}
\]
Although this diagram is not a pasting diagram, as it is not a
globular set, we would still expect it to fulfil a similar
contractability property to the one pasting diagrams do. One may
therefore be lead to believe that strict interchange could be
achieved in a type theory similar to \Catt by allowing a more general
class of pasting diagrams. This, however, does not work. We consider
the following counterexample due to
\citeauthor{forest2022unifying}~\cite{forest2022unifying}: let
\(\Gamma\) be the context generated by the following diagram.
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJcXGJ1bGxldCJdLFsyLDAsIlxcYnVsbGV0Il0sWzQsMCwiXFxidWxsZXQiXSxbMCwxLCIiLDAseyJjdXJ2ZSI6LTV9XSxbMCwxLCIiLDIseyJjdXJ2ZSI6NX1dLFswLDFdLFsxLDIsIiIsMSx7ImN1cnZlIjotNX1dLFsxLDIsIiIsMSx7ImN1cnZlIjo1fV0sWzEsMl0sWzQsNSwiXFxhbHBoYSIsMix7Im9mZnNldCI6LTQsInNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbNCw1LCJcXGFscGhhJyIsMix7Im9mZnNldCI6NCwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs1LDMsIlxcYmV0YSIsMix7Im9mZnNldCI6LTQsInNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbNSwzLCJcXGJldGEnIiwyLHsib2Zmc2V0Ijo0LCJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzgsNiwiXFxkZWx0YSIsMix7Im9mZnNldCI6LTQsInNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XSxbOCw2LCJcXGRlbHRhJyIsMix7Im9mZnNldCI6NCwic2hvcnRlbiI6eyJzb3VyY2UiOjIwLCJ0YXJnZXQiOjIwfX1dLFs3LDgsIlxcZ2FtbWEiLDIseyJvZmZzZXQiOi00LCJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9fV0sWzcsOCwiXFxnYW1tYSciLDIseyJvZmZzZXQiOjQsInNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH19XV0= % tex-fmt: skip
\[
  \begin{tikzcd}
    \bullet && \bullet && \bullet
    \arrow[""{name=0, anchor=center, inner sep=0}, "h",
    curve={height=-40pt}, from=1-1, to=1-3]
    \arrow[""{name=1, anchor=center, inner sep=0}, "f"',
    curve={height=40pt}, from=1-1, to=1-3]
    \arrow[""{name=2, anchor=center, inner sep=0}, "g"{description},
    from=1-1, to=1-3]
    \arrow[""{name=3, anchor=center, inner sep=0}, "k",
    curve={height=-40pt}, from=1-3, to=1-5]
    \arrow[""{name=4, anchor=center, inner sep=0}, "i"',
    curve={height=40pt}, from=1-3, to=1-5]
    \arrow[""{name=5, anchor=center, inner sep=0}, "j"{description},
    from=1-3, to=1-5]
    \arrow["\alpha\vphantom{\alpha'}"', shift left=3, shorten <=4pt,
    shorten >=4pt, Rightarrow, from=1, to=2]
    \arrow["{\alpha'}"', shift right=3, shorten <=4pt, shorten >=4pt,
    Rightarrow, from=1, to=2]
    \arrow["\beta\vphantom{\beta'}"', shift left=3, shorten <=4pt,
    shorten >=4pt, Rightarrow, from=2, to=0]
    \arrow["{\beta'}"', shift right=3, shorten <=4pt, shorten >=4pt,
    Rightarrow, from=2, to=0]
    \arrow["\delta\vphantom{\delta'}"', shift left=3, shorten <=4pt,
    shorten >=4pt, Rightarrow, from=5, to=3]
    \arrow["{\delta'}"', shift right=3, shorten <=4pt, shorten >=4pt,
    Rightarrow, from=5, to=3]
    \arrow["\gamma\vphantom{\gamma'}"', shift left=3, shorten <=4pt,
    shorten >=4pt, Rightarrow, from=4, to=5]
    \arrow["{\gamma'}"', shift right=3, shorten <=4pt, shorten >=4pt,
    Rightarrow, from=4, to=5]
  \end{tikzcd}
\]
and let \(\Delta = \Gamma, (X : \alpha *_0 \delta \to \alpha' *_0
\delta'), (Y : \beta *_0 \gamma \to \beta' *_0 \gamma')\). We then
have the following distinct composites:
\[
  \left(
    \begin{matrix}
      f *_0 \gamma\\
      *_1\\
      X\\
      *_1\\
      \beta *_0 k
    \end{matrix}
  \right)
  *_2
  \left(
    \begin{matrix}
      \alpha' *_0 i\\
      *_1\\
      Y\\
      *_1\\
      h *_0 \delta'
    \end{matrix}
  \right)
  \not\cong
  \left(
    \begin{matrix}
      \alpha *_0 i\\
      *_1\\
      Y\\
      *_1\\
      h *_0 \delta
    \end{matrix}
  \right)
  *_2
  \left(
    \begin{matrix}
      f *_0 \gamma'\\
      *_1\\
      X\\
      *_1\\
      \beta' *_0 k
    \end{matrix}
  \right)
\]
which are intuitively the composite of \(X\) and \(Y\) in either
order, where \(X\) and \(Y\) have been whiskered with the appropriate
terms. We note that the matrix notation above is only used to aid
comprehension, and does not represent the application of any matrix
operations. The approach described above of merging together
composites would lead to both of the above composites of \(X\) and
\(Y\) being reduced to the same composite over \(\Delta\),
contradicting the viability of such an approach.

An alternative, non-rewriting based approach could be defined by the
following equality rule:
\begin{equation*}
  \left\{ (\Gamma, s \sub \sigma, t \sub \sigma) \mathrel{\bigg\vert}{}
    \begin{matrix*}[l]
      \text{\(s\) and \(t\) are pure composite terms,}\\
      s = t \text{ in a strict \(\infty\)-category}
    \end{matrix*}
  \right\}
\end{equation*}
where a \emph{pure composite} is a term constructed only using
standard composites. Such an approach avoids the counter example
above, as the two composites of \(X\) and \(Y\) are not equal in a
strict \(\infty\)-category, and so would not be equated in the type
theory generated by this equality rule set.

We note that due to an algorithm of
\citeauthor{makkai2005word}~\cite{makkai2005word}, which is also
described and implemented by
\citeauthor{forest2021computational}~\cite{forest2021computational},
it can be decided whether terms \(s\) and \(t\) are equal in a strict
\(\infty\)-category. Therefore, to decide equality of the above
system, we need a method of finding the correct decomposition of a
term into a substitution applied to a purely compositional term. We
conjecture that there exists a factorisation system on
\(\mathsf{Catt}\) with the left class of morphisms given by purely
compositional substitutions, substitutions whose contained terms are
all pure composites, which could be used for this purpose. We leave
all details of such a construction for future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References:
%%

\printbibliography

\end{document}

% Local Variables:
% jinx-local-words: "CollegeShields ps"
% TeX-engine: xetex
% End:
